{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vtAgo5zYCClj"
   },
   "source": [
    "# 08. Natural Language Processing with TensorFlow\n",
    "\n",
    "![](https://raw.githubusercontent.com/mrdbourke/tensorflow-deep-learning/main/images/08-example-nlp-problems.png)\n",
    "*A handful of example natural language processing (NLP) and natural language understanding (NLU) problems. These are also often referred to as sequence problems (going from one sequence to another).*\n",
    "\n",
    "The main goal of [natural language processing (NLP)](https://becominghuman.ai/a-simple-introduction-to-natural-language-processing-ea66a1747b32) is to derive information from natural language.\n",
    "\n",
    "Natural language is a broad term but you can consider it to cover any of the following:\n",
    "* Text (such as that contained in an email, blog post, book, Tweet)\n",
    "* Speech (a conversation you have with a doctor, voice commands you give to a smart speaker)\n",
    "\n",
    "Under the umbrellas of text and speech there are many different things you might want to do.\n",
    "\n",
    "If you're building an email application, you might want to scan incoming emails to see if they're spam or not spam (classification).\n",
    "\n",
    "If you're trying to analyse customer feedback complaints, you might want to discover which section of your business they're for.\n",
    "\n",
    "> 🔑 **Note:** Both of these types of data are often referred to as *sequences* (a sentence is a sequence of words). So a common term you'll come across in NLP problems is called *seq2seq*, in other words, finding information in one sequence to produce another sequence (e.g. converting a speech command to a sequence of text-based steps).\n",
    "\n",
    "To get hands-on with NLP in TensorFlow, we're going to practice the steps we've used previously but this time with text data:\n",
    "\n",
    "```\n",
    "Text -> turn into numbers -> build a model -> train the model to find patterns -> use patterns (make predictions)\n",
    "```\n",
    "\n",
    "> 📖 **Resource:** For a great overview of NLP and the different problems within it, read the article [*A Simple Introduction to Natural Language Processing*](https://becominghuman.ai/a-simple-introduction-to-natural-language-processing-ea66a1747b32).\n",
    "\n",
    "## What we're going to cover\n",
    "\n",
    "Let's get specific hey?\n",
    "\n",
    "* Downloading a text dataset\n",
    "* Visualizing text data\n",
    "* Converting text into numbers using tokenization\n",
    "* Turning our tokenized text into an embedding\n",
    "* Modelling a text dataset\n",
    "  * Starting with a baseline (TF-IDF)\n",
    "  * Building several deep learning text models\n",
    "    * Dense, LSTM, GRU, Conv1D, Transfer learning\n",
    "* Comparing the performance of each our models\n",
    "* Combining our models into an ensemble\n",
    "* Saving and loading a trained model\n",
    "* Find the most wrong predictions\n",
    "\n",
    "## How you should approach this notebook\n",
    "\n",
    "You can read through the descriptions and the code (it should all run, except for the cells which error on purpose), but there's a better option.\n",
    "\n",
    "Write all of the code yourself.\n",
    "\n",
    "Yes. I'm serious. Create a new notebook, and rewrite each line by yourself. Investigate it, see if you can break it, why does it break?\n",
    "\n",
    "You don't have to write the text descriptions but writing the code yourself is a great way to get hands-on experience.\n",
    "\n",
    "Don't worry if you make mistakes, we all do. The way to get better and make less mistakes is to write more code.\n",
    "\n",
    "> 📖 **Resource:** See the full set of course materials on GitHub: https://github.com/mrdbourke/tensorflow-deep-learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4Zh2N1hZtvpN"
   },
   "source": [
    "## Check for GPU\n",
    "\n",
    "In order for our deep learning models to run as fast as possible, we'll need access to a GPU.\n",
    "\n",
    "In Google Colab, you can set this up by going to Runtime -> Change runtime type -> Hardware accelerator -> GPU.\n",
    "\n",
    "After selecting GPU, you may have to restart the runtime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DEYTFigmc3CI",
    "outputId": "1abc524b-0166-4237-f551-b0265972bec5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU 0: GeForce GTX 970 (UUID: GPU-de2d8fb2-b02f-477a-6898-721a844584fe)\r\n"
     ]
    }
   ],
   "source": [
    "# Check for GPU\n",
    "!nvidia-smi -L"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gS3YnNNI8oFk"
   },
   "source": [
    "## Get helper functions\n",
    "\n",
    "In past modules, we've created a bunch of helper functions to do small tasks required for our notebooks.\n",
    "\n",
    "Rather than rewrite all of these, we can import a script and load them in from there.\n",
    "\n",
    "The script containing our helper functions can be [found on GitHub](https://github.com/mrdbourke/tensorflow-deep-learning/blob/main/extras/helper_functions.py)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aFOHPqgE8pv-",
    "outputId": "03c9c050-d71c-4ea8-d7d6-514bd68f425c"
   },
   "outputs": [],
   "source": [
    "# Download helper functions script\n",
    "#!wget https://raw.githubusercontent.com/mrdbourke/tensorflow-deep-learning/main/extras/helper_functions.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "ICFbSkoM85tq"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-22 17:26:57.120956: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n"
     ]
    }
   ],
   "source": [
    "# Import series of helper functions for the notebook\n",
    "from helper_functions import unzip_data, create_tensorboard_callback, plot_loss_curves, compare_historys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cCZrclc2COWW"
   },
   "source": [
    "## Download a text dataset\n",
    "\n",
    "Let's start by download a text dataset. We'll be using the [Real or Not?](https://www.kaggle.com/c/nlp-getting-started/data) datset from Kaggle which contains text-based Tweets about natural disasters. \n",
    "\n",
    "The Real Tweets are actually about diasters, for example:\n",
    "\n",
    "```\n",
    "Jetstar and Virgin forced to cancel Bali flights again because of ash from Mount Raung volcano\n",
    "```\n",
    "\n",
    "The Not Real Tweets are Tweets not about diasters (they can be on anything), for example:\n",
    "\n",
    "```\n",
    "'Education is the most powerful weapon which you can use to change the world.' Nelson #Mandela #quote\n",
    "```\n",
    "\n",
    "For convenience, the dataset has been [downloaded from Kaggle](https://www.kaggle.com/c/nlp-getting-started/data) (doing this requires a Kaggle account) and uploaded as a downloadable zip file. \n",
    "\n",
    "> 🔑 **Note:** The original downloaded data has not been altered to how you would download it from Kaggle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "C0FEcci5IH8S",
    "outputId": "cf1821f9-527a-4f04-897a-93d2d7add437"
   },
   "outputs": [],
   "source": [
    "# Download data (same as from Kaggle)\n",
    "#!wget \"https://storage.googleapis.com/ztm_tf_course/nlp_getting_started.zip\"\n",
    "\n",
    "# Unzip data\n",
    "#unzip_data(\"nlp_getting_started.zip\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wBIR6tTI9QcR"
   },
   "source": [
    "Unzipping `nlp_getting_started.zip` gives the following 3 `.csv` files:\n",
    "* `sample_submission.csv` - an example of the file you'd submit to the Kaggle competition of your model's predictions.\n",
    "* `train.csv` - training samples of real and not real diaster Tweets.\n",
    "* `test.csv` - testing samples of real and not real diaster Tweets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7HpxZKYdD6V-"
   },
   "source": [
    "## Visualizing a text dataset\n",
    "\n",
    "Once you've acquired a new dataset to work with, what should you do first?\n",
    "\n",
    "Explore it? Inspect it? Verify it? Become one with it?\n",
    "\n",
    "All correct.\n",
    "\n",
    "Remember the motto: visualize, visualize, visualize.\n",
    "\n",
    "Right now, our text data samples are in the form of `.csv` files. For an easy way to make them visual, let's turn them into pandas DataFrame's.\n",
    "\n",
    "> 📖 **Reading:** You might come across text datasets in many different formats. Aside from CSV files (what we're working with), you'll probably encounter `.txt` files and `.json` files too. For working with these type of files, I'd recommend reading the two following articles by RealPython:\n",
    "* [How to Read and Write Files in Python](https://realpython.com/read-write-files-python/)\n",
    "* [Working with JSON Data in Python](https://realpython.com/python-json/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "qRvkeYEJIKsw",
    "outputId": "80e76bb8-0583-473b-ff9c-3459210e18fb"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location                                               text  \\\n",
       "0   1     NaN      NaN  Our Deeds are the Reason of this #earthquake M...   \n",
       "1   4     NaN      NaN             Forest fire near La Ronge Sask. Canada   \n",
       "2   5     NaN      NaN  All residents asked to 'shelter in place' are ...   \n",
       "3   6     NaN      NaN  13,000 people receive #wildfires evacuation or...   \n",
       "4   7     NaN      NaN  Just got sent this photo from Ruby #Alaska as ...   \n",
       "\n",
       "   target  \n",
       "0       1  \n",
       "1       1  \n",
       "2       1  \n",
       "3       1  \n",
       "4       1  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Turn .csv files into pandas DataFrame's\n",
    "import pandas as pd\n",
    "train_df = pd.read_csv(\"train.csv\")\n",
    "test_df = pd.read_csv(\"test.csv\")\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1xGqlnQaLmaT"
   },
   "source": [
    "The training data we downloaded is probably shuffled already. But just to be sure, let's shuffle it again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "ACCE7h6OMVjR",
    "outputId": "21bff309-9fc7-4ebd-ace1-24a20bb22099"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2644</th>\n",
       "      <td>3796</td>\n",
       "      <td>destruction</td>\n",
       "      <td>NaN</td>\n",
       "      <td>So you have a new weapon that can cause un-ima...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2227</th>\n",
       "      <td>3185</td>\n",
       "      <td>deluge</td>\n",
       "      <td>NaN</td>\n",
       "      <td>The f$&amp;amp;@ing things I do for #GISHWHES Just...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5448</th>\n",
       "      <td>7769</td>\n",
       "      <td>police</td>\n",
       "      <td>UK</td>\n",
       "      <td>DT @georgegalloway: RT @Galloway4Mayor: ÛÏThe...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>191</td>\n",
       "      <td>aftershock</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Aftershock back to school kick off was great. ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6845</th>\n",
       "      <td>9810</td>\n",
       "      <td>trauma</td>\n",
       "      <td>Montgomery County, MD</td>\n",
       "      <td>in response to trauma Children of Addicts deve...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id      keyword               location  \\\n",
       "2644  3796  destruction                    NaN   \n",
       "2227  3185       deluge                    NaN   \n",
       "5448  7769       police                     UK   \n",
       "132    191   aftershock                    NaN   \n",
       "6845  9810       trauma  Montgomery County, MD   \n",
       "\n",
       "                                                   text  target  \n",
       "2644  So you have a new weapon that can cause un-ima...       1  \n",
       "2227  The f$&amp;@ing things I do for #GISHWHES Just...       0  \n",
       "5448  DT @georgegalloway: RT @Galloway4Mayor: ÛÏThe...       1  \n",
       "132   Aftershock back to school kick off was great. ...       0  \n",
       "6845  in response to trauma Children of Addicts deve...       0  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Shuffle training dataframe\n",
    "train_df_shuffled = train_df.sample(frac=1, random_state=42) # shuffle with random_state=42 for reproducibility\n",
    "train_df_shuffled.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Lw4mKW1yL0kI"
   },
   "source": [
    "Notice how the training data has a `\"target\"` column.\n",
    "\n",
    "We're going to be writing code to find patterns (e.g. different combinations of words) in the `\"text\"` column of the training dataset to predict the value of the `\"target\"` column.\n",
    "\n",
    "The test dataset doesn't have a `\"target\"` column.\n",
    "\n",
    "```\n",
    "Inputs (text column) -> Machine Learning Algorithm -> Outputs (target column)\n",
    "```\n",
    "\n",
    "![](https://raw.githubusercontent.com/mrdbourke/tensorflow-deep-learning/main/images/08-text-classification-inputs-and-outputs.png)\n",
    "*Example text classification inputs and outputs for the problem of classifying whether a Tweet is about a diaster or not.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "tDh5t7thI5BM",
    "outputId": "5c5eaa7b-20a4-4ac1-a5d3-d5205321c46c"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just happened a terrible car crash</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Heard about #earthquake is different cities, s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>there is a forest fire at spot pond, geese are...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Apocalypse lighting. #Spokane #wildfires</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Typhoon Soudelor kills 28 in China and Taiwan</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location                                               text\n",
       "0   0     NaN      NaN                 Just happened a terrible car crash\n",
       "1   2     NaN      NaN  Heard about #earthquake is different cities, s...\n",
       "2   3     NaN      NaN  there is a forest fire at spot pond, geese are...\n",
       "3   9     NaN      NaN           Apocalypse lighting. #Spokane #wildfires\n",
       "4  11     NaN      NaN      Typhoon Soudelor kills 28 in China and Taiwan"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The test data doesn't have a target (that's what we'd try to predict)\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O4JhBRn5Mn-V"
   },
   "source": [
    "Let's check how many examples of each target we have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "k4P5DnLhIciD",
    "outputId": "c05bf503-253c-4215-befe-e7a1be840c74"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    4342\n",
       "1    3271\n",
       "Name: target, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# How many examples of each class?\n",
    "train_df.target.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WjEDQ297Ihy4"
   },
   "source": [
    "Since we have two target values, we're dealing with a **binary classification** problem.\n",
    "\n",
    "It's fairly balanced too, about 60% negative class (`target = 0`) and 40% positive class (`target = 1`).\n",
    "\n",
    "Where, \n",
    "\n",
    "* `1` = a real disaster Tweet\n",
    "* `0` = not a real disaster Tweet\n",
    "\n",
    "And what about the total number of samples we have?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jQxg7EKKIy5L",
    "outputId": "82239553-e4f3-4611-83f8-2669ad87e02f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total training samples: 7613\n",
      "Total test samples: 3263\n",
      "Total samples: 10876\n"
     ]
    }
   ],
   "source": [
    "# How many samples total?\n",
    "print(f\"Total training samples: {len(train_df)}\")\n",
    "print(f\"Total test samples: {len(test_df)}\")\n",
    "print(f\"Total samples: {len(train_df) + len(test_df)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q1upY8-xNPWV"
   },
   "source": [
    "Alright, seems like we've got a decent amount of training and test data. If anything, we've got an abundance of testing examples, usually a split of 90/10 (90% training, 10% testing) or 80/20 is suffice.\n",
    "\n",
    "Okay, time to visualize, let's write some code to visualize random text samples.\n",
    "\n",
    "> 🤔 **Question:** Why visualize random samples? You could visualize samples in order but this could lead to only seeing a certain subset of data. Better to visualize a substantial quantity (100+) of random samples to get an idea of the different kinds of data you're working with. In machine learning, never underestimate the power of randomness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vH3EXknTI3bQ",
    "outputId": "9b79fd3d-96a1-4296-9c61-85eeb0987f88"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target: 0 (not real disaster)\n",
      "Text:\n",
      "@TomcatArts 'who then were annihilated by the legion itself. The survivors of the imperfect hybrid project quickly formed a new secret cell\n",
      "\n",
      "---\n",
      "\n",
      "Target: 0 (not real disaster)\n",
      "Text:\n",
      "@sirtophamhat @SCynic1 @NafeezAhmed @jeremyduns and of course you don't have to melt the steel in order to cause structural failure.\n",
      "\n",
      "---\n",
      "\n",
      "Target: 0 (not real disaster)\n",
      "Text:\n",
      "@ryanoss123 No worries you'd have to be on every hitters most pitchers got destroyed\n",
      "\n",
      "---\n",
      "\n",
      "Target: 0 (not real disaster)\n",
      "Text:\n",
      "#PFT Barkevious Mingo missed Browns practice with a mystery injury http://t.co/D7m9KGMPJI\n",
      "\n",
      "---\n",
      "\n",
      "Target: 1 (real disaster)\n",
      "Text:\n",
      "Politics = Preschool Attitude: Russia orders to destroy all food coming from countries it doesn't like. - There is no hunger in the world?\n",
      "\n",
      "---\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Let's visualize some random training examples\n",
    "import random\n",
    "random_index = random.randint(0, len(train_df)-5) # create random indexes not higher than the total number of samples\n",
    "for row in train_df_shuffled[[\"text\", \"target\"]][random_index:random_index+5].itertuples():\n",
    "  _, text, target = row\n",
    "  print(f\"Target: {target}\", \"(real disaster)\" if target > 0 else \"(not real disaster)\")\n",
    "  print(f\"Text:\\n{text}\\n\")\n",
    "  print(\"---\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1FhRRewGPNS_"
   },
   "source": [
    "### Split data into training and validation sets\n",
    "\n",
    "Since the test set has no labels and we need a way to evalaute our trained models, we'll split off some of the training data and create a validation set.\n",
    "\n",
    "When our model trains (tries patterns in the Tweet samples), it'll only see data from the training set and we can see how it performs on unseen data using the validation set.\n",
    "\n",
    "We'll convert our splits from pandas Series datatypes to lists of strings (for the text) and lists of ints (for the labels) for ease of use later.\n",
    "\n",
    "To split our training dataset and create a validation dataset, we'll use Scikit-Learn's [`train_test_split()`](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html) method and dedicate 10% of the training samples to the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "7OJf31TQ-X8s"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Use train_test_split to split training data into training and validation sets\n",
    "train_sentences, val_sentences, train_labels, val_labels = train_test_split(train_df_shuffled[\"text\"].to_numpy(),\n",
    "                                                                            train_df_shuffled[\"target\"].to_numpy(),\n",
    "                                                                            test_size=0.1, # dedicate 10% of samples to validation set\n",
    "                                                                            random_state=42) # random state for reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NWGOTjanBaTQ",
    "outputId": "069bcbc7-dc1c-4833-ba00-532dbfac3246"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6851, 6851, 762, 762)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the lengths\n",
    "len(train_sentences), len(train_labels), len(val_sentences), len(val_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VqhvQK9wBTbw",
    "outputId": "24aea671-1711-46ce-87db-c31c3ffcc094"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array(['@mogacola @zamtriossu i screamed after hitting tweet',\n",
       "        'Imagine getting flattened by Kurt Zouma',\n",
       "        '@Gurmeetramrahim #MSGDoing111WelfareWorks Green S welfare force ke appx 65000 members har time disaster victim ki help ke liye tyar hai....',\n",
       "        \"@shakjn @C7 @Magnums im shaking in fear he's gonna hack the planet\",\n",
       "        'Somehow find you and I collide http://t.co/Ee8RpOahPk',\n",
       "        '@EvaHanderek @MarleyKnysh great times until the bus driver held us hostage in the mall parking lot lmfao',\n",
       "        'destroy the free fandom honestly',\n",
       "        'Weapons stolen from National Guard Armory in New Albany still missing #Gunsense http://t.co/lKNU8902JE',\n",
       "        '@wfaaweather Pete when will the heat wave pass? Is it really going to be mid month? Frisco Boy Scouts have a canoe trip in Okla.',\n",
       "        'Patient-reported outcomes in long-term survivors of metastatic colorectal cancer - British Journal of Surgery http://t.co/5Yl4DC1Tqt'],\n",
       "       dtype=object),\n",
       " array([0, 0, 1, 0, 0, 1, 1, 0, 1, 1]))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# View the first 10 training sentences and their labels\n",
    "train_sentences[:10], train_labels[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EN-houoSD-hP"
   },
   "source": [
    "## Converting text into numbers\n",
    "\n",
    "Wonderful! We've got a training set and a validation set containing Tweets and labels.\n",
    "\n",
    "Our labels are in numerical form (`0` and `1`) but our Tweets are in string form.\n",
    "\n",
    "> 🤔 **Question:** What do you think we have to do before we can use a machine learning algorithm with our text data? \n",
    "\n",
    "If you answered something along the lines of \"turn it into numbers\", you're correct. A machine learning algorithm requires its inputs to be in numerical form.\n",
    "\n",
    "In NLP, there are two main concepts for turning text into numbers:\n",
    "* **Tokenization** - A straight mapping from word or character or sub-word to a numerical value. There are three main levels of tokenization:\n",
    "  1. Using **word-level tokenization** with the sentence \"I love TensorFlow\" might result in \"I\" being `0`, \"love\" being `1` and \"TensorFlow\" being `2`. In this case, every word in a sequence considered a single **token**.\n",
    "  2. **Character-level tokenization**, such as converting the letters A-Z to values `1-26`. In this case, every character in a sequence considered a single **token**.\n",
    "  3. **Sub-word tokenization** is in between word-level and character-level tokenization. It involves breaking invidual words into smaller parts and then converting those smaller parts into numbers. For example, \"my favourite food is pineapple pizza\" might become \"my, fav, avour, rite, fo, oo, od, is, pin, ine, app, le, piz, za\". After doing this, these sub-words would then be mapped to a numerical value. In this case, every word could be considered multiple **tokens**.\n",
    "* **Embeddings** - An embedding is a representation of natural language which can be learned. Representation comes in the form of a **feature vector**. For example, the word \"dance\" could be represented by the 5-dimensional vector `[-0.8547, 0.4559, -0.3332, 0.9877, 0.1112]`. It's important to note here, the size of the feature vector is tuneable. There are two ways to use embeddings: \n",
    "  1. **Create your own embedding** - Once your text has been turned into numbers (required for an embedding), you can put them through an embedding layer (such as [`tf.keras.layers.Embedding`](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Embedding)) and an embedding representation will be learned during model training.\n",
    "  2. **Reuse a pre-learned embedding** - Many pre-trained embeddings exist online. These pre-trained embeddings have often been learned on large corpuses of text (such as all of Wikipedia) and thus have a good underlying representation of natural language. You can use a pre-trained embedding to initialize your model and fine-tune it to your own specific task.\n",
    "\n",
    "![](https://raw.githubusercontent.com/mrdbourke/tensorflow-deep-learning/main/images/08-tokenization-vs-embedding.png)\n",
    "*Example of **tokenization** (straight mapping from word to number) and **embedding** (richer representation of relationships between tokens).*\n",
    "\n",
    "> 🤔 **Question:** What level of tokenzation should I use? What embedding should should I choose?\n",
    "\n",
    "It depends on your problem. You could try character-level tokenization/embeddings and word-level tokenization/embeddings and see which perform best. You might even want to try stacking them (e.g. combining the outputs of your embedding layers using [`tf.keras.layers.concatenate`](https://www.tensorflow.org/api_docs/python/tf/keras/layers/concatenate)). \n",
    "\n",
    "If you're looking for pre-trained word embeddings, [Word2vec embeddings](http://jalammar.github.io/illustrated-word2vec/), [GloVe embeddings](https://nlp.stanford.edu/projects/glove/) and many of the options available on [TensorFlow Hub](https://tfhub.dev/s?module-type=text-embedding) are great places to start.\n",
    "\n",
    "> 🔑 **Note:** Much like searching for a pre-trained computer vision model, you can search for pre-trained word embeddings to use for your problem. Try searching for something like \"use pre-trained word embeddings in TensorFlow\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8UnRcM1PELHn"
   },
   "source": [
    "### Text vectorization (tokenization)\n",
    "\n",
    "Enough talking about tokenization and embeddings, let's create some.\n",
    "\n",
    "We'll practice tokenzation (mapping our words to numbers) first.\n",
    "\n",
    "To tokenize our words, we'll use the helpful preprocessing layer [`tf.keras.layers.experimental.preprocessing.TextVectorization`](https://www.tensorflow.org/api_docs/python/tf/keras/layers/experimental/preprocessing/TextVectorization).\n",
    "\n",
    "The `TextVectorization` layer takes the following parameters:\n",
    "* `max_tokens` - The maximum number of words in your vocabulary (e.g. 20000 or the number of unique words in your text), includes a value for OOV (out of vocabulary) tokens. \n",
    "* `standardize` - Method for standardizing text. Default is `\"lower_and_strip_punctuation\"` which lowers text and removes all punctuation marks.\n",
    "* `split` - How to split text, default is `\"whitespace\"` which splits on spaces.\n",
    "* `ngrams` - How many words to contain per token split, for example, `ngrams=2` splits tokens into continuous sequences of 2.\n",
    "* `output_mode` -  How to output tokens, can be `\"int\"` (integer mapping), `\"binary\"` (one-hot encoding), `\"count\"` or `\"tf-idf\"`. See documentation for more.\n",
    "* `output_sequence_length` - Length of tokenized sequence to output. For example, if `output_sequence_length=150`, all tokenized sequences will be 150 tokens long.\n",
    "* `pad_to_max_tokens` - Defaults to `False`, if `True`, the output feature axis will be padded to `max_tokens` even if the number of unique tokens in the vocabulary is less than `max_tokens`. Only valid in certain modes, see docs for more.\n",
    "\n",
    "Let's see it in action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "PVcZk-LcNunF"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-22 17:27:00.648693: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2022-08-22 17:27:00.657295: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcuda.so.1\n",
      "2022-08-22 17:27:00.736286: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-08-22 17:27:00.736628: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: \n",
      "pciBusID: 0000:01:00.0 name: GeForce GTX 970 computeCapability: 5.2\n",
      "coreClock: 1.367GHz coreCount: 13 deviceMemorySize: 3.94GiB deviceMemoryBandwidth: 208.91GiB/s\n",
      "2022-08-22 17:27:00.736653: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n",
      "2022-08-22 17:27:00.774795: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.10\n",
      "2022-08-22 17:27:00.774892: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.10\n",
      "2022-08-22 17:27:00.795062: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10\n",
      "2022-08-22 17:27:00.800600: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10\n",
      "2022-08-22 17:27:00.839896: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10\n",
      "2022-08-22 17:27:00.845322: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.10\n",
      "2022-08-22 17:27:00.916745: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.7\n",
      "2022-08-22 17:27:00.916938: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-08-22 17:27:00.917607: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-08-22 17:27:00.918165: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0\n",
      "2022-08-22 17:27:00.918654: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-08-22 17:27:00.919017: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2022-08-22 17:27:00.919170: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-08-22 17:27:00.919715: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: \n",
      "pciBusID: 0000:01:00.0 name: GeForce GTX 970 computeCapability: 5.2\n",
      "coreClock: 1.367GHz coreCount: 13 deviceMemorySize: 3.94GiB deviceMemoryBandwidth: 208.91GiB/s\n",
      "2022-08-22 17:27:00.919741: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n",
      "2022-08-22 17:27:00.919775: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.10\n",
      "2022-08-22 17:27:00.919797: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.10\n",
      "2022-08-22 17:27:00.919819: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10\n",
      "2022-08-22 17:27:00.919839: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10\n",
      "2022-08-22 17:27:00.919859: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10\n",
      "2022-08-22 17:27:00.919889: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.10\n",
      "2022-08-22 17:27:00.919909: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.7\n",
      "2022-08-22 17:27:00.919983: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-08-22 17:27:00.920538: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-08-22 17:27:00.920959: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0\n",
      "2022-08-22 17:27:00.921475: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n",
      "2022-08-22 17:27:02.050790: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1261] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
      "2022-08-22 17:27:02.050814: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1267]      0 \n",
      "2022-08-22 17:27:02.050819: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1280] 0:   N \n",
      "2022-08-22 17:27:02.051569: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-08-22 17:27:02.051973: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-08-22 17:27:02.052313: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-08-22 17:27:02.052567: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1406] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 2881 MB memory) -> physical GPU (device: 0, name: GeForce GTX 970, pci bus id: 0000:01:00.0, compute capability: 5.2)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n",
    "# Note: in TensorFlow 2.6+, you no longer need \"layers.experimental.preprocessing\"\n",
    "# you can use: \"tf.keras.layers.TextVectorization\", see https://github.com/tensorflow/tensorflow/releases/tag/v2.6.0 for more\n",
    "\n",
    "# Use the default TextVectorization variables\n",
    "text_vectorizer = TextVectorization(max_tokens=None, # how many words in the vocabulary (all of the different words in your text)\n",
    "                                    standardize=\"lower_and_strip_punctuation\", # how to process text\n",
    "                                    split=\"whitespace\", # how to split tokens\n",
    "                                    ngrams=None, # create groups of n-words?\n",
    "                                    output_mode=\"int\", # how to map tokens to numbers\n",
    "                                    output_sequence_length=None) # how long should the output sequence of tokens be?\n",
    "                                    # pad_to_max_tokens=True) # Not valid if using max_tokens=None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u0Ej5mzKGkK8"
   },
   "source": [
    "We've initialized a `TextVectorization` object with the default settings but let's customize it a little bit for our own use case.\n",
    "\n",
    "In particular, let's set values for `max_tokens` and `output_sequence_length`.\n",
    "\n",
    "For `max_tokens` (the number of words in the vocabulary), multiples of 10,000 (`10,000`, `20,000`, `30,000`) or the exact number of unique words in your text (e.g. `32,179`) are common values.\n",
    "\n",
    "For our use case, we'll use `10,000`.\n",
    "\n",
    "And for the `output_sequence_length` we'll use the average number of tokens per Tweet in the training set. But first, we'll need to find it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SQ3ZCINnR56H",
    "outputId": "66cffaa1-bdc0-4a94-c829-9176601bb34b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find average number of tokens (words) in training Tweets\n",
    "round(sum([len(i.split()) for i in train_sentences])/len(train_sentences))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AFGTRcw8Hv7R"
   },
   "source": [
    "Now let's create another `TextVectorization` object using our custom parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "eYPcGwdbafmW"
   },
   "outputs": [],
   "source": [
    "# Setup text vectorization with custom variables\n",
    "max_vocab_length = 10000 # max number of words to have in our vocabulary\n",
    "max_length = 15 # max length our sequences will be (e.g. how many words from a Tweet does our model see?)\n",
    "\n",
    "text_vectorizer = TextVectorization(max_tokens=max_vocab_length,\n",
    "                                    output_mode=\"int\",\n",
    "                                    output_sequence_length=max_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BSWycfB3H3wV"
   },
   "source": [
    "Beautiful!\n",
    "\n",
    "To map our `TextVectorization` instance `text_vectorizer` to our data, we can call the `adapt()` method on it whilst passing it our training text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "0083KHXPO4m2"
   },
   "outputs": [],
   "source": [
    "# Fit the text vectorizer to the training text\n",
    "text_vectorizer.adapt(train_sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Syh0VB9wIHUq"
   },
   "source": [
    "Training data mapped! Let's try our `text_vectorizer` on a custom sentence (one similar to what you might see in the training data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uizmdJKvO2OW",
    "outputId": "543ea3ef-1ee8-4194-a699-11ca070c2527"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 15), dtype=int64, numpy=\n",
       "array([[264,   3, 232,   4,  13, 698,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0]])>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create sample sentence and tokenize it\n",
    "sample_sentence = \"There's a flood in my street!\"\n",
    "text_vectorizer([sample_sentence])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M0RmAeplIW57"
   },
   "source": [
    "Wonderful, it seems we've got a way to turn our text into numbers (in this case, word-level tokenization). Notice the 0's at the end of the returned tensor, this is because we set `output_sequence_length=15`, meaning no matter the size of the sequence we pass to `text_vectorizer`, it always returns a sequence with a length of 15.\n",
    "\n",
    "How about we try our `text_vectorizer` on a few random sentences?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SZFka4BtRR6_",
    "outputId": "c0e1e0c9-2449-4873-e11a-f9b7027b1c21"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original text:\n",
      "U.S National Park Services Tonto National Forest: Stop the Annihilation of the Salt River Wild Horse... http://t.co/SB5R7ShcCJ via @Change      \n",
      "\n",
      "Vectorized version:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 15), dtype=int64, numpy=\n",
       "array([[  69,  372, 1001,  327, 1578,  372,  188,  240,    2,  796,    6,\n",
       "           2, 1180,  499,  250]])>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Choose a random sentence from the training dataset and tokenize it\n",
    "random_sentence = random.choice(train_sentences)\n",
    "print(f\"Original text:\\n{random_sentence}\\\n",
    "      \\n\\nVectorized version:\")\n",
    "text_vectorizer([random_sentence])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PErGKRbPJF89"
   },
   "source": [
    "Looking good!\n",
    "\n",
    "Finally, we can check the unique tokens in our vocabulary using the `get_vocabulary()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5nwNdgAZIhna",
    "outputId": "2953be64-e2b2-4ab5-c82f-d05e261fc539"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words in vocab: 10000\n",
      "Top 5 most common words: ['', '[UNK]', 'the', 'a', 'in']\n",
      "Bottom 5 least common words: ['pages', 'paeds', 'pads', 'padres', 'paddytomlinson1']\n"
     ]
    }
   ],
   "source": [
    "# Get the unique words in the vocabulary\n",
    "words_in_vocab = text_vectorizer.get_vocabulary()\n",
    "top_5_words = words_in_vocab[:5] # most common tokens (notice the [UNK] token for \"unknown\" words)\n",
    "bottom_5_words = words_in_vocab[-5:] # least common tokens\n",
    "print(f\"Number of words in vocab: {len(words_in_vocab)}\")\n",
    "print(f\"Top 5 most common words: {top_5_words}\") \n",
    "print(f\"Bottom 5 least common words: {bottom_5_words}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AHyCdO0uEOkH"
   },
   "source": [
    "### Creating an Embedding using an Embedding Layer\n",
    "\n",
    "We've got a way to map our text to numbers. How about we go a step further and turn those numbers into an embedding?\n",
    "\n",
    "The powerful thing about an embedding is it can be learned during training. This means rather than just being static (e.g. `1` = I, `2` = love, `3` = TensorFlow), a word's numeric representation can be improved as a model goes through data samples.\n",
    "\n",
    "We can see what an embedding of a word looks like by using the [`tf.keras.layers.Embedding`](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Embedding) layer. \n",
    "\n",
    "The main parameters we're concerned about here are:\n",
    "* `input_dim` - The size of the vocabulary (e.g. `len(text_vectorizer.get_vocabulary()`).\n",
    "* `output_dim` - The size of the output embedding vector, for example, a value of `100` outputs a  feature vector of size 100 for each word.\n",
    "* `embeddings_initializer` - How to initialize the embeddings matrix, default is `\"uniform\"` which randomly initalizes embedding matrix with uniform distribution. This can be changed for using pre-learned embeddings.\n",
    "* `input_length` - Length of sequences being passed to embedding layer.\n",
    "\n",
    "Knowing these, let's make an embedding layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OsB4StymSk_s",
    "outputId": "e93ad65f-777f-4409-e79c-48c9a1088a51"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.layers.embeddings.Embedding at 0x7f458daec2d0>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.random.set_seed(42)\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "embedding = layers.Embedding(input_dim=max_vocab_length, # set input shape\n",
    "                             output_dim=128, # set size of embedding vector\n",
    "                             embeddings_initializer=\"uniform\", # default, intialize randomly\n",
    "                             input_length=max_length, # how long is each input\n",
    "                             name=\"embedding_1\") \n",
    "\n",
    "embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bfML_IzlSUho"
   },
   "source": [
    "Excellent, notice how `embedding` is a TensoFlow layer? This is important because we can use it as part of a model, meaning its parameters (word representations) can be updated and improved as the model learns.\n",
    "\n",
    "How about we try it out on a sample sentence?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1Re6Eew6SZnG",
    "outputId": "5166bf99-4ff6-4d39-e221-324aa38b7bd4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original text:\n",
      "Sign the petition @david_cameron to protect bees instead of toxic chemical companies want to harm them! #savebees  - http://t.co/dB7ft3Yi6d      \n",
      "\n",
      "Embedded version:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 15, 128), dtype=float32, numpy=\n",
       "array([[[-0.03595455,  0.04682304, -0.00137482, ...,  0.04818661,\n",
       "          0.0481248 ,  0.04043898],\n",
       "        [ 0.02504804,  0.02770906,  0.01542609, ..., -0.04612003,\n",
       "         -0.02924401, -0.00331711],\n",
       "        [ 0.04168225, -0.01178509,  0.03455076, ...,  0.04030031,\n",
       "         -0.0188342 ,  0.03132861],\n",
       "        ...,\n",
       "        [-0.03856696, -0.03503034,  0.00736847, ..., -0.00099922,\n",
       "          0.00545276, -0.03953941],\n",
       "        [-0.03521683, -0.00981399, -0.00284491, ..., -0.04792733,\n",
       "         -0.01664252, -0.03293632],\n",
       "        [ 0.03670504, -0.00255904, -0.04655578, ..., -0.03427716,\n",
       "          0.02346648, -0.0208092 ]]], dtype=float32)>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get a random sentence from training set\n",
    "random_sentence = random.choice(train_sentences)\n",
    "print(f\"Original text:\\n{random_sentence}\\\n",
    "      \\n\\nEmbedded version:\")\n",
    "\n",
    "# Embed the random sentence (turn it into numerical representation)\n",
    "sample_embed = embedding(text_vectorizer([random_sentence]))\n",
    "sample_embed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e4Sn8o9pTBE5"
   },
   "source": [
    "Each token in the sentence gets turned into a length 128 feature vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "g_VBepuSTBDW",
    "outputId": "c3583950-f6dd-4516-d256-32a3092fea63"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(128,), dtype=float32, numpy=\n",
       "array([-0.03595455,  0.04682304, -0.00137482,  0.00170907, -0.00343113,\n",
       "        0.01844882, -0.02878734,  0.00424693,  0.03781022,  0.03186769,\n",
       "        0.02727688, -0.04279611,  0.00071875, -0.00254018,  0.04644595,\n",
       "       -0.04668961,  0.03942177, -0.02216952,  0.02868328,  0.03084049,\n",
       "       -0.0030229 , -0.0101627 ,  0.02486363, -0.03910517,  0.00908475,\n",
       "        0.02492665, -0.03595623,  0.03514603,  0.00184511,  0.04524601,\n",
       "        0.0280165 , -0.02441767,  0.01226504,  0.02146456, -0.00013439,\n",
       "        0.01497828, -0.04361645, -0.02413671, -0.01818156,  0.01844336,\n",
       "       -0.01657463,  0.01193291, -0.04159908,  0.03197839, -0.04692775,\n",
       "        0.02531082,  0.02999536,  0.00725546,  0.00972291,  0.03766352,\n",
       "       -0.00173335, -0.01270398,  0.03797093, -0.00244574, -0.0382048 ,\n",
       "       -0.03871272, -0.01033269,  0.00893563,  0.03931471, -0.0025013 ,\n",
       "       -0.01763197, -0.00681294,  0.01896712, -0.02079537,  0.04463801,\n",
       "       -0.01415546, -0.00304586,  0.02706685, -0.03587158,  0.0054546 ,\n",
       "       -0.01827102, -0.01831863,  0.02665807,  0.00598736, -0.04645128,\n",
       "       -0.04337672, -0.03118955, -0.02022572, -0.0103111 , -0.02196546,\n",
       "        0.02502174,  0.01951249, -0.02548628, -0.0141063 , -0.01114599,\n",
       "        0.03306608,  0.0187814 , -0.00448547, -0.02071407, -0.00384108,\n",
       "       -0.03101018,  0.04512549,  0.04435836,  0.0167272 , -0.0164824 ,\n",
       "       -0.01508171, -0.04503691,  0.00905095,  0.003537  , -0.01855898,\n",
       "        0.01877875,  0.01855116, -0.00810031, -0.02233317,  0.00731716,\n",
       "        0.01465951, -0.0228981 ,  0.00883083, -0.01108821,  0.01122264,\n",
       "        0.02894041, -0.01869463,  0.00905628, -0.01941934, -0.00453681,\n",
       "        0.03640833,  0.04501159,  0.03188956,  0.0145599 ,  0.04244098,\n",
       "       -0.01085069, -0.03939309,  0.04165092,  0.00620221,  0.00018927,\n",
       "        0.04818661,  0.0481248 ,  0.04043898], dtype=float32)>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check out a single token's embedding\n",
    "sample_embed[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z0NTsDklR0xw"
   },
   "source": [
    "These values might not mean much to us but they're what our computer sees each word as. When our model looks for patterns in different samples, these values will be updated as necessary.\n",
    "\n",
    "> 🔑 **Note:** The previous two concepts (tokenization and embeddings) are the foundation for many NLP tasks. So if you're not sure about anything, be sure to research and conduct your own experiments to further help your understanding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZJENUdF3F7Rn"
   },
   "source": [
    "## Modelling a text dataset\n",
    "\n",
    "![](https://raw.githubusercontent.com/mrdbourke/tensorflow-deep-learning/main/images/08-inputs-and-outputs-with-shapes-and-models-were-going-to-build.png)\n",
    "*Once you've got your inputs and outputs prepared, it's a matter of figuring out which machine learning model to build in between them to bridge the gap.*\n",
    "\n",
    "Now that we've got a way to turn our text data into numbers, we can start to build machine learning models to model it.\n",
    "\n",
    "To get plenty of practice, we're going to build a series of different models, each as its own experiment. We'll then compare the results of each model and see which one performed best.\n",
    "\n",
    "More specifically, we'll be building the following:\n",
    "* **Model 0**: Naive Bayes (baseline)\n",
    "* **Model 1**: Feed-forward neural network (dense model)\n",
    "* **Model 2**: LSTM model\n",
    "* **Model 3**: GRU model\n",
    "* **Model 4**: Bidirectional-LSTM model\n",
    "* **Model 5**: 1D Convolutional Neural Network\n",
    "* **Model 6**: TensorFlow Hub Pretrained Feature Extractor\n",
    "* **Model 7**: Same as model 6 with 10% of training data\n",
    "\n",
    "Model 0 is the simplest to acquire a baseline which we'll expect each other of the other deeper models to beat.\n",
    "\n",
    "Each experiment will go through the following steps:\n",
    "* Construct the model\n",
    "* Train the model\n",
    "* Make predictions with the model\n",
    "* Track prediction evaluation metrics for later comparison\n",
    "\n",
    "Let's get started."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q4i5BiQfF--y"
   },
   "source": [
    "### Model 0: Getting a baseline\n",
    "\n",
    "As with all machine learning modelling experiments, it's important to create a baseline model so you've got a benchmark for future experiments to build upon.\n",
    "\n",
    "To create our baseline, we'll create a Scikit-Learn Pipeline using the TF-IDF (term frequency-inverse document frequency) formula to convert our words to numbers and then model them with the [Multinomial Naive Bayes algorithm](https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.MultinomialNB.html#sklearn.naive_bayes.MultinomialNB). This was chosen via referring to the [Scikit-Learn machine learning map](https://scikit-learn.org/stable/tutorial/machine_learning_map/index.html).\n",
    "\n",
    "> 📖 **Reading:** The ins and outs of TF-IDF algorithm is beyond the scope of this notebook, however, the curious reader is encouraged to check out the [Scikit-Learn documentation for more](https://scikit-learn.org/stable/modules/feature_extraction.html#tfidf-term-weighting)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xFqjqWcXtOOs",
    "outputId": "6d7dc11d-2747-42fa-db6c-6a48e9da5c32"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('tfidf', TfidfVectorizer()), ('clf', MultinomialNB())])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Create tokenization and modelling pipeline\n",
    "model_0 = Pipeline([\n",
    "                    (\"tfidf\", TfidfVectorizer()), # convert words to numbers using tfidf\n",
    "                    (\"clf\", MultinomialNB()) # model the text\n",
    "])\n",
    "\n",
    "# Fit the pipeline to the training data\n",
    "model_0.fit(train_sentences, train_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ybOvOuVJbNjg"
   },
   "source": [
    "The benefit of using a shallow model like Multinomial Naive Bayes is that training is very fast.\n",
    "\n",
    "Let's evaluate our model and find our baseline metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "soPfnpmQuUIP",
    "outputId": "e5f38a5c-ba91-4ec0-95b0-de75a6e8541e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our baseline model achieves an accuracy of: 79.27%\n"
     ]
    }
   ],
   "source": [
    "baseline_score = model_0.score(val_sentences, val_labels)\n",
    "print(f\"Our baseline model achieves an accuracy of: {baseline_score*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hUv5dyuibf3M"
   },
   "source": [
    "How about we make some predictions with our baseline model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7n89JxrJufcf",
    "outputId": "fd5238cd-d178-4c01-b4bd-e545c5365a66"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make predictions\n",
    "baseline_preds = model_0.predict(val_sentences)\n",
    "baseline_preds[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K354svk_bmdf"
   },
   "source": [
    "### Creating an evaluation function for our model experiments\n",
    "\n",
    "We could evaluate these as they are but since we're going to be evaluating several models in the same way going forward, let's create a helper function which takes an array of predictions and ground truth labels and computes the following:\n",
    "* Accuracy\n",
    "* Precision\n",
    "* Recall\n",
    "* F1-score\n",
    "\n",
    "> 🔑 **Note:** Since we're dealing with a classification problem, the above metrics are the most appropriate. If we were working with a regression problem, other metrics such as MAE (mean absolute error) would be a better choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "gLmNlDjIxGgJ"
   },
   "outputs": [],
   "source": [
    "# Function to evaluate: accuracy, precision, recall, f1-score\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "\n",
    "def calculate_results(y_true, y_pred):\n",
    "  \"\"\"\n",
    "  Calculates model accuracy, precision, recall and f1 score of a binary classification model.\n",
    "\n",
    "  Args:\n",
    "  -----\n",
    "  y_true = true labels in the form of a 1D array\n",
    "  y_pred = predicted labels in the form of a 1D array\n",
    "\n",
    "  Returns a dictionary of accuracy, precision, recall, f1-score.\n",
    "  \"\"\"\n",
    "  # Calculate model accuracy\n",
    "  model_accuracy = accuracy_score(y_true, y_pred) * 100\n",
    "  # Calculate model precision, recall and f1 score using \"weighted\" average\n",
    "  model_precision, model_recall, model_f1, _ = precision_recall_fscore_support(y_true, y_pred, average=\"weighted\")\n",
    "  model_results = {\"accuracy\": model_accuracy,\n",
    "                  \"precision\": model_precision,\n",
    "                  \"recall\": model_recall,\n",
    "                  \"f1\": model_f1}\n",
    "  return model_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Sgy1omMhwr52",
    "outputId": "7e16a9d2-fcc6-400e-e12b-4b46796c4762"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 79.26509186351706,\n",
       " 'precision': 0.8111390004213173,\n",
       " 'recall': 0.7926509186351706,\n",
       " 'f1': 0.7862189758049549}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get baseline results\n",
    "baseline_results = calculate_results(y_true=val_labels,\n",
    "                                     y_pred=baseline_preds)\n",
    "baseline_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "noRJNm7dGNyh"
   },
   "source": [
    "### Model 1: A simple dense model\n",
    "\n",
    "The first \"deep\" model we're going to build is a single layer dense model. In fact, it's barely going to have a single layer. \n",
    "\n",
    "It'll take our text and labels as input, tokenize the text, create an embedding, find the average of the embedding (using Global Average Pooling) and then pass the average through a fully connected layer with one output unit and a sigmoid activation function.\n",
    "\n",
    "If the previous sentence sounds like a mouthful, it'll make sense when we code it out (remember, if in doubt, code it out).\n",
    "\n",
    "And since we're going to be building a number of TensorFlow deep learning models, we'll import our `create_tensorboard_callback()` function from `helper_functions.py` to keep track of the results of each. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "PVMPUd3HTit5"
   },
   "outputs": [],
   "source": [
    "# Create tensorboard callback (need to create a new one for each model)\n",
    "from helper_functions import create_tensorboard_callback\n",
    "\n",
    "# Create directory to save TensorBoard logs\n",
    "SAVE_DIR = \"model_logs\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Pib8hHtu7vt1"
   },
   "source": [
    "Now we've got a TensorBoard callback function ready to go, let's build our first deep model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "a_rVtJA7yVBI"
   },
   "outputs": [],
   "source": [
    "# Build model with the Functional API\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "inputs = layers.Input(shape=(1,), dtype=tf.string) # inputs are 1-dimensional strings\n",
    "\n",
    "x = text_vectorizer(inputs) # turn the input text into numbers\n",
    "x = embedding(x) # create an embedding of the numerized numbers\n",
    "x = layers.GlobalAveragePooling1D()(x) # lower the dimensionality of the embedding (try running the model without this layer and see what happens)\n",
    "\n",
    "outputs = layers.Dense(1, activation=\"sigmoid\")(x) # create the output layer, want binary outputs so use sigmoid activation\n",
    "\n",
    "model_1 = tf.keras.Model(inputs, outputs, name=\"model_1_dense\") # construct the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JYzsu36Y8JUe"
   },
   "source": [
    "Looking good. Our model takes a 1-dimensional string as input (in our case, a Tweet), it then tokenizes the string using `text_vectorizer` and creates an embedding using `embedding`.\n",
    "\n",
    "We then (optionally) pool the outputs of the embedding layer to reduce the dimensionality of the tensor we pass to the output layer.\n",
    "\n",
    "> 🛠 **Exercise:** Try building `model_1` with and without a `GlobalAveragePooling1D()` layer after the `embedding` layer. What happens? Why do you think this is?\n",
    "\n",
    "Finally, we pass the output of the pooling layer to a dense layer with sigmoid activation (we use sigmoid since our problem is binary classification).\n",
    "\n",
    "Before we can fit our model to the data, we've got to compile it. Since we're working with binary classification, we'll use `\"binary_crossentropy\"` as our loss function and the Adam optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "Ubq0ctLD8CQq"
   },
   "outputs": [],
   "source": [
    "# Compile model\n",
    "model_1.compile(loss=\"binary_crossentropy\",\n",
    "                optimizer=tf.keras.optimizers.Adam(),\n",
    "                metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "crgltz1O9uku"
   },
   "source": [
    "Model compiled. Let's get a summary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QkJa-t8aTw1H",
    "outputId": "638ed27e-d4e3-4023-93ce-21261a543377"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1_dense\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 1)]               0         \n",
      "_________________________________________________________________\n",
      "text_vectorization_1 (TextVe (None, 15)                0         \n",
      "_________________________________________________________________\n",
      "embedding_1 (Embedding)      (None, 15, 128)           1280000   \n",
      "_________________________________________________________________\n",
      "global_average_pooling1d (Gl (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 1,280,129\n",
      "Trainable params: 1,280,129\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Get a summary of the model\n",
    "model_1.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bH0JLyR09yYt"
   },
   "source": [
    "Most of the trainable parameters are contained within the embedding layer. Recall we created an embedding of size 128 (`output_dim=128`) for a vocabulary of size 10,000 (`input_dim=10000`), hence the 1,280,000 trainable parameters.\n",
    "\n",
    "Alright, our model is compiled, let's fit it to our training data for 5 epochs. We'll also pass our TensorBoard callback function to make sure our model's training metrics are logged."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1YRYpJIfTvHV",
    "outputId": "8c4d707f-83d8-4f64-dca5-6893e55cf278"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving TensorBoard log files to: model_logs/simple_dense_model/20220822-172702\n",
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-22 17:27:02.705624: I tensorflow/core/profiler/lib/profiler_session.cc:136] Profiler session initializing.\n",
      "2022-08-22 17:27:02.705649: I tensorflow/core/profiler/lib/profiler_session.cc:155] Profiler session started.\n",
      "2022-08-22 17:27:02.706350: I tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1365] Profiler found 1 GPUs\n",
      "2022-08-22 17:27:02.716736: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcupti.so.10.1\n",
      "2022-08-22 17:27:02.810039: I tensorflow/core/profiler/lib/profiler_session.cc:172] Profiler session tear down.\n",
      "2022-08-22 17:27:02.810185: I tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1487] CUPTI activity buffer flushed\n",
      "2022-08-22 17:27:02.842313: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2022-08-22 17:27:02.867175: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 3497805000 Hz\n",
      "2022-08-22 17:27:03.473342: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 13/215 [>.............................] - ETA: 3s - loss: 0.6912 - accuracy: 0.5421"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-22 17:27:03.891953: I tensorflow/core/profiler/lib/profiler_session.cc:136] Profiler session initializing.\n",
      "2022-08-22 17:27:03.891982: I tensorflow/core/profiler/lib/profiler_session.cc:155] Profiler session started.\n",
      "2022-08-22 17:27:03.914535: I tensorflow/core/profiler/lib/profiler_session.cc:71] Profiler session collecting data.\n",
      "2022-08-22 17:27:03.914733: I tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1487] CUPTI activity buffer flushed\n",
      "2022-08-22 17:27:03.916108: I tensorflow/core/profiler/internal/gpu/cupti_collector.cc:228]  GpuTracer has collected 120 callback api events and 108 activity events. \n",
      "2022-08-22 17:27:03.919404: I tensorflow/core/profiler/lib/profiler_session.cc:172] Profiler session tear down.\n",
      "2022-08-22 17:27:03.925212: I tensorflow/core/profiler/rpc/client/save_profile.cc:137] Creating directory: model_logs/simple_dense_model/20220822-172702/train/plugins/profile/2022_08_22_17_27_03\n",
      "2022-08-22 17:27:03.927822: I tensorflow/core/profiler/rpc/client/save_profile.cc:143] Dumped gzipped tool data for trace.json.gz to model_logs/simple_dense_model/20220822-172702/train/plugins/profile/2022_08_22_17_27_03/ubuntu-All-Series.trace.json.gz\n",
      "2022-08-22 17:27:03.936839: I tensorflow/core/profiler/rpc/client/save_profile.cc:137] Creating directory: model_logs/simple_dense_model/20220822-172702/train/plugins/profile/2022_08_22_17_27_03\n",
      "2022-08-22 17:27:03.938165: I tensorflow/core/profiler/rpc/client/save_profile.cc:143] Dumped gzipped tool data for memory_profile.json.gz to model_logs/simple_dense_model/20220822-172702/train/plugins/profile/2022_08_22_17_27_03/ubuntu-All-Series.memory_profile.json.gz\n",
      "2022-08-22 17:27:03.938569: I tensorflow/core/profiler/rpc/client/capture_profile.cc:251] Creating directory: model_logs/simple_dense_model/20220822-172702/train/plugins/profile/2022_08_22_17_27_03Dumped tool data for xplane.pb to model_logs/simple_dense_model/20220822-172702/train/plugins/profile/2022_08_22_17_27_03/ubuntu-All-Series.xplane.pb\n",
      "Dumped tool data for overview_page.pb to model_logs/simple_dense_model/20220822-172702/train/plugins/profile/2022_08_22_17_27_03/ubuntu-All-Series.overview_page.pb\n",
      "Dumped tool data for input_pipeline.pb to model_logs/simple_dense_model/20220822-172702/train/plugins/profile/2022_08_22_17_27_03/ubuntu-All-Series.input_pipeline.pb\n",
      "Dumped tool data for tensorflow_stats.pb to model_logs/simple_dense_model/20220822-172702/train/plugins/profile/2022_08_22_17_27_03/ubuntu-All-Series.tensorflow_stats.pb\n",
      "Dumped tool data for kernel_stats.pb to model_logs/simple_dense_model/20220822-172702/train/plugins/profile/2022_08_22_17_27_03/ubuntu-All-Series.kernel_stats.pb\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "215/215 [==============================] - 4s 14ms/step - loss: 0.6526 - accuracy: 0.6392 - val_loss: 0.5357 - val_accuracy: 0.7572\n",
      "Epoch 2/5\n",
      "215/215 [==============================] - 3s 13ms/step - loss: 0.4600 - accuracy: 0.8190 - val_loss: 0.4691 - val_accuracy: 0.7848\n",
      "Epoch 3/5\n",
      "215/215 [==============================] - 3s 12ms/step - loss: 0.3535 - accuracy: 0.8612 - val_loss: 0.4590 - val_accuracy: 0.7900\n",
      "Epoch 4/5\n",
      "215/215 [==============================] - 3s 12ms/step - loss: 0.2831 - accuracy: 0.8885 - val_loss: 0.4641 - val_accuracy: 0.7927\n",
      "Epoch 5/5\n",
      "215/215 [==============================] - 3s 12ms/step - loss: 0.2316 - accuracy: 0.9204 - val_loss: 0.4767 - val_accuracy: 0.7874\n"
     ]
    }
   ],
   "source": [
    "# Fit the model\n",
    "model_1_history = model_1.fit(train_sentences, # input sentences can be a list of strings due to text preprocessing layer built-in model\n",
    "                              train_labels,\n",
    "                              epochs=5,\n",
    "                              validation_data=(val_sentences, val_labels),\n",
    "                              callbacks=[create_tensorboard_callback(dir_name=SAVE_DIR, \n",
    "                                                                     experiment_name=\"simple_dense_model\")])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kZR5_j9C_LW-"
   },
   "source": [
    "Nice! Since we're using such a simple model, each epoch processes very quickly.\n",
    "\n",
    "Let's check our model's performance on the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zSTS87YGzuBG",
    "outputId": "4e661a30-f1ee-4cb9-cab1-b4e822ff86da"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/24 [==============================] - 0s 2ms/step - loss: 0.4767 - accuracy: 0.7874\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.47668465971946716, 0.787401556968689]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the results\n",
    "model_1.evaluate(val_sentences, val_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5M2CTAetBVfW",
    "outputId": "964d5477-ef18-4be6-f5ec-84df0c6a8381"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Variable 'embedding_1/embeddings:0' shape=(10000, 128) dtype=float32, numpy=\n",
       " array([[ 0.00073161,  0.01504799, -0.03425453, ..., -0.04403543,\n",
       "         -0.01042276,  0.01876437],\n",
       "        [ 0.04135864, -0.03945084, -0.03811939, ...,  0.00464735,\n",
       "          0.03163553,  0.02928303],\n",
       "        [ 0.00684032,  0.05363132, -0.00241555, ..., -0.07082171,\n",
       "         -0.04750702,  0.01448254],\n",
       "        ...,\n",
       "        [-0.03301444, -0.0052493 , -0.04209725, ...,  0.02028764,\n",
       "          0.00308807,  0.02215792],\n",
       "        [ 0.00692342,  0.05942352, -0.01975193, ..., -0.0619906 ,\n",
       "         -0.01018394,  0.03510419],\n",
       "        [-0.03723461,  0.06267188, -0.07451147, ..., -0.02367217,\n",
       "         -0.0864333 ,  0.01742156]], dtype=float32)>]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding.weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "M3rfhJFSBrga",
    "outputId": "6ad77fc0-9226-4ed0-ff5d-c3acd9291c8c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 128)\n"
     ]
    }
   ],
   "source": [
    "embed_weights = model_1.get_layer(\"embedding_1\").get_weights()[0]\n",
    "print(embed_weights.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I9dg2aba_VxK"
   },
   "source": [
    "And since we tracked our model's training logs with TensorBoard, how about we visualize them?\n",
    "\n",
    "We can do so by uploading our TensorBoard log files (contained in the `model_logs` directory) to [TensorBoard.dev](https://tensorboard.dev/).\n",
    "\n",
    "> 🔑 **Note:** Remember, whatever you upload to TensorBoard.dev becomes public. If there are training logs you don't want to share, don't upload them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "t6UrSgRVU6pl"
   },
   "outputs": [],
   "source": [
    "# # View tensorboard logs of transfer learning modelling experiments (should be 4 models)\n",
    "# # Upload TensorBoard dev records\n",
    "# !tensorboard dev upload --logdir ./model_logs \\\n",
    "#   --name \"First deep model on text data\" \\\n",
    "#   --description \"Trying a dense model with an embedding layer\" \\\n",
    "#   --one_shot # exits the uploader when upload has finished"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "DVyJl-VE1ACz"
   },
   "outputs": [],
   "source": [
    "# If you need to remove previous experiments, you can do so using the following command\n",
    "# !tensorboard dev delete --experiment_id EXPERIMENT_ID_TO_DELETE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PkinGcjQ_yI9"
   },
   "source": [
    "The TensorBoard.dev experiment for our first deep model can be viewed here: https://tensorboard.dev/experiment/5d1Xm10aT6m6MgyW3HAGfw/\n",
    "\n",
    "![](https://raw.githubusercontent.com/mrdbourke/tensorflow-deep-learning/main/images/08-tensorboard-dense-model-training-curves.png)\n",
    "\n",
    "*What the training curves of our model look like on TensorBoard. From looking at the curves can you tell if the model is overfitting or underfitting?*\n",
    "\n",
    "Beautiful! Those are some colorful training curves. Would you say the model is overfitting or underfitting?\n",
    "\n",
    "We've built and trained our first deep model, the next step is to make some predictions with it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5X7kbEmAzzxM",
    "outputId": "6540e4b0-e549-4790-d928-04671f7ae203"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.404882  ],\n",
       "       [0.7443312 ],\n",
       "       [0.997895  ],\n",
       "       [0.10890001],\n",
       "       [0.11143529],\n",
       "       [0.93556094],\n",
       "       [0.9134595 ],\n",
       "       [0.9925345 ],\n",
       "       [0.97156817],\n",
       "       [0.2657034 ]], dtype=float32)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make predictions (these come back in the form of probabilities)\n",
    "model_1_pred_probs = model_1.predict(val_sentences)\n",
    "model_1_pred_probs[:10] # only print out the first 10 prediction probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YWU5e1NLAKJ9"
   },
   "source": [
    "Since our final layer uses a sigmoid activation function, we get our predictions back in the form of probabilities.\n",
    "\n",
    "To convert them to prediction classes, we'll use `tf.round()`, meaning prediction probabilities below 0.5 will be rounded to 0 and those above 0.5 will be rounded to 1.\n",
    "\n",
    "> 🔑 **Note:** In practice, the output threshold of a sigmoid prediction probability doesn't necessarily have to 0.5. For example, through testing, you may find that a cut off of 0.25 is better for your chosen evaluation metrics. A common example of this threshold cutoff is the [precision-recall tradeoff](https://www.machinelearningaptitude.com/topics/machine-learning/what-is-precision-recall-tradeoff/#:~:text=precision%2Drecall%20tradeoff%20occur%20due,the%20threshold%20of%20the%20classifier.&text=When%20threshold%20is%20decreased%20to,but%20precision%20decreases%20to%200.4.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Qf-R_1vsz47P",
    "outputId": "bc1f0e0b-eafd-46dc-886c-aff1edc9f2d6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(20,), dtype=float32, numpy=\n",
       "array([0., 1., 1., 0., 0., 1., 1., 1., 1., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 1.], dtype=float32)>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Turn prediction probabilities into single-dimension tensor of floats\n",
    "model_1_preds = tf.squeeze(tf.round(model_1_pred_probs)) # squeeze removes single dimensions\n",
    "model_1_preds[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zc3ryY0yCHcI"
   },
   "source": [
    "Now we've got our model's predictions in the form of classes, we can use our `calculate_results()` function to compare them to the ground truth validation labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iDEEhYTF0X1y",
    "outputId": "ddbcab12-7d06-4da4-8a26-e19018c5e7b1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 78.74015748031496,\n",
       " 'precision': 0.7914920592553047,\n",
       " 'recall': 0.7874015748031497,\n",
       " 'f1': 0.7846966492209201}"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate model_1 metrics\n",
    "model_1_results = calculate_results(y_true=val_labels, \n",
    "                                    y_pred=model_1_preds)\n",
    "model_1_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gnkK6Uc7CYlX"
   },
   "source": [
    "How about we compare our first deep model to our baseline model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Jp88ystW1m0d",
    "outputId": "a6e852a7-5726-4923-c9a3-81e79048d0a3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([False, False, False, False])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Is our simple Keras model better than our baseline model?\n",
    "import numpy as np\n",
    "np.array(list(model_1_results.values())) > np.array(list(baseline_results.values()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lUINrCdRCpFf"
   },
   "source": [
    "Since we'll be doing this kind of comparison (baseline compared to new model) quite a few times, let's create a function to help us out. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wo3norTG3GrE",
    "outputId": "5a28657f-d188-420d-906c-224d36c7b456"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline accuracy: 79.27, New accuracy: 78.74, Difference: -0.52\n",
      "Baseline precision: 0.81, New precision: 0.79, Difference: -0.02\n",
      "Baseline recall: 0.79, New recall: 0.79, Difference: -0.01\n",
      "Baseline f1: 0.79, New f1: 0.78, Difference: -0.00\n"
     ]
    }
   ],
   "source": [
    "# Create a helper function to compare our baseline results to new model results\n",
    "def compare_baseline_to_new_results(baseline_results, new_model_results):\n",
    "  for key, value in baseline_results.items():\n",
    "    print(f\"Baseline {key}: {value:.2f}, New {key}: {new_model_results[key]:.2f}, Difference: {new_model_results[key]-value:.2f}\")\n",
    "\n",
    "compare_baseline_to_new_results(baseline_results=baseline_results, \n",
    "                                new_model_results=model_1_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6e-1LuioSLAM"
   },
   "source": [
    "## Visualizing learned embeddings\n",
    "\n",
    "Our first model (`model_1`) contained an embedding layer (`embedding`) which learned a way of representing words as feature vectors by passing over the training data.\n",
    "\n",
    "Hearing this for the first few times may sound confusing.\n",
    "\n",
    "So to further help understand what a text embedding is, let's visualize the embedding our model learned.\n",
    "\n",
    "To do so, let's remind ourselves of the words in our vocabulary.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-DkcfRQBVXuJ",
    "outputId": "6cfcdc34-ac8a-4f6a-96ad-59c4cbaebab8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, ['', '[UNK]', 'the', 'a', 'in', 'to', 'of', 'and', 'i', 'is'])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the vocabulary from the text vectorization layer\n",
    "words_in_vocab = text_vectorizer.get_vocabulary()\n",
    "len(words_in_vocab), words_in_vocab[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8EUR9PwrZphh",
    "outputId": "dfec8286-f68c-4624-9edf-1136d5c31ddc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1_dense\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 1)]               0         \n",
      "_________________________________________________________________\n",
      "text_vectorization_1 (TextVe (None, 15)                0         \n",
      "_________________________________________________________________\n",
      "embedding_1 (Embedding)      (None, 15, 128)           1280000   \n",
      "_________________________________________________________________\n",
      "global_average_pooling1d (Gl (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 1,280,129\n",
      "Trainable params: 1,280,129\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_1.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now let's get our embedding layer's weights (these are the numerical representations of each word)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9xJ5LrInWDLo",
    "outputId": "3f47fd40-6bfe-4b35-c3d3-1dcdbdd9cbe1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 128)\n"
     ]
    }
   ],
   "source": [
    "# Get the weight matrix of embedding layer \n",
    "# (these are the numerical patterns between the text in the training dataset the model has learned)\n",
    "embed_weights = model_1.get_layer(\"embedding_1\").get_weights()[0]\n",
    "print(embed_weights.shape) # same size as vocab size and embedding_dim (each word is a embedding_dim size vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jzOJhJHPW1ju"
   },
   "source": [
    "Now we've got these two objects, we can use the [Embedding Projector tool](http://projector.tensorflow.org/_) to visualize our embedding. \n",
    "\n",
    "To use the Embedding Projector tool, we need two files:\n",
    "* The embedding vectors (same as embedding weights).\n",
    "* The meta data of the embedding vectors (the words they represent - our vocabulary).\n",
    "\n",
    "Right now, we've got of these files as Python objects. To download them to file, we're going to [use the code example available on the TensorFlow word embeddings tutorial page](https://www.tensorflow.org/tutorials/text/word_embeddings#retrieve_the_trained_word_embeddings_and_save_them_to_disk).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "id": "4e9rfcK6WxQE"
   },
   "outputs": [],
   "source": [
    "# # Code below is adapted from: https://www.tensorflow.org/tutorials/text/word_embeddings#retrieve_the_trained_word_embeddings_and_save_them_to_disk\n",
    "# import io\n",
    "\n",
    "# # Create output writers\n",
    "# out_v = io.open(\"embedding_vectors.tsv\", \"w\", encoding=\"utf-8\")\n",
    "# out_m = io.open(\"embedding_metadata.tsv\", \"w\", encoding=\"utf-8\")\n",
    "\n",
    "# # Write embedding vectors and words to file\n",
    "# for num, word in enumerate(words_in_vocab):\n",
    "#   if num == 0: \n",
    "#      continue # skip padding token\n",
    "#   vec = embed_weights[num]\n",
    "#   out_m.write(word + \"\\n\") # write words to file\n",
    "#   out_v.write(\"\\t\".join([str(x) for x in vec]) + \"\\n\") # write corresponding word vector to file\n",
    "# out_v.close()\n",
    "# out_m.close()\n",
    "\n",
    "# # Download files locally to upload to Embedding Projector\n",
    "# try:\n",
    "#   from google.colab import files\n",
    "# except ImportError:\n",
    "#   pass\n",
    "# else:\n",
    "#   files.download(\"embedding_vectors.tsv\")\n",
    "#   files.download(\"embedding_metadata.tsv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BVM7ifzpZaxJ"
   },
   "source": [
    "Once you've downloaded the embedding vectors and metadata, you can visualize them using Embedding Vector tool:\n",
    "1. Go to  http://projector.tensorflow.org/\n",
    "2. Click on \"Load data\"\n",
    "3. Upload the two files you downloaded (`embedding_vectors.tsv` and `embedding_metadata.tsv`)\n",
    "4. Explore\n",
    "5. Optional: You can share the data you've created by clicking \"Publish\"\n",
    "\n",
    "What do you find?\n",
    "\n",
    "Are words with similar meanings close together?\n",
    "\n",
    "Remember, they might not be. The embeddings we downloaded are how our model interprets words, not necessarily how we interpret them. \n",
    "\n",
    "Also, since the embedding has been learned purely from Tweets, it may contain some strange values as Tweets are a very unique style of natural language.\n",
    "\n",
    "> 🤔 **Question:** Do you have to visualize embeddings every time?\n",
    "\n",
    "No. Although helpful for gaining an intuition of what natural language embeddings are, it's not completely necessary. Especially as the dimensions of your vocabulary and embeddings grow, trying to comprehend them would become an increasingly difficult task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AcRdDiEtGQj4"
   },
   "source": [
    "## Recurrent Neural Networks (RNN's)\n",
    "\n",
    "For our next series of modelling experiments we're going to be using a special kind of neural network called a **Recurrent Neural Network (RNN)**.\n",
    "\n",
    "The premise of an RNN is simple: use information from the past to help you with the future (this is where the term recurrent comes from). In other words, take an input (`X`) and compute an output (`y`) based on all previous inputs.\n",
    "\n",
    "This concept is especially helpful when dealing with sequences such as passages of natural language text (such as our Tweets).\n",
    "\n",
    "For example, when you read this sentence, you take into context the previous words when deciphering the meaning of the current word dog. \n",
    "\n",
    "See what happened there? \n",
    "\n",
    "I put the word \"dog\" at the end which is a valid word but it doesn't make sense in the context of the rest of the sentence.\n",
    "\n",
    "When an RNN looks at a sequence of text (already in numerical form), the patterns it learns are continually updated based on the order of the sequence. \n",
    "\n",
    "For a simple example, take two sentences:\n",
    "1. Massive earthquake last week, no?\n",
    "2. No massive earthquake last week.\n",
    "\n",
    "Both contain exactly the same words but have different meaning. The order of the words determines the meaning (one could argue punctuation marks also dictate the meaning but for simplicity sake, let's stay focused on the words).\n",
    "\n",
    "Recurrent neural networks can be used for a number of sequence-based problems:\n",
    "* **One to one:** one input, one output, such as image classification.\n",
    "* **One to many:** one input, many outputs, such as image captioning (image input, a sequence of text as caption output).\n",
    "* **Many to one:** many inputs, one outputs, such as text classification (classifying a Tweet as real diaster or not real diaster).\n",
    "* **Many to many:** many inputs, many outputs, such as machine translation (translating English to Spanish) or speech to text (audio wave as input, text as output).\n",
    "\n",
    "When you come across RNN's in the wild, you'll most likely come across variants of the following:\n",
    "* Long short-term memory cells (LSTMs).\n",
    "* Gated recurrent units (GRUs).\n",
    "* Bidirectional RNN's (passes forward and backward along a sequence, left to right and right to left).\n",
    "\n",
    "Going into the details of each these is beyond the scope of this notebook (we're going to focus on using them instead), the main thing you should know for now is that they've proven very effective at modelling sequences.\n",
    "\n",
    "For a deeper understanding of what's happening behind the scenes of the code we're about to write, I'd recommend the following resources:\n",
    "\n",
    "> 📖 **Resources:**\n",
    "> * [MIT Deep Learning Lecture on Recurrent Neural Networks](https://youtu.be/SEnXr6v2ifU) - explains the background of recurrent neural networks and introduces LSTMs.\n",
    "> * [The Unreasonable Effectiveness of Recurrent Neural Networks](http://karpathy.github.io/2015/05/21/rnn-effectiveness/) by Andrej Karpathy - demonstrates the power of RNN's with examples generating various sequences.\n",
    "> * [Understanding LSTMs](https://colah.github.io/posts/2015-08-Understanding-LSTMs/) by Chris Olah - an in-depth (and technical) look at the mechanics of the LSTM cell, possibly the most popular RNN building block.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tDERKwP_XWro"
   },
   "source": [
    "### Model 2: LSTM (Long Short Term Memory)\n",
    "\n",
    "With all this talk of what RNN's are and what they're good for, I'm sure you're eager to build one.\n",
    "\n",
    "We're going to start with an LSTM-powered RNN.\n",
    "\n",
    "To harness the power of the LSTM cell (LSTM cell and LSTM layer are often used interchangably) in TensorFlow, we'll use [`tensorflow.keras.layers.LSTM()`](https://www.tensorflow.org/api_docs/python/tf/keras/layers/LSTM).\n",
    "\n",
    "![](https://raw.githubusercontent.com/mrdbourke/tensorflow-deep-learning/main/images/08-RNN-architecture-coloured-block-edition.png)\n",
    "*Coloured block example of the structure of an recurrent neural network.*\n",
    "\n",
    "Our model is going to take on a very similar structure to `model_1`:\n",
    "\n",
    "```\n",
    "Input (text) -> Tokenize -> Embedding -> Layers (RNNs/Dense) -> Output (label probability)\n",
    "```\n",
    "\n",
    "The main difference will be that we're going to add an LSTM layer between our embedding and output.\n",
    "\n",
    "And to make sure we're not getting reusing trained embeddings (this would involve data leakage between models, leading to an uneven comparison later on), we'll create another embedding layer (`model_2_embedding`) for our model. The `text_vectorizer` layer can be reused since it doesn't get updated during training.\n",
    "\n",
    "> 🔑 **Note:** The reason we use a new embedding layer for each model is since the embedding layer is a *learned* representation of words (as numbers), if we were to use the same embedding layer (`embedding_1`) for each model, we'd be mixing what one model learned with the next. And because we want to compare our models later on, starting them with their own embedding layer each time is a better idea."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Pi3vjpFU46hi",
    "outputId": "2501629e-c3ff-4acf-80d1-530828481d70"
   },
   "outputs": [],
   "source": [
    "# Set random seed and create embedding layer (new embedding layer for each model)\n",
    "tf.random.set_seed(42)\n",
    "from tensorflow.keras import layers\n",
    "model_2_embedding = layers.Embedding(input_dim=max_vocab_length,\n",
    "                                     output_dim=128,\n",
    "                                     embeddings_initializer=\"uniform\",\n",
    "                                     input_length=max_length,\n",
    "                                     name=\"embedding_2\")\n",
    "\n",
    "\n",
    "# Create LSTM model\n",
    "inputs = layers.Input(shape=(1,), dtype=\"string\")\n",
    "x = text_vectorizer(inputs)\n",
    "x = model_2_embedding(x)\n",
    "# print(x.shape)\n",
    "# x = layers.LSTM(64, return_sequences=True)(x) # return vector for each word in the Tweet (you can stack RNN cells as long as return_sequences=True)\n",
    "# print(x.shape)\n",
    "x = layers.LSTM(64)(x) # return vector for whole sequence\n",
    "# print(x.shape)\n",
    "# x = layers.Dense(64, activation=\"relu\")(x) # optional dense layer on top of output of LSTM cell\n",
    "# print(x.shape)\n",
    "outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "model_2 = tf.keras.Model(inputs, outputs, name=\"model_2_LSTM\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e1wfTARuwWDg"
   },
   "source": [
    "> 🔑 **Note:** Reading the documentation for the [TensorFlow LSTM layer](https://www.tensorflow.org/api_docs/python/tf/keras/layers/LSTM), you'll find a plethora of parameters. Many of these have been tuned to make sure they compute as fast as possible. The main ones you'll be looking to adjust are `units` (number of hidden units) and `return_sequences` (set this to `True` when stacking LSTM or other recurrent layers).\n",
    "\n",
    "Now we've got our LSTM model built, let's compile it using `\"binary_crossentropy\"` loss and the Adam optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2_LSTM\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         [(None, 1)]               0         \n",
      "_________________________________________________________________\n",
      "text_vectorization_1 (TextVe (None, 15)                0         \n",
      "_________________________________________________________________\n",
      "embedding_2 (Embedding)      (None, 15, 128)           1280000   \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, 64)                49408     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 1,329,473\n",
      "Trainable params: 1,329,473\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "id": "pWdt3bFRwG6w"
   },
   "outputs": [],
   "source": [
    "# Compile model\n",
    "model_2.compile(loss=\"binary_crossentropy\",\n",
    "                optimizer=tf.keras.optimizers.Adam(),\n",
    "                metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I2e_t8RFxgXG"
   },
   "source": [
    "And before we fit our model to the data, let's get a summary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S5NLw3wD0aMz"
   },
   "source": [
    "Looking good! You'll notice a fair few more trainable parameters within our LSTM layer than `model_1`. \n",
    "\n",
    "If you'd like to know where this number comes from, I recommend going through the above resources as well the following on calculating the number of parameters in an LSTM cell:\n",
    "* [Stack Overflow answer to calculate the number of parameters in an LSTM cell](https://stackoverflow.com/questions/38080035/how-to-calculate-the-number-of-parameters-of-an-lstm-network) by Marcin Możejko\n",
    "* [Calculating number of parameters in a LSTM unit and layer](https://medium.com/@priyadarshi.cse/calculating-number-of-parameters-in-a-lstm-unit-layer-7e491978e1e4) by Shridhar Priyadarshi\n",
    "\n",
    "Now our first RNN model's compiled let's fit it to our training data, validating it on the validation data and tracking its training parameters using our TensorBoard callback."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YgZ7ojDvwKcq",
    "outputId": "280c9316-d0c8-4454-c133-5b1c2372ebfc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving TensorBoard log files to: model_logs/LSTM/20220822-172718\n",
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-22 17:27:18.106505: I tensorflow/core/profiler/lib/profiler_session.cc:136] Profiler session initializing.\n",
      "2022-08-22 17:27:18.106530: I tensorflow/core/profiler/lib/profiler_session.cc:155] Profiler session started.\n",
      "2022-08-22 17:27:18.106673: I tensorflow/core/profiler/lib/profiler_session.cc:172] Profiler session tear down.\n",
      "2022-08-22 17:27:18.106716: I tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1487] CUPTI activity buffer flushed\n",
      "2022-08-22 17:27:19.376506: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.7\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  7/215 [..............................] - ETA: 8s - loss: 0.6924 - accuracy: 0.5400 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-22 17:27:20.520769: I tensorflow/core/profiler/lib/profiler_session.cc:136] Profiler session initializing.\n",
      "2022-08-22 17:27:20.520816: I tensorflow/core/profiler/lib/profiler_session.cc:155] Profiler session started.\n",
      "2022-08-22 17:27:20.614706: I tensorflow/core/profiler/lib/profiler_session.cc:71] Profiler session collecting data.\n",
      "2022-08-22 17:27:20.615046: I tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1487] CUPTI activity buffer flushed\n",
      "2022-08-22 17:27:20.616988: I tensorflow/core/profiler/internal/gpu/cupti_collector.cc:228]  GpuTracer has collected 241 callback api events and 229 activity events. \n",
      "2022-08-22 17:27:20.622764: I tensorflow/core/profiler/lib/profiler_session.cc:172] Profiler session tear down.\n",
      "2022-08-22 17:27:20.627537: I tensorflow/core/profiler/rpc/client/save_profile.cc:137] Creating directory: model_logs/LSTM/20220822-172718/train/plugins/profile/2022_08_22_17_27_20\n",
      "2022-08-22 17:27:20.631600: I tensorflow/core/profiler/rpc/client/save_profile.cc:143] Dumped gzipped tool data for trace.json.gz to model_logs/LSTM/20220822-172718/train/plugins/profile/2022_08_22_17_27_20/ubuntu-All-Series.trace.json.gz\n",
      "2022-08-22 17:27:20.641571: I tensorflow/core/profiler/rpc/client/save_profile.cc:137] Creating directory: model_logs/LSTM/20220822-172718/train/plugins/profile/2022_08_22_17_27_20\n",
      "2022-08-22 17:27:20.643429: I tensorflow/core/profiler/rpc/client/save_profile.cc:143] Dumped gzipped tool data for memory_profile.json.gz to model_logs/LSTM/20220822-172718/train/plugins/profile/2022_08_22_17_27_20/ubuntu-All-Series.memory_profile.json.gz\n",
      "2022-08-22 17:27:20.643922: I tensorflow/core/profiler/rpc/client/capture_profile.cc:251] Creating directory: model_logs/LSTM/20220822-172718/train/plugins/profile/2022_08_22_17_27_20Dumped tool data for xplane.pb to model_logs/LSTM/20220822-172718/train/plugins/profile/2022_08_22_17_27_20/ubuntu-All-Series.xplane.pb\n",
      "Dumped tool data for overview_page.pb to model_logs/LSTM/20220822-172718/train/plugins/profile/2022_08_22_17_27_20/ubuntu-All-Series.overview_page.pb\n",
      "Dumped tool data for input_pipeline.pb to model_logs/LSTM/20220822-172718/train/plugins/profile/2022_08_22_17_27_20/ubuntu-All-Series.input_pipeline.pb\n",
      "Dumped tool data for tensorflow_stats.pb to model_logs/LSTM/20220822-172718/train/plugins/profile/2022_08_22_17_27_20/ubuntu-All-Series.tensorflow_stats.pb\n",
      "Dumped tool data for kernel_stats.pb to model_logs/LSTM/20220822-172718/train/plugins/profile/2022_08_22_17_27_20/ubuntu-All-Series.kernel_stats.pb\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "215/215 [==============================] - 6s 17ms/step - loss: 0.5814 - accuracy: 0.6733 - val_loss: 0.4566 - val_accuracy: 0.7822\n",
      "Epoch 2/5\n",
      "215/215 [==============================] - 3s 16ms/step - loss: 0.3128 - accuracy: 0.8776 - val_loss: 0.5138 - val_accuracy: 0.7756\n",
      "Epoch 3/5\n",
      "215/215 [==============================] - 3s 16ms/step - loss: 0.2098 - accuracy: 0.9194 - val_loss: 0.5858 - val_accuracy: 0.7677\n",
      "Epoch 4/5\n",
      "215/215 [==============================] - 3s 15ms/step - loss: 0.1411 - accuracy: 0.9488 - val_loss: 0.6041 - val_accuracy: 0.7743\n",
      "Epoch 5/5\n",
      "215/215 [==============================] - 3s 15ms/step - loss: 0.1004 - accuracy: 0.9651 - val_loss: 0.8746 - val_accuracy: 0.7507\n"
     ]
    }
   ],
   "source": [
    "# Fit model\n",
    "model_2_history = model_2.fit(train_sentences,\n",
    "                              train_labels,\n",
    "                              epochs=5,\n",
    "                              validation_data=(val_sentences, val_labels),\n",
    "                              callbacks=[create_tensorboard_callback(SAVE_DIR, \n",
    "                                                                     \"LSTM\")])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1gikGe_Z16PP"
   },
   "source": [
    "Nice! We've got our first trained RNN model using LSTM cells. Let's make some predictions with it.\n",
    "\n",
    "The same thing will happen as before, due to the sigmoid activiation function in the final layer, when we call the `predict()` method on our model, it'll return prediction probabilities rather than classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4c_lVbKLemrU",
    "outputId": "3c8f7372-a57e-4be9-b275-6d79be1dcfea"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((762, 1),\n",
       " array([[0.007126  ],\n",
       "        [0.78736997],\n",
       "        [0.9996376 ],\n",
       "        [0.05679166],\n",
       "        [0.00258225],\n",
       "        [0.9996238 ],\n",
       "        [0.92169976],\n",
       "        [0.9997993 ],\n",
       "        [0.9994954 ],\n",
       "        [0.6645802 ]], dtype=float32))"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make predictions on the validation dataset\n",
    "model_2_pred_probs = model_2.predict(val_sentences)\n",
    "model_2_pred_probs.shape, model_2_pred_probs[:10] # view the first 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fQ6ope-ddpOo"
   },
   "source": [
    "We can turn these prediction probabilities into prediction classes by rounding to the nearest integer (by default, prediction probabilities under 0.5 will go to 0 and those over 0.5 will go to 1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iFnIhtyE7hlb",
    "outputId": "4d882b1e-e9d8-40d1-ae5a-fed38e61f2b5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(10,), dtype=float32, numpy=array([0., 1., 1., 0., 0., 1., 1., 1., 1., 1.], dtype=float32)>"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Round out predictions and reduce to 1-dimensional array\n",
    "model_2_preds = tf.squeeze(tf.round(model_2_pred_probs))\n",
    "model_2_preds[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zTBy4poXd_7p"
   },
   "source": [
    "Beautiful, now let's use our `caculate_results()` function to evaluate our LSTM model and our `compare_baseline_to_new_results()` function to compare it to our baseline model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3iHXv04y76vj",
    "outputId": "b09ed2fa-21cb-42bf-a32e-c96c4d7891a1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 75.06561679790026,\n",
       " 'precision': 0.7510077975908164,\n",
       " 'recall': 0.7506561679790026,\n",
       " 'f1': 0.7489268622514025}"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate LSTM model results\n",
    "model_2_results = calculate_results(y_true=val_labels,\n",
    "                                    y_pred=model_2_preds)\n",
    "model_2_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZdQGn2L68B5Q",
    "outputId": "c0244b86-dc0a-4052-c7bb-5287962269c6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline accuracy: 79.27, New accuracy: 75.07, Difference: -4.20\n",
      "Baseline precision: 0.81, New precision: 0.75, Difference: -0.06\n",
      "Baseline recall: 0.79, New recall: 0.75, Difference: -0.04\n",
      "Baseline f1: 0.79, New f1: 0.75, Difference: -0.04\n"
     ]
    }
   ],
   "source": [
    "# Compare model 2 to baseline\n",
    "compare_baseline_to_new_results(baseline_results, model_2_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q0pAtADt8ju7"
   },
   "source": [
    "### Model 3: GRU\n",
    "\n",
    "Another popular and effective RNN component is the GRU or gated recurrent unit.\n",
    "\n",
    "The GRU cell has similar features to an LSTM cell but has less parameters.\n",
    "\n",
    "> 📖 **Resource:** A full explanation of the GRU cell is beyond the scope of this noteook but I'd suggest the following resources to learn more:\n",
    "* [Gated Recurrent Unit](https://en.wikipedia.org/wiki/Gated_recurrent_unit) Wikipedia page\n",
    "* [Understanding GRU networks](https://towardsdatascience.com/understanding-gru-networks-2ef37df6c9be) by Simeon Kostadinov\n",
    "\n",
    "To use the GRU cell in TensorFlow, we can call the [`tensorflow.keras.layers.GRU()`](https://www.tensorflow.org/api_docs/python/tf/keras/layers/GRU) class.\n",
    "\n",
    "The architecture of the GRU-powered model will follow the same structure we've been using:\n",
    "\n",
    "```\n",
    "Input (text) -> Tokenize -> Embedding -> Layers -> Output (label probability)\n",
    "```\n",
    "\n",
    "Again, the only difference will be the layer(s) we use between the embedding and the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "id": "SoSCGq3H47Yo"
   },
   "outputs": [],
   "source": [
    "# Set random seed and create embedding layer (new embedding layer for each model)\n",
    "tf.random.set_seed(42)\n",
    "from tensorflow.keras import layers\n",
    "model_3_embedding = layers.Embedding(input_dim=max_vocab_length,\n",
    "                                     output_dim=128,\n",
    "                                     embeddings_initializer=\"uniform\",\n",
    "                                     input_length=max_length,\n",
    "                                     name=\"embedding_3\")\n",
    "\n",
    "# Build an RNN using the GRU cell\n",
    "inputs = layers.Input(shape=(1,), dtype=\"string\")\n",
    "x = text_vectorizer(inputs)\n",
    "x = model_3_embedding(x)\n",
    "# x = layers.GRU(64, return_sequences=True) # stacking recurrent cells requires return_sequences=True\n",
    "x = layers.GRU(64)(x) \n",
    "# x = layers.Dense(64, activation=\"relu\")(x) # optional dense layer after GRU cell\n",
    "outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "model_3 = tf.keras.Model(inputs, outputs, name=\"model_3_GRU\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JLT5maFWhKH1"
   },
   "source": [
    "TensorFlow makes it easy to use powerful components such as the GRU cell in our models. And now our third model is built, let's compile it, just as before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "id": "lBL1mb31hHDS"
   },
   "outputs": [],
   "source": [
    "# Compile GRU model\n",
    "model_3.compile(loss=\"binary_crossentropy\",\n",
    "                optimizer=tf.keras.optimizers.Adam(),\n",
    "                metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yvnksvkmha2A"
   },
   "source": [
    "What does a summary of our model look like?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JVnB5yQeiAWs",
    "outputId": "42fda310-a474-4b01-809d-4b40592ebc5f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_3_GRU\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_3 (InputLayer)         [(None, 1)]               0         \n",
      "_________________________________________________________________\n",
      "text_vectorization_1 (TextVe (None, 15)                0         \n",
      "_________________________________________________________________\n",
      "embedding_3 (Embedding)      (None, 15, 128)           1280000   \n",
      "_________________________________________________________________\n",
      "gru (GRU)                    (None, 64)                37248     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 1,317,313\n",
      "Trainable params: 1,317,313\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Get a summary of the GRU model\n",
    "model_3.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KcXzKqgXhdez"
   },
   "source": [
    "Notice the difference in number of trainable parameters between `model_2` (LSTM) and `model_3` (GRU). The difference comes from the LSTM cell having more trainable parameters than the GRU cell.\n",
    "\n",
    "We'll fit our model just as we've been doing previously. We'll also track our models results using our `create_tensorboard_callback()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Gvamg5JOh_jC",
    "outputId": "a61ecf34-9779-45ef-f455-765e2ad345bc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving TensorBoard log files to: model_logs/GRU/20220822-172737\n",
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-22 17:27:37.928374: I tensorflow/core/profiler/lib/profiler_session.cc:136] Profiler session initializing.\n",
      "2022-08-22 17:27:37.928408: I tensorflow/core/profiler/lib/profiler_session.cc:155] Profiler session started.\n",
      "2022-08-22 17:27:37.928610: I tensorflow/core/profiler/lib/profiler_session.cc:172] Profiler session tear down.\n",
      "2022-08-22 17:27:37.928698: I tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1487] CUPTI activity buffer flushed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  3/215 [..............................] - ETA: 20s - loss: 0.6925 - accuracy: 0.5191"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-22 17:27:39.440350: I tensorflow/core/profiler/lib/profiler_session.cc:136] Profiler session initializing.\n",
      "2022-08-22 17:27:39.440375: I tensorflow/core/profiler/lib/profiler_session.cc:155] Profiler session started.\n",
      "2022-08-22 17:27:39.544268: I tensorflow/core/profiler/lib/profiler_session.cc:71] Profiler session collecting data.\n",
      "2022-08-22 17:27:39.544608: I tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1487] CUPTI activity buffer flushed\n",
      "2022-08-22 17:27:39.546879: I tensorflow/core/profiler/internal/gpu/cupti_collector.cc:228]  GpuTracer has collected 238 callback api events and 226 activity events. \n",
      "2022-08-22 17:27:39.553302: I tensorflow/core/profiler/lib/profiler_session.cc:172] Profiler session tear down.\n",
      "2022-08-22 17:27:39.557585: I tensorflow/core/profiler/rpc/client/save_profile.cc:137] Creating directory: model_logs/GRU/20220822-172737/train/plugins/profile/2022_08_22_17_27_39\n",
      "2022-08-22 17:27:39.561527: I tensorflow/core/profiler/rpc/client/save_profile.cc:143] Dumped gzipped tool data for trace.json.gz to model_logs/GRU/20220822-172737/train/plugins/profile/2022_08_22_17_27_39/ubuntu-All-Series.trace.json.gz\n",
      "2022-08-22 17:27:39.571325: I tensorflow/core/profiler/rpc/client/save_profile.cc:137] Creating directory: model_logs/GRU/20220822-172737/train/plugins/profile/2022_08_22_17_27_39\n",
      "2022-08-22 17:27:39.572925: I tensorflow/core/profiler/rpc/client/save_profile.cc:143] Dumped gzipped tool data for memory_profile.json.gz to model_logs/GRU/20220822-172737/train/plugins/profile/2022_08_22_17_27_39/ubuntu-All-Series.memory_profile.json.gz\n",
      "2022-08-22 17:27:39.573357: I tensorflow/core/profiler/rpc/client/capture_profile.cc:251] Creating directory: model_logs/GRU/20220822-172737/train/plugins/profile/2022_08_22_17_27_39Dumped tool data for xplane.pb to model_logs/GRU/20220822-172737/train/plugins/profile/2022_08_22_17_27_39/ubuntu-All-Series.xplane.pb\n",
      "Dumped tool data for overview_page.pb to model_logs/GRU/20220822-172737/train/plugins/profile/2022_08_22_17_27_39/ubuntu-All-Series.overview_page.pb\n",
      "Dumped tool data for input_pipeline.pb to model_logs/GRU/20220822-172737/train/plugins/profile/2022_08_22_17_27_39/ubuntu-All-Series.input_pipeline.pb\n",
      "Dumped tool data for tensorflow_stats.pb to model_logs/GRU/20220822-172737/train/plugins/profile/2022_08_22_17_27_39/ubuntu-All-Series.tensorflow_stats.pb\n",
      "Dumped tool data for kernel_stats.pb to model_logs/GRU/20220822-172737/train/plugins/profile/2022_08_22_17_27_39/ubuntu-All-Series.kernel_stats.pb\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "215/215 [==============================] - 5s 17ms/step - loss: 0.6017 - accuracy: 0.6532 - val_loss: 0.4553 - val_accuracy: 0.7769\n",
      "Epoch 2/5\n",
      "215/215 [==============================] - 3s 15ms/step - loss: 0.3159 - accuracy: 0.8726 - val_loss: 0.4937 - val_accuracy: 0.7808\n",
      "Epoch 3/5\n",
      "215/215 [==============================] - 3s 15ms/step - loss: 0.2106 - accuracy: 0.9227 - val_loss: 0.5607 - val_accuracy: 0.7743\n",
      "Epoch 4/5\n",
      "215/215 [==============================] - 3s 14ms/step - loss: 0.1447 - accuracy: 0.9506 - val_loss: 0.6220 - val_accuracy: 0.7782\n",
      "Epoch 5/5\n",
      "215/215 [==============================] - 3s 15ms/step - loss: 0.1107 - accuracy: 0.9656 - val_loss: 0.6205 - val_accuracy: 0.7677\n"
     ]
    }
   ],
   "source": [
    "# Fit model\n",
    "model_3_history = model_3.fit(train_sentences,\n",
    "                              train_labels,\n",
    "                              epochs=5,\n",
    "                              validation_data=(val_sentences, val_labels),\n",
    "                              callbacks=[create_tensorboard_callback(SAVE_DIR, \"GRU\")])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hM4mQj1Sh7Gn"
   },
   "source": [
    "Due to the optimized default settings of the GRU cell in TensorFlow, training doesn't take long at all. \n",
    "\n",
    "Time to make some predictions on the validation samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "W5TUVHCl9pe-",
    "outputId": "27e6936e-29d8-4a81-8c5a-a4169392aa84"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((762, 1),\n",
       " array([[0.33325398],\n",
       "        [0.87741166],\n",
       "        [0.9980252 ],\n",
       "        [0.11561737],\n",
       "        [0.01235961],\n",
       "        [0.9925638 ],\n",
       "        [0.6214277 ],\n",
       "        [0.99813336],\n",
       "        [0.9982377 ],\n",
       "        [0.50180894]], dtype=float32))"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make predictions on the validation data\n",
    "model_3_pred_probs = model_3.predict(val_sentences)\n",
    "model_3_pred_probs.shape, model_3_pred_probs[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hasS7dzRiYQh"
   },
   "source": [
    "Again we get an array of prediction probabilities back which we can convert to prediction classes by rounding them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "haILbddg98CY",
    "outputId": "23bbba85-6c32-416b-e621-f19a9837bafd"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(10,), dtype=float32, numpy=array([0., 1., 1., 0., 0., 1., 1., 1., 1., 1.], dtype=float32)>"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert prediction probabilities to prediction classes\n",
    "model_3_preds = tf.squeeze(tf.round(model_3_pred_probs))\n",
    "model_3_preds[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_7yAgh-viglB"
   },
   "source": [
    "Now we've got predicted classes, let's evaluate them against the ground truth labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "h9OZbQu1-LPp",
    "outputId": "6bbb090b-4c73-410b-82a7-e842467bba59"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 76.77165354330708,\n",
       " 'precision': 0.7675450859410361,\n",
       " 'recall': 0.7677165354330708,\n",
       " 'f1': 0.7667932666650168}"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calcuate model_3 results\n",
    "model_3_results = calculate_results(y_true=val_labels, \n",
    "                                    y_pred=model_3_preds)\n",
    "model_3_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o9t7wcALiuRk"
   },
   "source": [
    "Finally we can compare our GRU model's results to our baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_7AE6vtn-RQZ",
    "outputId": "3dc7e6a1-02c6-4b24-9a71-83fd36683df8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline accuracy: 79.27, New accuracy: 76.77, Difference: -2.49\n",
      "Baseline precision: 0.81, New precision: 0.77, Difference: -0.04\n",
      "Baseline recall: 0.79, New recall: 0.77, Difference: -0.02\n",
      "Baseline f1: 0.79, New f1: 0.77, Difference: -0.02\n"
     ]
    }
   ],
   "source": [
    "# Compare to baseline\n",
    "compare_baseline_to_new_results(baseline_results, model_3_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oLm6r4nQ-Wdr"
   },
   "source": [
    "### Model 4: Bidirectonal RNN model \n",
    "\n",
    "Look at us go! We've already built two RNN's with GRU and LSTM cells. Now we're going to look into another kind of RNN, the bidirectional RNN.\n",
    "\n",
    "A standard RNN will process a sequence from left to right, where as a bidirectional RNN will process the sequence from left to right and then again from right to left.\n",
    "\n",
    "Intuitively, this can be thought of as if you were reading a sentence for the first time in the normal fashion (left to right) but for some reason it didn't make sense so you traverse back through the words and go back over them again (right to left).\n",
    "\n",
    "In practice, many sequence models often see an improvement in performance when using bidirectional RNN's.\n",
    "\n",
    "However, this improvement in performance often comes at the cost of longer training times and increased model parameters (since the model goes left to right and right to left, the number of trainable parameters doubles).\n",
    "\n",
    "Okay enough talk, let's build a bidirectional RNN.\n",
    "\n",
    "Once again, TensorFlow helps us out by providing the [`tensorflow.keras.layers.Bidirectional`](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Bidirectional) class. We can use the `Bidirectional` class to wrap our existing RNNs, instantly making them bidirectional."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "id": "NAU9dvGm47_2"
   },
   "outputs": [],
   "source": [
    "# Set random seed and create embedding layer (new embedding layer for each model)\n",
    "tf.random.set_seed(42)\n",
    "from tensorflow.keras import layers\n",
    "model_4_embedding = layers.Embedding(input_dim=max_vocab_length,\n",
    "                                     output_dim=128,\n",
    "                                     embeddings_initializer=\"uniform\",\n",
    "                                     input_length=max_length,\n",
    "                                     name=\"embedding_4\")\n",
    "\n",
    "# Build a Bidirectional RNN in TensorFlow\n",
    "inputs = layers.Input(shape=(1,), dtype=\"string\")\n",
    "x = text_vectorizer(inputs)\n",
    "x = model_4_embedding(x)\n",
    "# x = layers.Bidirectional(layers.LSTM(64, return_sequences=True))(x) # stacking RNN layers requires return_sequences=True\n",
    "x = layers.Bidirectional(layers.LSTM(64))(x) # bidirectional goes both ways so has double the parameters of a regular LSTM layer\n",
    "outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "model_4 = tf.keras.Model(inputs, outputs, name=\"model_4_Bidirectional\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9Hm5cwmNm-g4"
   },
   "source": [
    "> 🔑 **Note:** You can use the `Bidirectional` wrapper on any RNN cell in TensorFlow. For example, `layers.Bidirectional(layers.GRU(64))` creates a bidirectional GRU cell.\n",
    "\n",
    "Our bidirectional model is built, let's compile it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "id": "wP1jeF0am9x0"
   },
   "outputs": [],
   "source": [
    "# Compile\n",
    "model_4.compile(loss=\"binary_crossentropy\",\n",
    "                optimizer=tf.keras.optimizers.Adam(),\n",
    "                metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NtpYyjsbnEwN"
   },
   "source": [
    "And of course, we'll check out a summary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-sUd9AQ6nFXI",
    "outputId": "f0b55790-9d26-4708-ec15-9f016afe74a0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_4_Bidirectional\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_4 (InputLayer)         [(None, 1)]               0         \n",
      "_________________________________________________________________\n",
      "text_vectorization_1 (TextVe (None, 15)                0         \n",
      "_________________________________________________________________\n",
      "embedding_4 (Embedding)      (None, 15, 128)           1280000   \n",
      "_________________________________________________________________\n",
      "bidirectional (Bidirectional (None, 128)               98816     \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 1,378,945\n",
      "Trainable params: 1,378,945\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Get a summary of our bidirectional model\n",
    "model_4.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TvItfzeZnIE-"
   },
   "source": [
    "Notice the increased number of trainable parameters in `model_4` (bidirectional LSTM) compared to `model_2` (regular LSTM). This is due to the bidirectionality we added to our RNN.\n",
    "\n",
    "Time to fit our bidirectional model and track its performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bAKY_QbHXPHB",
    "outputId": "91104d25-d1b4-4b7c-fda1-0e873a4cbf9a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving TensorBoard log files to: model_logs/bidirectional_RNN/20220822-172756\n",
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-22 17:27:56.470240: I tensorflow/core/profiler/lib/profiler_session.cc:136] Profiler session initializing.\n",
      "2022-08-22 17:27:56.470274: I tensorflow/core/profiler/lib/profiler_session.cc:155] Profiler session started.\n",
      "2022-08-22 17:27:56.470479: I tensorflow/core/profiler/lib/profiler_session.cc:172] Profiler session tear down.\n",
      "2022-08-22 17:27:56.470549: I tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1487] CUPTI activity buffer flushed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  2/215 [..............................] - ETA: 25s - loss: 0.6919 - accuracy: 0.6016 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-22 17:27:58.794408: I tensorflow/core/profiler/lib/profiler_session.cc:136] Profiler session initializing.\n",
      "2022-08-22 17:27:58.794443: I tensorflow/core/profiler/lib/profiler_session.cc:155] Profiler session started.\n",
      "2022-08-22 17:27:58.937140: I tensorflow/core/profiler/lib/profiler_session.cc:71] Profiler session collecting data.\n",
      "2022-08-22 17:27:58.937765: I tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1487] CUPTI activity buffer flushed\n",
      "2022-08-22 17:27:58.941173: I tensorflow/core/profiler/internal/gpu/cupti_collector.cc:228]  GpuTracer has collected 377 callback api events and 365 activity events. \n",
      "2022-08-22 17:27:58.949181: I tensorflow/core/profiler/lib/profiler_session.cc:172] Profiler session tear down.\n",
      "2022-08-22 17:27:58.956466: I tensorflow/core/profiler/rpc/client/save_profile.cc:137] Creating directory: model_logs/bidirectional_RNN/20220822-172756/train/plugins/profile/2022_08_22_17_27_58\n",
      "2022-08-22 17:27:58.961740: I tensorflow/core/profiler/rpc/client/save_profile.cc:143] Dumped gzipped tool data for trace.json.gz to model_logs/bidirectional_RNN/20220822-172756/train/plugins/profile/2022_08_22_17_27_58/ubuntu-All-Series.trace.json.gz\n",
      "2022-08-22 17:27:58.977548: I tensorflow/core/profiler/rpc/client/save_profile.cc:137] Creating directory: model_logs/bidirectional_RNN/20220822-172756/train/plugins/profile/2022_08_22_17_27_58\n",
      "2022-08-22 17:27:58.980987: I tensorflow/core/profiler/rpc/client/save_profile.cc:143] Dumped gzipped tool data for memory_profile.json.gz to model_logs/bidirectional_RNN/20220822-172756/train/plugins/profile/2022_08_22_17_27_58/ubuntu-All-Series.memory_profile.json.gz\n",
      "2022-08-22 17:27:58.981694: I tensorflow/core/profiler/rpc/client/capture_profile.cc:251] Creating directory: model_logs/bidirectional_RNN/20220822-172756/train/plugins/profile/2022_08_22_17_27_58Dumped tool data for xplane.pb to model_logs/bidirectional_RNN/20220822-172756/train/plugins/profile/2022_08_22_17_27_58/ubuntu-All-Series.xplane.pb\n",
      "Dumped tool data for overview_page.pb to model_logs/bidirectional_RNN/20220822-172756/train/plugins/profile/2022_08_22_17_27_58/ubuntu-All-Series.overview_page.pb\n",
      "Dumped tool data for input_pipeline.pb to model_logs/bidirectional_RNN/20220822-172756/train/plugins/profile/2022_08_22_17_27_58/ubuntu-All-Series.input_pipeline.pb\n",
      "Dumped tool data for tensorflow_stats.pb to model_logs/bidirectional_RNN/20220822-172756/train/plugins/profile/2022_08_22_17_27_58/ubuntu-All-Series.tensorflow_stats.pb\n",
      "Dumped tool data for kernel_stats.pb to model_logs/bidirectional_RNN/20220822-172756/train/plugins/profile/2022_08_22_17_27_58/ubuntu-All-Series.kernel_stats.pb\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "215/215 [==============================] - 7s 21ms/step - loss: 0.5804 - accuracy: 0.6822 - val_loss: 0.4606 - val_accuracy: 0.7795\n",
      "Epoch 2/5\n",
      "215/215 [==============================] - 3s 16ms/step - loss: 0.3082 - accuracy: 0.8781 - val_loss: 0.5144 - val_accuracy: 0.7690\n",
      "Epoch 3/5\n",
      "215/215 [==============================] - 3s 16ms/step - loss: 0.2030 - accuracy: 0.9238 - val_loss: 0.5626 - val_accuracy: 0.7677\n",
      "Epoch 4/5\n",
      "215/215 [==============================] - 3s 16ms/step - loss: 0.1412 - accuracy: 0.9525 - val_loss: 0.6365 - val_accuracy: 0.7769\n",
      "Epoch 5/5\n",
      "215/215 [==============================] - 3s 16ms/step - loss: 0.1011 - accuracy: 0.9682 - val_loss: 0.6509 - val_accuracy: 0.7664\n"
     ]
    }
   ],
   "source": [
    "# Fit the model (takes longer because of the bidirectional layers)\n",
    "model_4_history = model_4.fit(train_sentences,\n",
    "                              train_labels,\n",
    "                              epochs=5,\n",
    "                              validation_data=(val_sentences, val_labels),\n",
    "                              callbacks=[create_tensorboard_callback(SAVE_DIR, \"bidirectional_RNN\")])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zkt8GVRHoJz6"
   },
   "source": [
    "Due to the bidirectionality of our model we see a slight increase in training time.\n",
    "\n",
    "Not to worry, it's not too dramatic of an increase.\n",
    "\n",
    "Let's make some predictions with it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uFc7QHRtXmn7",
    "outputId": "0f8e97e9-e468-49b2-9993-a0277c967673"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.04000026],\n",
       "       [0.8279293 ],\n",
       "       [0.99842227],\n",
       "       [0.13531058],\n",
       "       [0.00311333],\n",
       "       [0.99220747],\n",
       "       [0.9552838 ],\n",
       "       [0.99945647],\n",
       "       [0.99898285],\n",
       "       [0.28141686]], dtype=float32)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make predictions with bidirectional RNN on the validation data\n",
    "model_4_pred_probs = model_4.predict(val_sentences)\n",
    "model_4_pred_probs[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L_9HmNIYobDB"
   },
   "source": [
    "And we'll convert them to prediction classes and evaluate them against the ground truth labels and baseline model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "G5z8bMdaXw51",
    "outputId": "125ca498-7dcc-4ea1-e82d-af5c600a2588"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(10,), dtype=float32, numpy=array([0., 1., 1., 0., 0., 1., 1., 1., 1., 0.], dtype=float32)>"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert prediction probabilities to labels\n",
    "model_4_preds = tf.squeeze(tf.round(model_4_pred_probs))\n",
    "model_4_preds[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-a7Ym_vKYAO4",
    "outputId": "59967b50-ab0c-48d5-e40a-e81ca69fdb4d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 76.64041994750657,\n",
       " 'precision': 0.7665895370389821,\n",
       " 'recall': 0.7664041994750657,\n",
       " 'f1': 0.7651213533864446}"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate bidirectional RNN model results\n",
    "model_4_results = calculate_results(val_labels, model_4_preds)\n",
    "model_4_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hAET-LKpYT18",
    "outputId": "3489d5ab-9ac2-46c5-9aa6-c4f9cb290212"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline accuracy: 79.27, New accuracy: 76.64, Difference: -2.62\n",
      "Baseline precision: 0.81, New precision: 0.77, Difference: -0.04\n",
      "Baseline recall: 0.79, New recall: 0.77, Difference: -0.03\n",
      "Baseline f1: 0.79, New f1: 0.77, Difference: -0.02\n"
     ]
    }
   ],
   "source": [
    "# Check to see how the bidirectional model performs against the baseline\n",
    "compare_baseline_to_new_results(baseline_results, model_4_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wcvt_7emuKlR"
   },
   "source": [
    "## Convolutional Neural Networks for Text\n",
    "\n",
    "You might've used convolutional neural networks (CNNs) for images before but they can also be used for sequences.\n",
    "\n",
    "The main difference between using CNNs for images and sequences is the shape of the data. Images come in 2-dimensions (height x width) where as sequences are often 1-dimensional (a string of text).\n",
    "\n",
    "So to use CNNs with sequences, we use a 1-dimensional convolution instead of a 2-dimensional convolution.\n",
    "\n",
    "A typical CNN architecture for sequences will look like the following: \n",
    "\n",
    "```\n",
    "Inputs (text) -> Tokenization -> Embedding -> Layers -> Outputs (class probabilities)\n",
    "```\n",
    "\n",
    "You might be thinking \"that just looks like the architecture layout we've been using for the other models...\"\n",
    "\n",
    "And you'd be right.\n",
    "\n",
    "The difference again is in the layers component. Instead of using an LSTM or GRU cell, we're going to use a [`tensorflow.keras.layers.Conv1D()`](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Conv1D) layer followed by a [`tensorflow.keras.layers.GlobablMaxPool1D()`](https://www.tensorflow.org/api_docs/python/tf/keras/layers/GlobalMaxPool1D) layer.\n",
    "\n",
    "> 📖 **Resource:** The intuition here is explained succinctly in the paper [*Understanding Convolutional Neural Networks for Text Classification*](https://www.aclweb.org/anthology/W18-5408.pdf), where they state that CNNs classify text through the following steps:\n",
    "1. 1-dimensional convolving filters are used as ngram detectors, each filter specializing in a closely-related family of ngrams (an ngram is a collection of n-words, for example, an ngram of 5 might result in \"hello, my name is Daniel\").\n",
    "2. Max-pooling over time extracts the relevant ngrams for making a decision.\n",
    "3. The rest of the network classifies the text based on this information.\n",
    "\n",
    "> \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lgXEorf9GWY1"
   },
   "source": [
    "### Model 5: Conv1D\n",
    "\n",
    "Before we build a full 1-dimensional CNN model, let's see a 1-dimensional convolutional layer (also called a **temporal convolution**) in action.\n",
    "\n",
    "We'll first create an embedding of a sample of text and experiment passing it through a `Conv1D()` layer and `GlobalMaxPool1D()` layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "563hl7nPWP_3",
    "outputId": "1095e539-101e-4707-9538-48cf42a3afa1"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-22 17:28:17.520591: W tensorflow/stream_executor/gpu/asm_compiler.cc:63] Running ptxas --version returned 256\n",
      "2022-08-22 17:28:17.559899: W tensorflow/stream_executor/gpu/redzone_allocator.cc:314] Internal: ptxas exited with non-zero error code 256, output: \n",
      "Relying on driver to perform ptx compilation. \n",
      "Modify $PATH to customize ptxas location.\n",
      "This message will be only logged once.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(TensorShape([1, 15, 128]), TensorShape([1, 11, 32]), TensorShape([1, 32]))"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test out the embedding, 1D convolutional and max pooling\n",
    "embedding_test = embedding(text_vectorizer([\"this is a test sentence\"])) # turn target sentence into embedding\n",
    "conv_1d = layers.Conv1D(filters=32, kernel_size=5, activation=\"relu\") # convolve over target sequence 5 words at a time\n",
    "conv_1d_output = conv_1d(embedding_test) # pass embedding through 1D convolutional layer\n",
    "max_pool = layers.GlobalMaxPool1D() \n",
    "max_pool_output = max_pool(conv_1d_output) # get the most important features\n",
    "embedding_test.shape, conv_1d_output.shape, max_pool_output.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-WzTeShEemJ2"
   },
   "source": [
    "Notice the output shapes of each layer.\n",
    "\n",
    "The embedding has an output shape dimension of the parameters we set it to (`input_length=15` and `output_dim=128`).\n",
    "\n",
    "The 1-dimensional convolutional layer has an output which has been compressed inline with its parameters. And the same goes for the max pooling layer output.\n",
    "\n",
    "Our text starts out as a string but gets converted to a feature vector of length 64 through various transformation steps (from tokenization to embedding to 1-dimensional convolution to max pool).\n",
    "\n",
    "Let's take a peek at what each of these transformations looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gRcxYgs-dxM8",
    "outputId": "2e7dc780-d2b5-4413-8895-723a3354936c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(1, 15, 128), dtype=float32, numpy=\n",
       " array([[[ 0.02534915, -0.03109056,  0.00285616, ..., -0.00783164,\n",
       "          -0.02685578, -0.04434134],\n",
       "         [-0.06586259,  0.09451496, -0.01477603, ..., -0.00657783,\n",
       "          -0.04238791,  0.07777896],\n",
       "         [-0.04803651, -0.00709754, -0.02330894, ..., -0.01807334,\n",
       "           0.02351035,  0.02676386],\n",
       "         ...,\n",
       "         [ 0.00073161,  0.01504799, -0.03425453, ..., -0.04403543,\n",
       "          -0.01042276,  0.01876437],\n",
       "         [ 0.00073161,  0.01504799, -0.03425453, ..., -0.04403543,\n",
       "          -0.01042276,  0.01876437],\n",
       "         [ 0.00073161,  0.01504799, -0.03425453, ..., -0.04403543,\n",
       "          -0.01042276,  0.01876437]]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(1, 11, 32), dtype=float32, numpy=\n",
       " array([[[0.08324987, 0.00648713, 0.        , 0.03983571, 0.        ,\n",
       "          0.01144418, 0.00416252, 0.02288384, 0.        , 0.00900975,\n",
       "          0.        , 0.        , 0.03401769, 0.06408271, 0.08103721,\n",
       "          0.00409015, 0.01579618, 0.        , 0.07930174, 0.        ,\n",
       "          0.        , 0.        , 0.14525083, 0.        , 0.        ,\n",
       "          0.        , 0.03682074, 0.06534284, 0.        , 0.        ,\n",
       "          0.05094628, 0.        ],\n",
       "         [0.        , 0.05387187, 0.        , 0.11491334, 0.        ,\n",
       "          0.        , 0.16237086, 0.        , 0.        , 0.00171247,\n",
       "          0.14336714, 0.        , 0.        , 0.        , 0.        ,\n",
       "          0.01197935, 0.        , 0.        , 0.1355136 , 0.00401052,\n",
       "          0.1030983 , 0.09445542, 0.0839029 , 0.        , 0.0421304 ,\n",
       "          0.04487599, 0.06560452, 0.        , 0.02272682, 0.        ,\n",
       "          0.        , 0.        ],\n",
       "         [0.03683228, 0.0489576 , 0.        , 0.15324757, 0.        ,\n",
       "          0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "          0.        , 0.04650322, 0.00496453, 0.07349397, 0.01608639,\n",
       "          0.        , 0.0277912 , 0.        , 0.08080553, 0.01403162,\n",
       "          0.        , 0.03768811, 0.10382764, 0.        , 0.03361665,\n",
       "          0.        , 0.02577595, 0.00140343, 0.        , 0.        ,\n",
       "          0.03211503, 0.        ],\n",
       "         [0.00887835, 0.10450967, 0.        , 0.06974541, 0.02328688,\n",
       "          0.        , 0.04052208, 0.        , 0.        , 0.02733754,\n",
       "          0.08674348, 0.        , 0.        , 0.06129848, 0.02007263,\n",
       "          0.        , 0.        , 0.        , 0.03364258, 0.        ,\n",
       "          0.04525349, 0.05219699, 0.06375694, 0.        , 0.        ,\n",
       "          0.00774412, 0.00273457, 0.        , 0.        , 0.00499646,\n",
       "          0.        , 0.        ],\n",
       "         [0.        , 0.02369065, 0.        , 0.05827622, 0.05297645,\n",
       "          0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "          0.01719723, 0.02936826, 0.00466107, 0.06879892, 0.01944806,\n",
       "          0.01585536, 0.01294544, 0.        , 0.06866522, 0.        ,\n",
       "          0.00623778, 0.03514046, 0.02407517, 0.        , 0.0597982 ,\n",
       "          0.        , 0.01170137, 0.        , 0.        , 0.        ,\n",
       "          0.04444933, 0.        ],\n",
       "         [0.03544877, 0.        , 0.        , 0.05054981, 0.06105442,\n",
       "          0.        , 0.00997427, 0.01403004, 0.        , 0.01680719,\n",
       "          0.03148514, 0.03889389, 0.        , 0.07710688, 0.00590962,\n",
       "          0.        , 0.00263031, 0.        , 0.08935821, 0.        ,\n",
       "          0.        , 0.05331147, 0.05227932, 0.        , 0.06658385,\n",
       "          0.01881713, 0.02448691, 0.        , 0.        , 0.        ,\n",
       "          0.02008455, 0.        ],\n",
       "         [0.03544877, 0.        , 0.        , 0.0505498 , 0.06105442,\n",
       "          0.        , 0.00997426, 0.01403006, 0.        , 0.01680719,\n",
       "          0.03148514, 0.0388939 , 0.        , 0.07710689, 0.00590963,\n",
       "          0.        , 0.00263031, 0.        , 0.08935821, 0.        ,\n",
       "          0.        , 0.05331148, 0.05227932, 0.        , 0.06658386,\n",
       "          0.01881713, 0.02448689, 0.        , 0.        , 0.        ,\n",
       "          0.02008456, 0.        ],\n",
       "         [0.03544879, 0.        , 0.        , 0.05054981, 0.06105442,\n",
       "          0.        , 0.00997425, 0.01403006, 0.        , 0.01680718,\n",
       "          0.03148513, 0.0388939 , 0.        , 0.07710689, 0.00590963,\n",
       "          0.        , 0.00263032, 0.        , 0.08935821, 0.        ,\n",
       "          0.        , 0.05331148, 0.05227932, 0.        , 0.06658384,\n",
       "          0.01881713, 0.0244869 , 0.        , 0.        , 0.        ,\n",
       "          0.02008456, 0.        ],\n",
       "         [0.03544879, 0.        , 0.        , 0.0505498 , 0.06105443,\n",
       "          0.        , 0.00997425, 0.01403005, 0.        , 0.01680719,\n",
       "          0.03148513, 0.03889389, 0.        , 0.07710689, 0.00590963,\n",
       "          0.        , 0.00263031, 0.        , 0.0893582 , 0.        ,\n",
       "          0.        , 0.05331147, 0.05227933, 0.        , 0.06658386,\n",
       "          0.01881713, 0.02448691, 0.        , 0.        , 0.        ,\n",
       "          0.02008456, 0.        ],\n",
       "         [0.03544879, 0.        , 0.        , 0.05054981, 0.06105442,\n",
       "          0.        , 0.00997427, 0.01403006, 0.        , 0.01680719,\n",
       "          0.03148513, 0.0388939 , 0.        , 0.07710689, 0.00590962,\n",
       "          0.        , 0.00263031, 0.        , 0.08935821, 0.        ,\n",
       "          0.        , 0.05331147, 0.05227932, 0.        , 0.06658386,\n",
       "          0.01881713, 0.02448691, 0.        , 0.        , 0.        ,\n",
       "          0.02008455, 0.        ],\n",
       "         [0.03544879, 0.        , 0.        , 0.0505498 , 0.06105442,\n",
       "          0.        , 0.00997426, 0.01403005, 0.        , 0.01680719,\n",
       "          0.03148513, 0.03889389, 0.        , 0.07710689, 0.00590962,\n",
       "          0.        , 0.00263031, 0.        , 0.0893582 , 0.        ,\n",
       "          0.        , 0.05331149, 0.05227932, 0.        , 0.06658386,\n",
       "          0.01881712, 0.0244869 , 0.        , 0.        , 0.        ,\n",
       "          0.02008456, 0.        ]]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(1, 32), dtype=float32, numpy=\n",
       " array([[0.08324987, 0.10450967, 0.        , 0.15324757, 0.06105443,\n",
       "         0.01144418, 0.16237086, 0.02288384, 0.        , 0.02733754,\n",
       "         0.14336714, 0.04650322, 0.03401769, 0.07710689, 0.08103721,\n",
       "         0.01585536, 0.0277912 , 0.        , 0.1355136 , 0.01403162,\n",
       "         0.1030983 , 0.09445542, 0.14525083, 0.        , 0.06658386,\n",
       "         0.04487599, 0.06560452, 0.06534284, 0.02272682, 0.00499646,\n",
       "         0.05094628, 0.        ]], dtype=float32)>)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# See the outputs of each layer\n",
    "embedding_test[:1], conv_1d_output[:1], max_pool_output[:1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kMcrthJwg3B2"
   },
   "source": [
    "Alright, we've seen the outputs of several components of a CNN for sequences, let's put them together and construct a full model, compile it (just as we've done with our other models) and get a summary. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "G9aphPWCYkWN",
    "outputId": "8bb43256-5421-4b0d-9639-574bd6649d0e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_5_Conv1D\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_5 (InputLayer)         [(None, 1)]               0         \n",
      "_________________________________________________________________\n",
      "text_vectorization_1 (TextVe (None, 15)                0         \n",
      "_________________________________________________________________\n",
      "embedding_5 (Embedding)      (None, 15, 128)           1280000   \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 11, 32)            20512     \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_1 (Glob (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 1,300,545\n",
      "Trainable params: 1,300,545\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Set random seed and create embedding layer (new embedding layer for each model)\n",
    "tf.random.set_seed(42)\n",
    "from tensorflow.keras import layers\n",
    "model_5_embedding = layers.Embedding(input_dim=max_vocab_length,\n",
    "                                     output_dim=128,\n",
    "                                     embeddings_initializer=\"uniform\",\n",
    "                                     input_length=max_length,\n",
    "                                     name=\"embedding_5\")\n",
    "\n",
    "# Create 1-dimensional convolutional layer to model sequences\n",
    "from tensorflow.keras import layers\n",
    "inputs = layers.Input(shape=(1,), dtype=\"string\")\n",
    "x = text_vectorizer(inputs)\n",
    "x = model_5_embedding(x)\n",
    "x = layers.Conv1D(filters=32, kernel_size=5, activation=\"relu\")(x)\n",
    "x = layers.GlobalMaxPool1D()(x)\n",
    "# x = layers.Dense(64, activation=\"relu\")(x) # optional dense layer\n",
    "outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "model_5 = tf.keras.Model(inputs, outputs, name=\"model_5_Conv1D\")\n",
    "\n",
    "# Compile Conv1D model\n",
    "model_5.compile(loss=\"binary_crossentropy\",\n",
    "                optimizer=tf.keras.optimizers.Adam(),\n",
    "                metrics=[\"accuracy\"])\n",
    "\n",
    "# Get a summary of our 1D convolution model\n",
    "model_5.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o1Y4BpMGh0jG"
   },
   "source": [
    "Woohoo! Looking great! Notice how the number of trainable parameters for the 1-dimensional convolutional layer is similar to that of the LSTM layer in `model_2`.\n",
    "\n",
    "Let's fit our 1D CNN model to our text data. In line with previous experiments, we'll save its results using our `create_tensorboard_callback()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9fzlaKm1ZrMX",
    "outputId": "bf7828c5-a720-45a7-d09b-854a33d37638"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving TensorBoard log files to: model_logs/Conv1D/20220822-172818\n",
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-22 17:28:18.124732: I tensorflow/core/profiler/lib/profiler_session.cc:136] Profiler session initializing.\n",
      "2022-08-22 17:28:18.124756: I tensorflow/core/profiler/lib/profiler_session.cc:155] Profiler session started.\n",
      "2022-08-22 17:28:18.124921: I tensorflow/core/profiler/lib/profiler_session.cc:172] Profiler session tear down.\n",
      "2022-08-22 17:28:18.124981: I tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1487] CUPTI activity buffer flushed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  3/215 [..............................] - ETA: 18s - loss: 0.6883 - accuracy: 0.5729 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-22 17:28:19.800984: I tensorflow/core/profiler/lib/profiler_session.cc:136] Profiler session initializing.\n",
      "2022-08-22 17:28:19.801011: I tensorflow/core/profiler/lib/profiler_session.cc:155] Profiler session started.\n",
      "2022-08-22 17:28:19.910136: I tensorflow/core/profiler/lib/profiler_session.cc:71] Profiler session collecting data.\n",
      "2022-08-22 17:28:19.910416: I tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1487] CUPTI activity buffer flushed\n",
      "2022-08-22 17:28:19.912272: I tensorflow/core/profiler/internal/gpu/cupti_collector.cc:228]  GpuTracer has collected 145 callback api events and 133 activity events. \n",
      "2022-08-22 17:28:19.917908: I tensorflow/core/profiler/lib/profiler_session.cc:172] Profiler session tear down.\n",
      "2022-08-22 17:28:19.922104: I tensorflow/core/profiler/rpc/client/save_profile.cc:137] Creating directory: model_logs/Conv1D/20220822-172818/train/plugins/profile/2022_08_22_17_28_19\n",
      "2022-08-22 17:28:19.925378: I tensorflow/core/profiler/rpc/client/save_profile.cc:143] Dumped gzipped tool data for trace.json.gz to model_logs/Conv1D/20220822-172818/train/plugins/profile/2022_08_22_17_28_19/ubuntu-All-Series.trace.json.gz\n",
      "2022-08-22 17:28:19.935086: I tensorflow/core/profiler/rpc/client/save_profile.cc:137] Creating directory: model_logs/Conv1D/20220822-172818/train/plugins/profile/2022_08_22_17_28_19\n",
      "2022-08-22 17:28:19.937582: I tensorflow/core/profiler/rpc/client/save_profile.cc:143] Dumped gzipped tool data for memory_profile.json.gz to model_logs/Conv1D/20220822-172818/train/plugins/profile/2022_08_22_17_28_19/ubuntu-All-Series.memory_profile.json.gz\n",
      "2022-08-22 17:28:19.938180: I tensorflow/core/profiler/rpc/client/capture_profile.cc:251] Creating directory: model_logs/Conv1D/20220822-172818/train/plugins/profile/2022_08_22_17_28_19Dumped tool data for xplane.pb to model_logs/Conv1D/20220822-172818/train/plugins/profile/2022_08_22_17_28_19/ubuntu-All-Series.xplane.pb\n",
      "Dumped tool data for overview_page.pb to model_logs/Conv1D/20220822-172818/train/plugins/profile/2022_08_22_17_28_19/ubuntu-All-Series.overview_page.pb\n",
      "Dumped tool data for input_pipeline.pb to model_logs/Conv1D/20220822-172818/train/plugins/profile/2022_08_22_17_28_19/ubuntu-All-Series.input_pipeline.pb\n",
      "Dumped tool data for tensorflow_stats.pb to model_logs/Conv1D/20220822-172818/train/plugins/profile/2022_08_22_17_28_19/ubuntu-All-Series.tensorflow_stats.pb\n",
      "Dumped tool data for kernel_stats.pb to model_logs/Conv1D/20220822-172818/train/plugins/profile/2022_08_22_17_28_19/ubuntu-All-Series.kernel_stats.pb\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "215/215 [==============================] - 6s 22ms/step - loss: 0.6309 - accuracy: 0.6450 - val_loss: 0.4733 - val_accuracy: 0.7795\n",
      "Epoch 2/5\n",
      "215/215 [==============================] - 3s 14ms/step - loss: 0.3418 - accuracy: 0.8647 - val_loss: 0.4758 - val_accuracy: 0.7730\n",
      "Epoch 3/5\n",
      "215/215 [==============================] - 3s 14ms/step - loss: 0.2026 - accuracy: 0.9268 - val_loss: 0.5457 - val_accuracy: 0.7730\n",
      "Epoch 4/5\n",
      "215/215 [==============================] - 3s 14ms/step - loss: 0.1249 - accuracy: 0.9594 - val_loss: 0.6163 - val_accuracy: 0.7730\n",
      "Epoch 5/5\n",
      "215/215 [==============================] - 3s 14ms/step - loss: 0.0853 - accuracy: 0.9724 - val_loss: 0.6779 - val_accuracy: 0.7782\n"
     ]
    }
   ],
   "source": [
    "# Fit the model\n",
    "model_5_history = model_5.fit(train_sentences,\n",
    "                              train_labels,\n",
    "                              epochs=5,\n",
    "                              validation_data=(val_sentences, val_labels),\n",
    "                              callbacks=[create_tensorboard_callback(SAVE_DIR, \n",
    "                                                                     \"Conv1D\")])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d2up-1tLiXKD"
   },
   "source": [
    "Nice! Thanks to GPU acceleration, our 1D convolutional model trains nice and fast. Let's make some predictions with it and evaluate them just as before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZHYw5GkxZ2OK",
    "outputId": "e994aad4-2f46-43e6-88ac-bcf4aa0373c6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.2253448 ],\n",
       "       [0.7534111 ],\n",
       "       [0.9995602 ],\n",
       "       [0.05562792],\n",
       "       [0.01449849],\n",
       "       [0.9858518 ],\n",
       "       [0.98418933],\n",
       "       [0.99758804],\n",
       "       [0.99862623],\n",
       "       [0.26914376]], dtype=float32)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make predictions with model_5\n",
    "model_5_pred_probs = model_5.predict(val_sentences)\n",
    "model_5_pred_probs[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "v9YqTtjiaauS",
    "outputId": "adaaab91-d793-4432-9180-5dadc47ddd5f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(10,), dtype=float32, numpy=array([0., 1., 1., 0., 0., 1., 1., 1., 1., 0.], dtype=float32)>"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert model_5 prediction probabilities to labels\n",
    "model_5_preds = tf.squeeze(tf.round(model_5_pred_probs))\n",
    "model_5_preds[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wMY3s1Pnaj34",
    "outputId": "2a9a8f52-1936-4bac-f23e-47d0c3fa0ea3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 77.82152230971128,\n",
       " 'precision': 0.7807522349051432,\n",
       " 'recall': 0.7782152230971129,\n",
       " 'f1': 0.7758810170952618}"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate model_5 evaluation metrics \n",
    "model_5_results = calculate_results(y_true=val_labels, \n",
    "                                    y_pred=model_5_preds)\n",
    "model_5_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wRfF4B6_at8k",
    "outputId": "cfe5ed3b-9de5-4734-e2e2-66c527d540a9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline accuracy: 79.27, New accuracy: 77.82, Difference: -1.44\n",
      "Baseline precision: 0.81, New precision: 0.78, Difference: -0.03\n",
      "Baseline recall: 0.79, New recall: 0.78, Difference: -0.01\n",
      "Baseline f1: 0.79, New f1: 0.78, Difference: -0.01\n"
     ]
    }
   ],
   "source": [
    "# Compare model_5 results to baseline \n",
    "compare_baseline_to_new_results(baseline_results, model_5_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g_roVSSRt-7h"
   },
   "source": [
    "## Using Pretrained Embeddings (transfer learning for NLP)\n",
    "\n",
    "For all of the previous deep learning models we've built and trained, we've created and used our own embeddings from scratch each time.\n",
    "\n",
    "However, a common practice is to leverage pretrained embeddings through **transfer learning**. This is one of the main benefits of using deep models: being able to take what one (often larger) model has learned (often on a large amount of data) and adjust it for our own use case.\n",
    "\n",
    "For our next model, instead of using our own embedding layer, we're going to replace it with a pretrained embedding layer.\n",
    "\n",
    "More specifically, we're going to be using the [Universal Sentence Encoder](https://www.aclweb.org/anthology/D18-2029.pdf) from [TensorFlow Hub](https://tfhub.dev/google/universal-sentence-encoder/4) (a great resource containing a plethora of pretrained model resources for a variety of tasks).\n",
    "\n",
    "> 🔑 **Note:** There are many different pretrained text embedding options on TensorFlow Hub, however, some require different levels of text preprocessing than others. Best to experiment with a few and see which best suits your use case.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R-NQ2MA5GZBo"
   },
   "source": [
    "### Model 6: TensorFlow Hub Pretrained Sentence Encoder\n",
    "\n",
    "The main difference between the embedding layer we created and the Universal Sentence Encoder is that rather than create a word-level embedding, the Universal Sentence Encoder, as you might've guessed, creates a whole sentence-level embedding.\n",
    "\n",
    "Our embedding layer also outputs an a 128 dimensional vector for each word, where as, the Universal Sentence Encoder outputs a 512 dimensional vector for each sentence.\n",
    "\n",
    "![](https://raw.githubusercontent.com/mrdbourke/tensorflow-deep-learning/main/images/08-USE-tensorflow-hub-encoder-decoder-model.png)\n",
    "*The feature extractor model we're building through the eyes of an **encoder/decoder** model.*\n",
    "\n",
    "> 🔑 **Note:** An **encoder** is the name for a model which converts raw data such as text into a numerical representation (feature vector), a **decoder** converts the numerical representation to a desired output.\n",
    "\n",
    "As usual, this is best demonstrated with an example.\n",
    "\n",
    "We can load in a TensorFlow Hub module using the [`hub.load()`](https://www.tensorflow.org/hub/api_docs/python/hub/load) method and passing it the target URL of the module we'd like to use, in our case, it's \"https://tfhub.dev/google/universal-sentence-encoder/4\".\n",
    "\n",
    "Let's load the Universal Sentence Encoder model and test it on a couple of sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7piW5jtxbUkV",
    "outputId": "b2e44c0e-3a6b-4fd7-d884-df68fd34cfcb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[-0.01157032  0.02485909  0.02878049 -0.01271501  0.03971539  0.08827762\n",
      "  0.02680985  0.05589838 -0.0106873  -0.00597291  0.00639325 -0.0181952\n",
      "  0.00030816  0.09105889  0.05874644 -0.03180628  0.01512473 -0.05162929\n",
      "  0.00991365 -0.06865346 -0.04209306  0.02678981  0.03011008  0.00321067\n",
      " -0.0033797  -0.04787361  0.02266723 -0.00985925 -0.04063613 -0.0129209\n",
      " -0.04666385  0.056303   -0.03949255  0.00517688  0.02495828 -0.07014444\n",
      "  0.02871508  0.04947681 -0.00633976 -0.08960193  0.02807116 -0.00808363\n",
      " -0.01360604  0.0599865  -0.10361787 -0.05195372  0.00232956 -0.02332528\n",
      " -0.03758106  0.03327731], shape=(50,), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# Example of pretrained embedding with universal sentence encoder - https://tfhub.dev/google/universal-sentence-encoder/4\n",
    "import tensorflow_hub as hub\n",
    "embed = hub.load(\"https://tfhub.dev/google/universal-sentence-encoder/4\") # load Universal Sentence Encoder\n",
    "embed_samples = embed([sample_sentence,\n",
    "                      \"When you call the universal sentence encoder on a sentence, it turns it into numbers.\"])\n",
    "\n",
    "print(embed_samples[0][:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vvArnKkGb4vu",
    "outputId": "4d7860c4-1da8-48f0-9b38-efecf802b831"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([512])"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Each sentence has been encoded into a 512 dimension vector\n",
    "embed_samples[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZxYFDkGD-XjF"
   },
   "source": [
    "Passing our sentences to the Universal Sentence Encoder (USE) encodes them from strings to 512 dimensional vectors, which make no sense to us but hopefully make sense to our machine learning models.\n",
    "\n",
    "Speaking of models, let's build one with the USE as our embedding layer.\n",
    "\n",
    "We can convert the TensorFlow Hub USE module into a Keras layer using the [`hub.KerasLayer`](https://www.tensorflow.org/hub/api_docs/python/hub/KerasLayer) class.\n",
    "\n",
    "> 🔑 **Note:** Due to the size of the USE TensorFlow Hub module, it may take a little while to download. Once it's downloaded though, it'll be cached and ready to use. And as with many TensorFlow Hub modules, there is a [\"lite\" version of the USE](https://tfhub.dev/google/universal-sentence-encoder-lite/2) which takes up less space but sacrifices some performance and requires more preprocessing steps. However, depending on your available compute power, the lite version may be better for your application use case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "id": "ZcbBj0aXqrs9"
   },
   "outputs": [],
   "source": [
    "# We can use this encoding layer in place of our text_vectorizer and embedding layer\n",
    "sentence_encoder_layer = hub.KerasLayer(\"https://tfhub.dev/google/universal-sentence-encoder/4\",\n",
    "                                        input_shape=[], # shape of inputs coming to our model, this model takes variable lengths so that's why we have '[]' \n",
    "                                        dtype=tf.string, # data type of inputs coming to the USE layer\n",
    "                                        trainable=False, # keep the pretrained weights (we'll create a feature extractor)\n",
    "                                        name=\"USE\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WvjQl4p7BO_A"
   },
   "source": [
    "Beautiful! Now we've got the USE as a Keras layer, we can use it in a Keras Sequential model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "M_pjIvPuYltA",
    "outputId": "80b0d1a5-0557-477b-e97b-19be3596f286"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <bound method KerasLayer.call of <tensorflow_hub.keras_layer.KerasLayer object at 0x7f44c9329990>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Index'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <bound method KerasLayer.call of <tensorflow_hub.keras_layer.KerasLayer object at 0x7f44c9329990>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Index'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: AutoGraph could not transform <bound method KerasLayer.call of <tensorflow_hub.keras_layer.KerasLayer object at 0x7f44c9329990>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Index'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Model: \"model_6_USE\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "USE (KerasLayer)             (None, 512)               256797824 \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 64)                32832     \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 256,830,721\n",
      "Trainable params: 32,897\n",
      "Non-trainable params: 256,797,824\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# We don't need the tokenization or embed layers since the pre-trained model does all the pre-processing for us.\n",
    "\n",
    "# Create model using the Sequential API\n",
    "model_6 = tf.keras.Sequential([\n",
    "  sentence_encoder_layer, # take in sentences and then encode them into an embedding\n",
    "  layers.Dense(64, activation=\"relu\"),\n",
    "  layers.Dense(1, activation=\"sigmoid\")\n",
    "], name=\"model_6_USE\")\n",
    "\n",
    "# Compile model\n",
    "model_6.compile(loss=\"binary_crossentropy\",\n",
    "                optimizer=tf.keras.optimizers.Adam(),\n",
    "                metrics=[\"accuracy\"])\n",
    "\n",
    "model_6.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yukgxOgCCR2Z"
   },
   "source": [
    "Notice the number of paramters in the USE layer, these are the pretrained weights its learned on various text sources (Wikipedia, web news, web question-answer forums, etc, see the [Universal Sentence Encoder paper](https://www.aclweb.org/anthology/D18-2029.pdf) for more).\n",
    "\n",
    "The trainable parameters are only in our output layers, in other words, we're keeping the USE weights frozen and using it as a feature-extractor. We could fine-tune these weights by setting `trainable=True` when creating the `hub.KerasLayer` instance.\n",
    "\n",
    "Now we've got a feature extractor model ready, let's train it and track its results to TensorBoard using our `create_tensorboard_callback()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uX9S0YvafybG",
    "outputId": "38c99b5c-4581-4374-a07f-e9f240d5e4f8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving TensorBoard log files to: model_logs/tf_hub_sentence_encoder/20220822-175532\n",
      "Epoch 1/5\n",
      "\r",
      "  1/215 [..............................] - ETA: 2s - loss: 0.0762 - accuracy: 1.0000"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-22 17:55:32.194867: I tensorflow/core/profiler/lib/profiler_session.cc:136] Profiler session initializing.\n",
      "2022-08-22 17:55:32.194905: I tensorflow/core/profiler/lib/profiler_session.cc:155] Profiler session started.\n",
      "2022-08-22 17:55:32.195085: I tensorflow/core/profiler/lib/profiler_session.cc:172] Profiler session tear down.\n",
      "2022-08-22 17:55:32.195171: I tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1487] CUPTI activity buffer flushed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 16/215 [=>............................] - ETA: 4s - loss: 0.1310 - accuracy: 0.9668"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-22 17:55:32.409934: I tensorflow/core/profiler/lib/profiler_session.cc:136] Profiler session initializing.\n",
      "2022-08-22 17:55:32.409960: I tensorflow/core/profiler/lib/profiler_session.cc:155] Profiler session started.\n",
      "2022-08-22 17:55:32.424043: I tensorflow/core/profiler/lib/profiler_session.cc:71] Profiler session collecting data.\n",
      "2022-08-22 17:55:32.424431: I tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1487] CUPTI activity buffer flushed\n",
      "2022-08-22 17:55:32.427844: I tensorflow/core/profiler/internal/gpu/cupti_collector.cc:228]  GpuTracer has collected 283 callback api events and 271 activity events. \n",
      "2022-08-22 17:55:32.439916: I tensorflow/core/profiler/lib/profiler_session.cc:172] Profiler session tear down.\n",
      "2022-08-22 17:55:32.447717: I tensorflow/core/profiler/rpc/client/save_profile.cc:137] Creating directory: model_logs/tf_hub_sentence_encoder/20220822-175532/train/plugins/profile/2022_08_22_17_55_32\n",
      "2022-08-22 17:55:32.454502: I tensorflow/core/profiler/rpc/client/save_profile.cc:143] Dumped gzipped tool data for trace.json.gz to model_logs/tf_hub_sentence_encoder/20220822-175532/train/plugins/profile/2022_08_22_17_55_32/ubuntu-All-Series.trace.json.gz\n",
      "2022-08-22 17:55:32.477157: I tensorflow/core/profiler/rpc/client/save_profile.cc:137] Creating directory: model_logs/tf_hub_sentence_encoder/20220822-175532/train/plugins/profile/2022_08_22_17_55_32\n",
      "2022-08-22 17:55:32.480657: I tensorflow/core/profiler/rpc/client/save_profile.cc:143] Dumped gzipped tool data for memory_profile.json.gz to model_logs/tf_hub_sentence_encoder/20220822-175532/train/plugins/profile/2022_08_22_17_55_32/ubuntu-All-Series.memory_profile.json.gz\n",
      "2022-08-22 17:55:32.481637: I tensorflow/core/profiler/rpc/client/capture_profile.cc:251] Creating directory: model_logs/tf_hub_sentence_encoder/20220822-175532/train/plugins/profile/2022_08_22_17_55_32Dumped tool data for xplane.pb to model_logs/tf_hub_sentence_encoder/20220822-175532/train/plugins/profile/2022_08_22_17_55_32/ubuntu-All-Series.xplane.pb\n",
      "Dumped tool data for overview_page.pb to model_logs/tf_hub_sentence_encoder/20220822-175532/train/plugins/profile/2022_08_22_17_55_32/ubuntu-All-Series.overview_page.pb\n",
      "Dumped tool data for input_pipeline.pb to model_logs/tf_hub_sentence_encoder/20220822-175532/train/plugins/profile/2022_08_22_17_55_32/ubuntu-All-Series.input_pipeline.pb\n",
      "Dumped tool data for tensorflow_stats.pb to model_logs/tf_hub_sentence_encoder/20220822-175532/train/plugins/profile/2022_08_22_17_55_32/ubuntu-All-Series.tensorflow_stats.pb\n",
      "Dumped tool data for kernel_stats.pb to model_logs/tf_hub_sentence_encoder/20220822-175532/train/plugins/profile/2022_08_22_17_55_32/ubuntu-All-Series.kernel_stats.pb\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "215/215 [==============================] - 2s 11ms/step - loss: 0.1482 - accuracy: 0.9550 - val_loss: 0.5212 - val_accuracy: 0.8123\n",
      "Epoch 2/5\n",
      "215/215 [==============================] - 2s 8ms/step - loss: 0.1415 - accuracy: 0.9596 - val_loss: 0.5297 - val_accuracy: 0.8097\n",
      "Epoch 3/5\n",
      "215/215 [==============================] - 2s 9ms/step - loss: 0.1341 - accuracy: 0.9600 - val_loss: 0.5313 - val_accuracy: 0.8123\n",
      "Epoch 4/5\n",
      "215/215 [==============================] - 2s 9ms/step - loss: 0.1287 - accuracy: 0.9642 - val_loss: 0.5429 - val_accuracy: 0.8031\n",
      "Epoch 5/5\n",
      "215/215 [==============================] - 2s 9ms/step - loss: 0.1229 - accuracy: 0.9667 - val_loss: 0.5471 - val_accuracy: 0.8123\n"
     ]
    }
   ],
   "source": [
    "# Train a classifier on top of pretrained embeddings\n",
    "model_6_history = model_6.fit(train_sentences,\n",
    "                              train_labels,\n",
    "                              epochs=5,\n",
    "                              validation_data=(val_sentences, val_labels),\n",
    "                              callbacks=[create_tensorboard_callback(SAVE_DIR, \n",
    "                                                                     \"tf_hub_sentence_encoder\")])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KeI0kvVVDmbl"
   },
   "source": [
    "USE model trained! Let's make some predictions with it an evaluate them as we've done with our other models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xeyNXqU-gM2p",
    "outputId": "0111cacc-4f82-4b44-bbee-6c9e5f0bfe14"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.02558996],\n",
       "       [0.78501576],\n",
       "       [0.99984753],\n",
       "       [0.14509375],\n",
       "       [0.43127716],\n",
       "       [0.91624564],\n",
       "       [0.9988323 ],\n",
       "       [0.9994696 ],\n",
       "       [0.9892189 ],\n",
       "       [0.00127925]], dtype=float32)"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make predictions with USE TF Hub model\n",
    "model_6_pred_probs = model_6.predict(val_sentences)\n",
    "model_6_pred_probs[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Gbn1Z0FfgVdx",
    "outputId": "38cddcfb-cb8d-48bb-c06f-a6ef275bf62b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(10,), dtype=float32, numpy=array([0., 1., 1., 0., 0., 1., 1., 1., 1., 0.], dtype=float32)>"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert prediction probabilities to labels\n",
    "model_6_preds = tf.squeeze(tf.round(model_6_pred_probs))\n",
    "model_6_preds[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "N2Ow2de3okcb",
    "outputId": "c836fc36-b57a-4a95-8ab6-742238a8e3bb"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 81.23359580052494,\n",
       " 'precision': 0.8152105639975652,\n",
       " 'recall': 0.8123359580052494,\n",
       " 'f1': 0.8105808468856647}"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate model 6 performance metrics\n",
    "model_6_results = calculate_results(val_labels, model_6_preds)\n",
    "model_6_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-BHnRHHHgp1r",
    "outputId": "17cd3dec-b0ce-471a-a8b6-81c282cd8dfa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline accuracy: 79.27, New accuracy: 82.02, Difference: 2.76\n",
      "Baseline precision: 0.81, New precision: 0.82, Difference: 0.01\n",
      "Baseline recall: 0.79, New recall: 0.82, Difference: 0.03\n",
      "Baseline f1: 0.79, New f1: 0.82, Difference: 0.03\n"
     ]
    }
   ],
   "source": [
    "# Compare TF Hub model to baseline\n",
    "compare_baseline_to_new_results(baseline_results, model_6_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LHwu4QjijYWG"
   },
   "source": [
    "### Model 7: TensorFlow Hub Pretrained Sentence Encoder 10% of the training data\n",
    "\n",
    "One of the benefits of using transfer learning methods, such as, the pretrained embeddings within the USE is the ability to get great results on a small amount of data (the USE paper even mentions this in the abstract).\n",
    "\n",
    "To put this to the test, we're going to make a small subset of the training data (10%), train a model and evaluate it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "W5Sal8DpjzWm"
   },
   "outputs": [],
   "source": [
    "### NOTE: Making splits like this will lead to data leakage ###\n",
    "### (some of the training examples in the validation set) ###\n",
    "\n",
    "### WRONG WAY TO MAKE SPLITS (train_df_shuffled has already been split) ### \n",
    "\n",
    "# # Create subsets of 10% of the training data\n",
    "# train_10_percent = train_df_shuffled[[\"text\", \"target\"]].sample(frac=0.1, random_state=42)\n",
    "# train_sentences_10_percent = train_10_percent[\"text\"].to_list()\n",
    "# train_labels_10_percent = train_10_percent[\"target\"].to_list()\n",
    "# len(train_sentences_10_percent), len(train_labels_10_percent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "id": "XHgowC3GUPJH"
   },
   "outputs": [],
   "source": [
    "# One kind of correct way (there are more) to make data subset\n",
    "# (split the already split train_sentences/train_labels)\n",
    "train_sentences_90_percent, train_sentences_10_percent, train_labels_90_percent, train_labels_10_percent = train_test_split(np.array(train_sentences),\n",
    "                                                                                                                            train_labels,\n",
    "                                                                                                                            test_size=0.1,\n",
    "                                                                                                                            random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "j8jaydmiVnJP",
    "outputId": "1e5cd05a-e8fa-4068-c67f-ea6735add71e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total training examples: 6851\n",
      "Length of 10% training examples: 686\n"
     ]
    }
   ],
   "source": [
    "# Check length of 10 percent datasets\n",
    "print(f\"Total training examples: {len(train_sentences)}\")\n",
    "print(f\"Length of 10% training examples: {len(train_sentences_10_percent)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7E2jr7rSEYT8"
   },
   "source": [
    "Because we've selected a random subset of the training samples, the classes should be roughly balanced (as they are in the full training dataset)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "V0lEpFT0k0RB",
    "outputId": "734546c8-3e01-4c0f-d712-279f855197b6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    415\n",
       "1    271\n",
       "dtype: int64"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the number of targets in our subset of data \n",
    "# (this should be close to the distribution of labels in the original train_labels)\n",
    "pd.Series(train_labels_10_percent).value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ghl1qeGOEnXG"
   },
   "source": [
    "To make sure we're making an appropriate comparison between our model's ability to learn from the full training set and 10% subset, we'll clone our USE model (`model_6`) using the [`tf.keras.models.clone_model()`](https://www.tensorflow.org/api_docs/python/tf/keras/models/clone_model) method.\n",
    "\n",
    "Doing this will create the same architecture but reset the learned weights of the clone target (pretrained weights from the USE will remain but all others will be reset)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PGmxeAOBjdg2",
    "outputId": "14f1a7f6-04e4-4cbb-cd65-0168f68312b1"
   },
   "outputs": [],
   "source": [
    "# ** KAS this Clone ran out of memory on my Linux machine **\n",
    "\n",
    "# Clone model_6 but reset weights \n",
    "# model_7 = tf.keras.models.clone_model(model_6)\n",
    "\n",
    "model_7 = tf.keras.Sequential([\n",
    "  sentence_encoder_layer, # take in sentences and then encode them into an embedding\n",
    "  layers.Dense(64, activation=\"relu\"),\n",
    "  layers.Dense(1, activation=\"sigmoid\")\n",
    "], name=\"model_7_USE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile model\n",
    "model_7.compile(loss=\"binary_crossentropy\",\n",
    "                optimizer=tf.keras.optimizers.Adam(),\n",
    "                metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_7_USE\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "USE (KerasLayer)             (None, 512)               256797824 \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 64)                32832     \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 256,834,881\n",
      "Trainable params: 37,057\n",
      "Non-trainable params: 256,797,824\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Get a summary (will be same as model_6)\n",
    "model_7.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LxFkEM_aFoLK"
   },
   "source": [
    "Notice the layout of `model_7` is the same as `model_6`. Now let's train the newly created model on our 10% training data subset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LklU2maOkgUF",
    "outputId": "d57f0e86-8315-455a-b868-ac46105a226c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving TensorBoard log files to: model_logs/10_percent_tf_hub_sentence_encoder/20220822-181042\n",
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-22 18:10:42.362142: I tensorflow/core/profiler/lib/profiler_session.cc:136] Profiler session initializing.\n",
      "2022-08-22 18:10:42.362173: I tensorflow/core/profiler/lib/profiler_session.cc:155] Profiler session started.\n",
      "2022-08-22 18:10:42.362356: I tensorflow/core/profiler/lib/profiler_session.cc:172] Profiler session tear down.\n",
      "2022-08-22 18:10:42.362426: I tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1487] CUPTI activity buffer flushed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 2/22 [=>............................] - ETA: 3s - loss: 0.6931 - accuracy: 0.4688 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-22 18:10:43.535960: I tensorflow/core/profiler/lib/profiler_session.cc:136] Profiler session initializing.\n",
      "2022-08-22 18:10:43.536048: I tensorflow/core/profiler/lib/profiler_session.cc:155] Profiler session started.\n",
      "2022-08-22 18:10:43.679346: I tensorflow/core/profiler/lib/profiler_session.cc:71] Profiler session collecting data.\n",
      "2022-08-22 18:10:43.679793: I tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1487] CUPTI activity buffer flushed\n",
      "2022-08-22 18:10:43.683951: I tensorflow/core/profiler/internal/gpu/cupti_collector.cc:228]  GpuTracer has collected 292 callback api events and 280 activity events. \n",
      "2022-08-22 18:10:43.691199: I tensorflow/core/profiler/lib/profiler_session.cc:172] Profiler session tear down.\n",
      "2022-08-22 18:10:43.697889: I tensorflow/core/profiler/rpc/client/save_profile.cc:137] Creating directory: model_logs/10_percent_tf_hub_sentence_encoder/20220822-181042/train/plugins/profile/2022_08_22_18_10_43\n",
      "2022-08-22 18:10:43.704920: I tensorflow/core/profiler/rpc/client/save_profile.cc:143] Dumped gzipped tool data for trace.json.gz to model_logs/10_percent_tf_hub_sentence_encoder/20220822-181042/train/plugins/profile/2022_08_22_18_10_43/ubuntu-All-Series.trace.json.gz\n",
      "2022-08-22 18:10:43.725811: I tensorflow/core/profiler/rpc/client/save_profile.cc:137] Creating directory: model_logs/10_percent_tf_hub_sentence_encoder/20220822-181042/train/plugins/profile/2022_08_22_18_10_43\n",
      "2022-08-22 18:10:43.729246: I tensorflow/core/profiler/rpc/client/save_profile.cc:143] Dumped gzipped tool data for memory_profile.json.gz to model_logs/10_percent_tf_hub_sentence_encoder/20220822-181042/train/plugins/profile/2022_08_22_18_10_43/ubuntu-All-Series.memory_profile.json.gz\n",
      "2022-08-22 18:10:43.730043: I tensorflow/core/profiler/rpc/client/capture_profile.cc:251] Creating directory: model_logs/10_percent_tf_hub_sentence_encoder/20220822-181042/train/plugins/profile/2022_08_22_18_10_43Dumped tool data for xplane.pb to model_logs/10_percent_tf_hub_sentence_encoder/20220822-181042/train/plugins/profile/2022_08_22_18_10_43/ubuntu-All-Series.xplane.pb\n",
      "Dumped tool data for overview_page.pb to model_logs/10_percent_tf_hub_sentence_encoder/20220822-181042/train/plugins/profile/2022_08_22_18_10_43/ubuntu-All-Series.overview_page.pb\n",
      "Dumped tool data for input_pipeline.pb to model_logs/10_percent_tf_hub_sentence_encoder/20220822-181042/train/plugins/profile/2022_08_22_18_10_43/ubuntu-All-Series.input_pipeline.pb\n",
      "Dumped tool data for tensorflow_stats.pb to model_logs/10_percent_tf_hub_sentence_encoder/20220822-181042/train/plugins/profile/2022_08_22_18_10_43/ubuntu-All-Series.tensorflow_stats.pb\n",
      "Dumped tool data for kernel_stats.pb to model_logs/10_percent_tf_hub_sentence_encoder/20220822-181042/train/plugins/profile/2022_08_22_18_10_43/ubuntu-All-Series.kernel_stats.pb\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/22 [==============================] - 2s 50ms/step - loss: 0.6825 - accuracy: 0.5924 - val_loss: 0.6582 - val_accuracy: 0.5866\n",
      "Epoch 2/5\n",
      "22/22 [==============================] - 0s 17ms/step - loss: 0.6078 - accuracy: 0.7176 - val_loss: 0.5756 - val_accuracy: 0.7415\n",
      "Epoch 3/5\n",
      "22/22 [==============================] - 0s 17ms/step - loss: 0.4731 - accuracy: 0.8361 - val_loss: 0.5102 - val_accuracy: 0.7730\n",
      "Epoch 4/5\n",
      "22/22 [==============================] - 0s 17ms/step - loss: 0.3816 - accuracy: 0.8537 - val_loss: 0.4888 - val_accuracy: 0.7861\n",
      "Epoch 5/5\n",
      "22/22 [==============================] - 0s 17ms/step - loss: 0.3009 - accuracy: 0.8935 - val_loss: 0.4989 - val_accuracy: 0.7835\n"
     ]
    }
   ],
   "source": [
    "# Fit the model to 10% of the training data\n",
    "model_7_history = model_7.fit(x=train_sentences_10_percent,\n",
    "                              y=train_labels_10_percent,\n",
    "                              epochs=5,\n",
    "                              validation_data=(val_sentences, val_labels),\n",
    "                              callbacks=[create_tensorboard_callback(SAVE_DIR, \"10_percent_tf_hub_sentence_encoder\")])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9Qpyqdh-F6Eh"
   },
   "source": [
    "Due to the smaller amount of training data, training happens even quicker than before.\n",
    "\n",
    "Let's evaluate our model's performance after learning on 10% of the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ot6MRnznlgCL",
    "outputId": "00f376b1-52f8-4fd0-f059-11d5a34b644b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.12233341],\n",
       "       [0.9055875 ],\n",
       "       [0.95165354],\n",
       "       [0.15006264],\n",
       "       [0.5762579 ],\n",
       "       [0.9048836 ],\n",
       "       [0.9247079 ],\n",
       "       [0.88882005],\n",
       "       [0.9277177 ],\n",
       "       [0.06899612]], dtype=float32)"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make predictions with the model trained on 10% of the data\n",
    "model_7_pred_probs = model_7.predict(val_sentences)\n",
    "model_7_pred_probs[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Vj_4aZellpRu",
    "outputId": "d9ee5417-1dc5-4ee4-aef7-d0212645c307"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(10,), dtype=float32, numpy=array([0., 1., 1., 0., 1., 1., 1., 1., 1., 0.], dtype=float32)>"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert prediction probabilities to labels\n",
    "model_7_preds = tf.squeeze(tf.round(model_7_pred_probs))\n",
    "model_7_preds[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "T_lTXrDblyva",
    "outputId": "41b9c96b-8857-4bdd-f68a-8f153e1824ac"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 78.34645669291339,\n",
       " 'precision': 0.7888731352165597,\n",
       " 'recall': 0.7834645669291339,\n",
       " 'f1': 0.780192419920975}"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate model results\n",
    "model_7_results = calculate_results(val_labels, model_7_preds)\n",
    "model_7_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "G84ezltll6DT",
    "outputId": "af18d38c-a535-4f7f-8cdc-577df0c9f570"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline accuracy: 79.27, New accuracy: 78.35, Difference: -0.92\n",
      "Baseline precision: 0.81, New precision: 0.79, Difference: -0.02\n",
      "Baseline recall: 0.79, New recall: 0.78, Difference: -0.01\n",
      "Baseline f1: 0.79, New f1: 0.78, Difference: -0.01\n"
     ]
    }
   ],
   "source": [
    "# Compare to baseline\n",
    "compare_baseline_to_new_results(baseline_results, model_7_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iBs9V61EGh0J"
   },
   "source": [
    "## Comparing the performance of each of our models\n",
    "\n",
    "Woah. We've come a long way! From training a baseline to several deep models.\n",
    "\n",
    "Now it's time to compare our model's results.\n",
    "\n",
    "But just before we do, it's worthwhile mentioning, this type of practice is a standard deep learning workflow. Training various different models, then comparing them to see which one performed best and continuing to train it if necessary.\n",
    "\n",
    "The important thing to note is that for all of our modelling experiments we used the same training data (except for `model_7` where we used 10% of the training data).\n",
    "\n",
    "To visualize our model's performances, let's create a pandas DataFrame we our results dictionaries and then plot it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 300
    },
    "id": "Ex0NSaz7lRf-",
    "outputId": "ee2bd44e-df7d-43b2-95e1-13b6f5885def"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0_baseline</th>\n",
       "      <td>79.265092</td>\n",
       "      <td>0.811139</td>\n",
       "      <td>0.792651</td>\n",
       "      <td>0.786219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1_simple_dense</th>\n",
       "      <td>78.740157</td>\n",
       "      <td>0.791492</td>\n",
       "      <td>0.787402</td>\n",
       "      <td>0.784697</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2_lstm</th>\n",
       "      <td>75.065617</td>\n",
       "      <td>0.751008</td>\n",
       "      <td>0.750656</td>\n",
       "      <td>0.748927</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3_gru</th>\n",
       "      <td>76.771654</td>\n",
       "      <td>0.767545</td>\n",
       "      <td>0.767717</td>\n",
       "      <td>0.766793</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4_bidirectional</th>\n",
       "      <td>76.640420</td>\n",
       "      <td>0.766590</td>\n",
       "      <td>0.766404</td>\n",
       "      <td>0.765121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5_conv1d</th>\n",
       "      <td>77.821522</td>\n",
       "      <td>0.780752</td>\n",
       "      <td>0.778215</td>\n",
       "      <td>0.775881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6_tf_hub_sentence_encoder</th>\n",
       "      <td>81.233596</td>\n",
       "      <td>0.815211</td>\n",
       "      <td>0.812336</td>\n",
       "      <td>0.810581</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7_tf_hub_10_percent_data</th>\n",
       "      <td>78.346457</td>\n",
       "      <td>0.788873</td>\n",
       "      <td>0.783465</td>\n",
       "      <td>0.780192</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            accuracy  precision    recall        f1\n",
       "0_baseline                 79.265092   0.811139  0.792651  0.786219\n",
       "1_simple_dense             78.740157   0.791492  0.787402  0.784697\n",
       "2_lstm                     75.065617   0.751008  0.750656  0.748927\n",
       "3_gru                      76.771654   0.767545  0.767717  0.766793\n",
       "4_bidirectional            76.640420   0.766590  0.766404  0.765121\n",
       "5_conv1d                   77.821522   0.780752  0.778215  0.775881\n",
       "6_tf_hub_sentence_encoder  81.233596   0.815211  0.812336  0.810581\n",
       "7_tf_hub_10_percent_data   78.346457   0.788873  0.783465  0.780192"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Combine model results into a DataFrame\n",
    "all_model_results = pd.DataFrame({\"0_baseline\": baseline_results,\n",
    "                                  \"1_simple_dense\": model_1_results,\n",
    "                                  \"2_lstm\": model_2_results,\n",
    "                                  \"3_gru\": model_3_results,\n",
    "                                  \"4_bidirectional\": model_4_results,\n",
    "                                  \"5_conv1d\": model_5_results,\n",
    "                                  \"6_tf_hub_sentence_encoder\": model_6_results,\n",
    "                                  \"7_tf_hub_10_percent_data\": model_7_results})\n",
    "all_model_results = all_model_results.transpose()\n",
    "all_model_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "id": "v-s2DSLpmM1F"
   },
   "outputs": [],
   "source": [
    "# Reduce the accuracy to same scale as other metrics\n",
    "all_model_results[\"accuracy\"] = all_model_results[\"accuracy\"]/100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 546
    },
    "id": "Wp69bR8umD5g",
    "outputId": "aa77f9c3-e70d-4b47-ab6c-ce55a93824ee"
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAqkAAAIdCAYAAAAefeSkAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAABANklEQVR4nO3deZhU5Zn+8ftuFhEF1xZRRHBhaRdkEaMx0Rg1mLjrRI1mMQvjGEXNGOMkk00nyZhEfxNjZnBDTaJxspiIkaiZRDFRo4IKAgJBJYiKoiKgqNDw/P44p7RoGrrQps7bnO/nuvqizkL103VB9V3vec/zOiIEAAAApKSh6AIAAACAlgipAAAASA4hFQAAAMkhpAIAACA5hFQAAAAkh5AKAACA5HQu6htvu+220a9fv6K+PQAAQM0mT578UkQ0Fl1HmRQWUvv166dJkyYV9e0BAABqZvsfRddQNlzuBwAAQHIIqQAAAEgOIRUAAADJKWxOKgAAQEc2efLk7Tp37nyNpD3FwN/6WiVpWnNz8+eHDx/+YmsnEFIBAADehc6dO1+z/fbbD25sbFzU0NAQRdfTkaxatcoLFy5sWrBgwTWSjm7tHFI/AADAu7NnY2PjEgLq+mtoaIjGxsbFykahWz+njvUAAABsTBoIqO9e/tqtNYsSUgEAAJAc5qQCAAC0g34X3j68PZ9v7n9+bHJ7Pt97sWLFCnXp0qWu35ORVAAAgA7s0EMP3XWPPfYYvNtuu+3xwx/+cFtJ+vWvf92zqalp8MCBA5v233//AZK0ePHihhNPPLHfgAEDmgYMGNB0/fXXbylJ3bt3H1p5ruuuu26rE044oZ8knXDCCf0+//nP99lvv/0GnHnmmX3uvvvu7kOHDh00ePDgpqFDhw6aMmXKJpLU3Nys0aNH96k873e+853tbr311h6HHXbYrpXn/e1vf9vz8MMP31XrgZFUAACADuzGG2+c26tXr5Wvvfaahw4d2nTSSSe9etZZZ/W75557Zg4aNGj5Cy+80EmSLrzwwt49e/ZcOXv27BmStHDhwk5tPfeTTz7Z7b777pvduXNnvfLKKw0PPfTQzC5duuh3v/tdjwsuuKDPnXfe+eSll17a+I9//GOT6dOnz+jSpYteeOGFTo2NjSvPPffcvs8991znHXbYoXncuHHbfOYzn3lpfX4uQioAAEAHdskll/S6/fbbt5SkBQsWdLn88ssbR44cuXTQoEHLJalXr14rJenee+/tefPNNz9V+XuNjY0r23ru448/flHnzllcfOWVVzqddNJJ/efOndvNdqxYscKS9Oc//7nnGWecsbAyHaDy/T7+8Y+/fPXVV2/9xS9+8eVHHnlk81tuueXp9fm5CKkAAAAd1O9///seEydO7DFp0qSZPXr0WDVy5MiB++yzz7LZs2d3a3luRMj2Gs9Rve+NN95Y7YTNN998VeXxV77ylR0POuigpX/84x+fnDVrVtdDDjlkYNXzrtHl4F/+5V9e/tjHPrZbt27d4qijjlq0vnNamZMKAADQQb366qudtthii5U9evRY9eijj3abMmXKZm+99VbDgw8+2GPmzJldJalyuf/ggw9ectlll21X+buVy/3bbLPNikceeaTbypUrdeutt261tu+1ZMmSTn369FkuSVdeeeW2lf2HHnrokrFjxzauWLFC1d+vX79+K3r16rXi0ksv7f2FL3xhvS71S4RUAACADuuEE05Y3Nzc7AEDBjR99atf3WHIkCGvb7fdds2XX3753OOOO263gQMHNh133HG7SNL3vve951999dVOu++++x4DBw5smjBhQg9J+va3v/3sMcccs9v+++8/sFevXivW9r2+8pWvLPjWt77VZ9iwYYNWrnxnpsB55523sE+fPssHDRq0x8CBA5uuvfbarSvHTj755Jd79+69fPjw4W+u78/miGJ60I4YMSImTZpUyPcGAABYH7YnR8SI6n1TpkyZO2TIkPUeISyTT33qU32HDh267Lzzzmv1dZoyZcq2Q4YM6dfaMeakAgCwoX1rixrOWbzh6wDqaI899hi86aabrrryyiufeTd/n5AKAMB70O/C29s8Z+4at7Csaa8b9lrn8cc//XitJQFJmD59+hPv5e9v/CGVT68AgI3AE4MGt3nO4JnvKRMASanpxinbo2zPsj3H9oWtHN/C9m22p9iebvv09i8VAAAAZdFmSLXdSdJPJB0hqUnSKbabWpz2RUkzImKIpIMlXWq7azvXCgAAgJKoZSR1pKQ5EfFURCyXdLOkY1qcE5J6OOsGu7mkVyQ1t2ulAAAAKI1aQuqOkqrvypqf76t2haTBkp6T9LikcyJilQAAANCh3Hvvvd0/85nP7LS243Pnzu0yatSoXTZ0HbXcOLXm+lnZyGm1j0h6TNIhknaV9Efbf4mIJas9kT1a0mhJ6tu373oXCwAAkKxvbTG8fZ9v8eT2eJrm5mZ17lz7vfIf/OAHl33wgx9ctrbj/fr1W3HHHXc81R61rUstI6nzJVWn6T7KRkyrnS7plsjMkfS0pEEtnygiroqIERExorGx8d3WDAAAAEmzZs3q2r9//z2OP/74fgMGDGgaNWrULkuXLm3Ycccd9zr//PN7Dx8+fOC4ceO2uuWWW3rus88+g5qamgYfccQRuyxevLhBkiZOnNh96NChgwYOHNi01157DV60aFHD73//+x4f+tCHdpOk22+/ffNBgwY1DRo0qGnw4MFNixYtapg1a1bX3XfffQ9JWrZsmU888cR+AwYMaBo8eHDTbbfd1kOSLr/88m0OP/zwXT/wgQ/svvPOO+95xhln9Fnfn62WkPqwpN1t989vhjpZ0vgW58yT9GFJst1L0kBJGzxhAwAAlN3cuXO7nXHGGQtnz549o0ePHqt+8IMfNEpSt27dVk2ePHnWUUcdtfS73/1u73vvvXf2jBkznhg2bNiyiy++uNebb77pU089ddf/+q//mjdr1qwZEydOnLX55puvNl3z0ksv3f7yyy//x8yZM2f87W9/m9ny+CWXXLKdJM2ePXvGTTfd9NTo0aP7LVu2zJI0Y8aM7r/73e+eeuKJJ6aPHz9+qzlz5nRZn5+rzbHfiGi2fZakOyV1kjQuIqbbPiM/PlbSxZKut/24sukBX4mIuiwT1lYT5fZooCzRRBkAAKRp++23X3744Ye/Lkmf/OQnX7788su3k6RPfepTiyTpnnvu2ezJJ5/sNnLkyEGStGLFCg8fPvy1qVOndttuu+1WHHTQQcskaeutt17jfqL3ve99r51//vk7ffzjH3/llFNOWbTrrruuds7999+/+dlnn/2iJA0dOvTNHXbYYfnjjz/eTZIOPPDAJdtss81KSdptt93efPLJJzfZbbfdVtT6c9U0QSEiJkia0GLf2KrHz0k6vNZvigSwyAEAABuFrLnSmts9evRYJUkRoQMPPHDJbbfd9nT1eQ8++OCmtlveZ7Sa7373uwuOPfbYxbfeeusWBxxwwOA77rhjdvfu3d8OqhFr/+tdu3Z9+2CnTp1ixYoVrd3ntFY1NfMHAABod9/aYt1fqMnzzz/f9f/+7/82k6Sbbrpp6wMOOOC16uMHH3zw65MmTdp82rRpm0jS0qVLG6ZOnbrJkCFD3nzhhRe6Tpw4sbskLVq0qGHFitUHOqdPn77JyJEj3/jOd76zYK+99np92rRpq12jPvDAA1/7+c9/vrUkTZ06dZPnn3++69577/1me/xcG/+yqO2kIy1HV691pCWmQQAAULRddtnlzXHjxm1z5pln7ty/f/+3zj///IXXXHPNdpXjO+ywQ/OVV1459+STT95l+fLllqRvfvObz+69995v3XjjjU+OGTOm75tvvtnQrVu3Vffee+/s6uf+/ve/v93999/fs6GhIQYMGPDGiSeeuHjevHlvzy294IILXvzkJz+584ABA5o6deqkK6+8cu6mm266ztHZWhFS8Z60Fd5TCe4AgPpqjwGTDjdY0k4to9ZXQ0ODbrrppnnV+5599tnVXpijjz566dFHH73GL+WDDjpo2ZQpU2ZW7zvyyCOXHnnkkUsl6YYbbnim5d8ZOHDg8r///e/TJal79+7xm9/8Zm7Lc8aMGfOypJcr23ffffec9fupCKkAgPbEfHcA7YSQCgAAOqyONB1vQ6ge1dzYEFIBADVhvjuAeiKkAgCSw3x3ALSgAgAAQHIIqQAAAEgOl/uBkqttnuEn1nl8r/5923wO5hgCQMdw+eWXbzNp0qTNfvrTn8770pe+tMPmm2++8qKLLnqh3nUQUgHURUe7A7et8N5WcJdqC++//F5zm+ek9LoAWLu9bthreHs+3+Offny9+q6uWrVKEaFOnTq1ZxmF4XI/AABABzVr1qyuu+yyyx6nnXZa3z322KPpggsu6L3nnnsOHjBgQNN55523Q+W8K664YpsBAwY0DRw4sOnYY4/tL0k33XTTFnvvvfegwYMHNx1wwAEDnnnmmaQGL5MqBgAAAOtn7ty53a6++uq5xx9//Ku/+tWvtpo6deoTEaFDDz10tz/84Q+bNzY2Nv/whz/s/cADD8zs3bt38wsvvNBJkg477LDXTj755JkNDQ267LLLtr3ooou2v/rqq+cX/fNUEFIBAAA6sN69ey//8Ic//Pro0aP73HvvvT2bmpqaJGnZsmUNM2fO7PbII480HHXUUYt69+7dLEm9evVaKUlPP/1012OPPbbPwoULuyxfvrxhp512eqvIn6MlLvcDAAB0YN27d18lSRGhc8899/mZM2fOmDlz5ox58+ZNO++8816KCNmOln/vrLPO6nvmmWe+OHv27BlXXHHFP956662kcmFSxQAAAODdOeKII5b87Gc/23bx4sUNkvT00093efbZZzuPGjVqyfjx47desGBBJ0mqXO5funRpp759+66QpOuvv36b4ipvHZf7AQAANgLHH3/8kunTp3fbd999B0nZCOuNN9749IgRI97813/91+c/8IEPDGpoaIg999xz2W9+85u5X/va15475ZRTdu3Vq9fyESNGvD5v3rxNiv4ZqhFSURrt0Q9UarutEP1AAaCc1rdlVHsYOHDg8r///e/TK9tf//rXX/z617/+Ysvzzj777JfPPvvsl6v3nXbaaa+edtppr7Y8d8yYMS9LelmSLrvssufav+raEFKBdtbR+oECAJAi5qQCAAAgOYRUAAAAJIeQCgAAgOQQUgEAAJAcQioAAACSQ0gFAADooP7jP/5ju1122WWPj3zkI7vus88+g7p27TrsG9/4Rq+i62oPtKACAABoB08MGjy8PZ9v8Mwn2uy7eu211zb+4Q9/+HuPHj1WzZkzp+uvf/3rrdqzhiIxkgoAANABfeITn+g7f/78TY4++ujdrrnmmq0POuigZV26dImi62ovjKQCAAB0QDfddNO8iRMnbjFx4sTZvXv3bi66nvbGSCoAAACSQ0gFAABAcgipAAAASA5zUgEAADq4efPmdd53332bXn/99U6248orr+z1xBNPTNt6661XFV3bu0VIBQAAaAe1tIxqb88+++zjlccvvPDC1Hp//w2Jy/0AAABIDiEVAAAAySGkAgAAIDk1hVTbo2zPsj3H9oWtHP+y7cfyr2m2V9reuv3LBQAASMaqVatWuegiOqr8tVvrjV1thlTbnST9RNIRkpoknWK7qfqciPhBROwTEftI+jdJEyPilfdSOAAAQOKmLVy4cAuC6vpbtWqVFy5cuIWkaWs7p5a7+0dKmhMRT0mS7ZslHSNpxlrOP0XSL9azVgAAgA6lubn58wsWLLhmwYIFe4oplOtrlaRpzc3Nn1/bCbWE1B0lPVO1PV/Sfq2daLu7pFGSzlrL8dGSRktS3759a/jWAAAAaRo+fPiLko4uuo6NVS2pv7Uh7FjLuUdJum9tl/oj4qqIGBERIxobG2utEQAAACVTS0idL2mnqu0+kp5by7kni0v9AAAAeI9qCakPS9rddn/bXZUF0fEtT7K9haSDJN3aviUCAACgbNqckxoRzbbPknSnpE6SxkXEdNtn5MfH5qceJ+muiHh9g1ULAACAUqjlxilFxARJE1rsG9ti+3pJ17dXYQAAACgv2iUAAAAgOYRUAAAAJIeQCgAAgOQQUgEAAJAcQioAAACSQ0gFAABAcgipAAAASA4hFQAAAMkhpAIAACA5hFQAAAAkh5AKAACA5BBSAQAAkBxCKgAAAJJDSAUAAEByCKkAAABIDiEVAAAAySGkAgAAIDmEVAAAACSHkAoAAIDkEFIBAACQHEIqAAAAkkNIBQAAQHIIqQAAAEgOIRUAAADJIaQCAAAgOYRUAAAAJIeQCgAAgOQQUgEAAJAcQioAAACSQ0gFAABAcgipAAAASA4hFQAAAMkhpAIAACA5hFQAAAAkh5AKAACA5NQUUm2Psj3L9hzbF67lnINtP2Z7uu2J7VsmAAAAyqRzWyfY7iTpJ5IOkzRf0sO2x0fEjKpztpT035JGRcQ829ttoHoBAABQArWMpI6UNCcinoqI5ZJulnRMi3M+IemWiJgnSRHxYvuWCQAAgDKpJaTuKOmZqu35+b5qAyRtZfse25Ntf6q9CgQAAED5tHm5X5Jb2RetPM9wSR+WtKmkB2z/LSJmr/ZE9mhJoyWpb9++618tAAAASqGWkdT5knaq2u4j6blWzrkjIl6PiJck3StpSMsnioirImJERIxobGx8tzUDAABgI1dLSH1Y0u62+9vuKulkSeNbnHOrpA/Y7my7u6T9JD3RvqUCAACgLNq83B8RzbbPknSnpE6SxkXEdNtn5MfHRsQTtu+QNFXSKknXRMS0DVk4AAAANl61zElVREyQNKHFvrEttn8g6QftVxoAAADKihWnAAAAkBxCKgAAAJJDSAUAAEByCKkAAABIDiEVAAAAySGkAgAAIDmEVAAAACSHkAoAAIDkEFIBAACQHEIqAAAAkkNIBQAAQHIIqQAAAEgOIRUAAADJIaQCAAAgOYRUAAAAJIeQCgAAgOQQUgEAAJAcQioAAACSQ0gFAABAcgipAAAASA4hFQAAAMkhpAIAACA5hFQAAAAkh5AKAACA5BBSAQAAkBxCKgAAAJJDSAUAAEByCKkAAABIDiEVAAAAySGkAgAAIDmEVAAAACSHkAoAAIDkEFIBAACQHEIqAAAAkkNIBQAAQHJqCqm2R9meZXuO7QtbOX6w7cW2H8u/vtH+pQIAAKAsOrd1gu1Okn4i6TBJ8yU9bHt8RMxocepfIuLIDVAjAAAASqaWkdSRkuZExFMRsVzSzZKO2bBlAQAAoMxqCak7Snqmant+vq+l/W1Psf0H23u0S3UAAAAopTYv90tyK/uixfYjknaOiNdsf1TS7yTtvsYT2aMljZakvn37rl+lAAAAKI1aRlLnS9qparuPpOeqT4iIJRHxWv54gqQutrdt+UQRcVVEjIiIEY2Nje+hbAAAAGzMagmpD0va3XZ/210lnSxpfPUJtre37fzxyPx5X27vYgEAAFAObV7uj4hm22dJulNSJ0njImK67TPy42MlnSjpX2w3S3pD0skR0XJKAAAAAFCTWuakVi7hT2ixb2zV4yskXdG+pQEAAKCsWHEKAAAAySGkAgAAIDmEVAAAACSHkAoAAIDkEFIBAACQHEIqAAAAkkNIBQAAQHIIqQAAAEgOIRUAAADJIaQCAAAgOYRUAAAAJIeQCgAAgOQQUgEAAJAcQioAAACSQ0gFAABAcgipAAAASA4hFQAAAMkhpAIAACA5hFQAAAAkh5AKAACA5BBSAQAAkBxCKgAAAJJDSAUAAEByCKkAAABIDiEVAAAAySGkAgAAIDmEVAAAACSHkAoAAIDkEFIBAACQHEIqAAAAkkNIBQAAQHIIqQAAAEgOIRUAAADJIaQCAAAgOYRUAAAAJIeQCgAAgOTUFFJtj7I9y/Yc2xeu47x9ba+0fWL7lQgAAICyaTOk2u4k6SeSjpDUJOkU201rOe8SSXe2d5EAAAAol1pGUkdKmhMRT0XEckk3SzqmlfPOlvQbSS+2Y30AAAAooVpC6o6Snqnanp/ve5vtHSUdJ2ls+5UGAACAsqolpLqVfdFi+78kfSUiVq7ziezRtifZnrRw4cIaSwQAAEDZdK7hnPmSdqra7iPpuRbnjJB0s21J2lbSR203R8Tvqk+KiKskXSVJI0aMaBl0AQAAAEm1hdSHJe1uu7+kZyWdLOkT1SdERP/KY9vXS/p9y4AKAAAA1KrNkBoRzbbPUnbXfidJ4yJiuu0z8uPMQwUAAEC7qmUkVRExQdKEFvtaDacR8Zn3XhYAAADKjBWnAAAAkBxCKgAAAJJDSAUAAEByCKkAAABIDiEVAAAAySGkAgAAIDmEVAAAACSHkAoAAIDkEFIBAACQHEIqAAAAkkNIBQAAQHIIqQAAAEgOIRUAAADJIaQCAAAgOYRUAAAAJIeQCgAAgOQQUgEAAJAcQioAAACSQ0gFAABAcgipAAAASA4hFQAAAMkhpAIAACA5hFQAAAAkh5AKAACA5BBSAQAAkBxCKgAAAJJDSAUAAEByCKkAAABIDiEVAAAAySGkAgAAIDmEVAAAACSHkAoAAIDkEFIBAACQHEIqAAAAkkNIBQAAQHJqCqm2R9meZXuO7QtbOX6M7am2H7M9yfaB7V8qAAAAyqJzWyfY7iTpJ5IOkzRf0sO2x0fEjKrT/iRpfESE7b0l/VLSoA1RMAAAADZ+tYykjpQ0JyKeiojlkm6WdEz1CRHxWkREvrmZpBAAAADwLtUSUneU9EzV9vx832psH2d7pqTbJX22fcoDAABAGdUSUt3KvjVGSiPitxExSNKxki5u9Yns0fmc1UkLFy5cr0IBAABQHrWE1PmSdqra7iPpubWdHBH3StrV9ratHLsqIkZExIjGxsb1LhYAAADlUEtIfVjS7rb72+4q6WRJ46tPsL2bbeePh0nqKunl9i4WAAAA5dDm3f0R0Wz7LEl3SuokaVxETLd9Rn58rKQTJH3K9gpJb0g6qepGKgAAAGC9tBlSJSkiJkia0GLf2KrHl0i6pH1LAwAAQFmx4hQAAACSQ0gFAABAcgipAAAASA4hFQAAAMkhpAIAACA5hFQAAAAkh5AKAACA5BBSAQAAkBxCKgAAAJJDSAUAAEByCKkAAABIDiEVAAAAySGkAgAAIDmEVAAAACSHkAoAAIDkEFIBAACQHEIqAAAAkkNIBQAAQHIIqQAAAEgOIRUAAADJIaQCAAAgOYRUAAAAJIeQCgAAgOQQUgEAAJAcQioAAACSQ0gFAABAcgipAAAASA4hFQAAAMkhpAIAACA5hFQAAAAkh5AKAACA5BBSAQAAkBxCKgAAAJJDSAUAAEByCKkAAABIDiEVAAAAyakppNoeZXuW7Tm2L2zl+Km2p+Zf99se0v6lAgAAoCzaDKm2O0n6iaQjJDVJOsV2U4vTnpZ0UETsLeliSVe1d6EAAAAoj1pGUkdKmhMRT0XEckk3Szqm+oSIuD8iFuWbf5PUp33LBAAAQJnUElJ3lPRM1fb8fN/afE7SH95LUQAAACi3zjWc41b2Rasn2h9SFlIPXMvx0ZJGS1Lfvn1rLBEAAABlU8tI6nxJO1Vt95H0XMuTbO8t6RpJx0TEy609UURcFREjImJEY2Pju6kXAAAAJVBLSH1Y0u62+9vuKulkSeOrT7DdV9Itkj4ZEbPbv0wAAACUSZuX+yOi2fZZku6U1EnSuIiYbvuM/PhYSd+QtI2k/7YtSc0RMWLDlQ0AAICNWS1zUhUREyRNaLFvbNXjz0v6fPuWBgAAgLJixSkAAAAkh5AKAACA5BBSAQAAkBxCKgAAAJJDSAUAAEByCKkAAABIDiEVAAAAySGkAgAAIDmEVAAAACSHkAoAAIDkEFIBAACQHEIqAAAAkkNIBQAAQHIIqQAAAEgOIRUAAADJIaQCAAAgOYRUAAAAJIeQCgAAgOQQUgEAAJAcQioAAACSQ0gFAABAcgipAAAASA4hFQAAAMkhpAIAACA5hFQAAAAkh5AKAACA5BBSAQAAkBxCKgAAAJJDSAUAAEByCKkAAABIDiEVAAAAySGkAgAAIDmEVAAAACSHkAoAAIDkEFIBAACQnJpCqu1RtmfZnmP7wlaOD7L9gO23bJ/f/mUCAACgTDq3dYLtTpJ+IukwSfMlPWx7fETMqDrtFUljJB27IYoEAABAudQykjpS0pyIeCoilku6WdIx1SdExIsR8bCkFRugRgAAAJRMLSF1R0nPVG3Pz/cBAAAAG0QtIdWt7It3881sj7Y9yfakhQsXvpunAAAAQAnUElLnS9qparuPpOfezTeLiKsiYkREjGhsbHw3TwEAAIASqCWkPixpd9v9bXeVdLKk8Ru2LAAAAJRZm3f3R0Sz7bMk3Smpk6RxETHd9hn58bG2t5c0SVJPSatsnyupKSKWbLjSAQAAsLFqM6RKUkRMkDShxb6xVY8XKJsGAAAAALxnrDgFAACA5BBSAQAAkBxCKgAAAJJDSAUAAEByCKkAAABIDiEVAAAAySGkAgAAIDmEVAAAACSHkAoAAIDkEFIBAACQHEIqAAAAkkNIBQAAQHIIqQAAAEgOIRUAAADJIaQCAAAgOYRUAAAAJIeQCgAAgOQQUgEAAJAcQioAAACSQ0gFAABAcgipAAAASA4hFQAAAMkhpAIAACA5hFQAAAAkh5AKAACA5BBSAQAAkBxCKgAAAJJDSAUAAEByCKkAAABIDiEVAAAAySGkAgAAIDmEVAAAACSHkAoAAIDkEFIBAACQHEIqAAAAkkNIBQAAQHJqCqm2R9meZXuO7QtbOW7bl+fHp9oe1v6lAgAAoCzaDKm2O0n6iaQjJDVJOsV2U4vTjpC0e/41WtL/tHOdAAAAKJFaRlJHSpoTEU9FxHJJN0s6psU5x0j6aWT+JmlL273buVYAAACURC0hdUdJz1Rtz8/3re85AAAAQE0613COW9kX7+Ic2R6tbDqAJL1me1YN3/89aa2wNU3bVtJL6zqj5fyG1r9Zbd8tBXV7XTrQayK1z+uysf1bkWp5Xfg/1Dpel9bx3tI63lvWlNT/oZ3b40lQu1pC6nxJO1Vt95H03Ls4RxFxlaSr1rPGDc72pIgYUXQdqeF1aR2vy5p4TVrH69I6XpfW8bqsidek3Gq53P+wpN1t97fdVdLJksa3OGe8pE/ld/m/T9LiiHi+nWsFAABASbQ5khoRzbbPknSnpE6SxkXEdNtn5MfHSpog6aOS5khaJun0DVcyAAAANna1XO5XRExQFkSr942tehySvti+pdVVclMQEsHr0jpelzXxmrSO16V1vC6t43VZE69JiTnLlwAAAEA6WBYVAAAAySGkAgAAIDmEVAAACmC7wfYBRdcBpKq0c1Jt95L0XUk7RMQRtpsk7R8R1xZcWhJsbxYRrxddR0psb6WsH/DbNxxGxCPFVVQ8259qbX9E/LTetRTN9pfWdTwiLqtXLeg4bD8QEfsXXUeKbO+prFd/t8q+Mr63lFlNd/dvpK6XdJ2kr+XbsyX9r6RSh9T8U/01kjaX1Nf2EEn/HBFnFltZsWxfLOkzkp7UO6uphaRDiqopEftWPe4m6cOSHpFUxl8kPYouIEW2l6qVFQgrIqJnHctJ0V22T5B0S5R11KgVtr8p6WBlIXWCpCMk/VXlfG8prTKPpD4cEfvafjQihub7HouIfQourVC2H5R0oqTxVa/LtIjYs9jKipUv4btXRCwvupaU2d5C0s8i4uiia0FabF8kaYGknylbDfNUST0i4vuFFlawPMRvJmmlpDeUvTZR9vBu+3FJQyQ9GhFD8quf10TEUQWXhjoq80jq67a3Uf4Jv7JSVrElpSEinvHq6xyvLKqWhEyTtKWkFwuuI3XLJO1edBFFst1N0uck7aHVL1N+trCi0vCRiNivavt/8g/FpQ6pEcEIfOveiIhVtptt91T23rtL0UWhvsocUr+kbDnXXW3fJ6lR2Qhi2T2TX/KPfBncMZKeKLimFHxP0qO2p0l6q7Kz7COGtm/TO5dyG5RdmvtlcRUl4WeSZkr6iKSLlI0Y8n9IWmn7VEk3K/s3c4r4ACxnIwKnSuofERfb3klS74h4qODSijbJ9paSrpY0WdJrksr+mpROaS/3S5LtzpIGKru8MisiVhRcUuFsbyvpR5IOVfa63CXpnIh4udDCCmZ7uqQrJT0uaVVlf0RMLKyoBNg+qGqzWdI/ImJ+UfWkoDKFyPbUiNjbdhdJd0ZEqecv2+6n7L3l/cpC6n2Szo2IuQWWVTjb/6PsPeWQiBic36B5V0Ts28ZfLY38307PiJhadC2orzKPpErSSEn9lL0Ow2yX/s7BiHhJ2ad6rO6liLi86CJSYruTpK9HxKFF15KYyofdV/O7kxcoe58ptTyMHlN0HQnaLyKG2X5UkiJiUX4Vq9Rs/ykiPiy9/W9ntX0oh9L2SbX9M0k/lHSgsjuU95U0otCiEmD7+7Z72u5i+0+2X7J9WtF1JWCy7e/Z3t/2sMpX0UUVKSJWSlqW3yyFd1yVj4Z9XdmUohkq+bzLtbH9jaJrSMCK/ANf5f6IRlVdrSkb291sby1pW9tb2d46/+onaYeCy0OdlfZyv+0nJDXR8mN1lQ4Hto+TdKyk8yTdHRFDiq2sWLbvbmV3cAnXv5T0Pkl/lPR2X92IGFNYUegwbM+LiL5F11GkfJ7uSZKGSbpB2b0R/x4Rvyq0sILYPkfSucoC6bPKpp1J0hJJV0fEFQWVhgKU+XL/NEnbS3q+6EIS0yX/86OSfhERr7S407+sPhcRT1XvsM2dptLt+RdytjeRdILemUokSYqIi4qqqUi2l6ztkKRN61lLiiLiRtuTlfUYtqRjI6K0N9pFxI8k/cj22RHx46LrQbHKPJJ6t6R9lN0tyN3aOdv/qWwE9Q1lc3a3lPT7Fq1jSsf2IxExrMW+yRExvKiakCbbdyhrZzdZVXevR8SlhRVVINvzJO0bES+0cuyZiNipgLIKl1/SXquIeKVetaSKFadQ5pHUbxVdQIoi4kLbl0haEhErbb+uEt/sYHuQsn6XW9g+vupQT1W9cZZV3nC75SfdxZImSfqPknaF6BMRo4ouIiE/lbSzpDVCqqSb6lxLSiYr+79jSX0lLcofbylpnqT+hVWWAFacglTikFr21kFtGCypX96iq6KsbwwDJR2p7BdH9UonSyV9oYiCEvMHZaOFlbBxsrJftIuVLT1cxtVh7re9V0Q8XnQhKYiIf1/Hsa/Us5aURER/SbI9VtkKfxPy7SOUtQAsuxP1zopTp1dWnCq4JtRZ6S732/5rRBzYynrSLEWnt7se7CrpMb1zqTLKfiOM7f0j4oGi60iN7fsi4v2t7bP9eETsVVRtRbE9Q9Jukp5WNpWo8t6yd6GFFcz2eGWN/G+NiNfbOr8sWps2ZHtSRJS624zthyJiZD5f90PKBgamRcQeBZeGOirdSGpEHJj/yVJ0rRshuh605ri8of8bku5Q9gn/3Ij4ebFlFW5z2/tFxIOSZHukpM3zY83FlVWoI4ouIFGXKruL/Xu2H5L0v8rmu79ZbFmFe8n2v0v6ubKBk9MklXGaTEusOIVSjqQyWX0dbP9K0piIoOtBFVpztc72vpLG6Z1gulTZuvUzJH0sIkq5RKrtIZI+kG/+JSKmFFlPSvKeoIcomy4ziqtX3lrSNyV9MN91r6Rvl/13UTVWnCqv0o2kavXJ6i2FpLK3FdpW0ox8pIOuB++gNVcrIuJhSXvlDf0dEa9WHf6l7U9HxA3FVFeMvM/jFyTdku/6ue2raKcj2d5U2Tzl6r6gpZaH0XNs95S0KiJeK7qmIq1rkRTbwyLikXrWg2KVbiQV69ZiLfa3lf1GM1pzvTutte7a2NmeKmn/yrxL25tJeoA5qf5fSfspmy7zS0n3RERpV1aqsL2XshtTK1f5XpL06YiYVlxVxalaOKWbsulnU5QNKu0t6cHKlD2UQxlHUiVJzobBTpXUPyIutt1X0vYRUeo5LxEx0fbOknaPiP+z3V1Sp6LrKhqtud61Mg43W1X9UfPHZXwdWrpO0ify5XTxjislfSki7pYk2wdLukrSAQXWVJiI+JAk2b5Z0uhKl4y8Z+r5RdaG+ittSJX038rWRz5E0sXK5tL9RtK+RRZVNNtfkDRa2af6XSXtKGmsstVQSqdFb9TKvurNW1oex2rKeKnmOkkP2v5tvn2spGuLKycNEXGH7QPy+YXVK3GVtb1dxWaVgCpJEXFPPvpedoOq27hFxDTb+xRYDwpQ5pC6X0QMs/2oJEXEIttdiy4qAV9Udjn7QUmKiL/b3q7Ykgq1rj6fIUJqW0o3ghgRl9m+R9KByn7+0yPi0WKrKt7a2tupvD2YK56y/XVJP8u3T1PWvqzsnrB9jVbvelDa5WLLqswhdUV+l2lIku1GZSOrZfdWRCyvjBbmDf3LOBomSYqI02s5r6Q3CI1U1v/zYdtNkkZJmllpSp67r5jq6s92z4hYkt+tPTf/qhzbmru1aW+3Fp+V9G2984H3Xkk1ve9s5E6X9C+Szsm375X0P8WVgyKU9sYp26dq9TtMT5T07xHxq0ILK5jt70t6VdKnJJ0t6UxJMyLia0XWlbqy3SCUL1l4hLIPun9UdkPMPcpWyrkzIr5TXHXFsP37iDjS9tNqfaGQUncOob0d2pPt30TECUXXgQ2rtCFVentd9g8r+yXyp4go/aUE2w3K+lwerux1uVPSNYx+rJvtRyNiaNF11IvtxyXtI2kTSQuUrVe/JG8x9GDZ72THmvK7tvdR1pCd9nY523+U9E+V9m22t5J0c0R8pNDCEle299yyKu3lftu7Sno6In6S3015mO3nW/R5LJ28JczV+RdqV7YQ35zfpb3M9pMRsUSSIuIN26WeNmP7TxHx4bb2ldC3ii4gUdtW/97J748o830AtSrbe24plTakKruTf4Tt3SRdI+k2STcpa9ZeOvnI2Fr/0zMy1qay3SC03Hb3iFgm6e11x/Om/qUMqba7Seouadt8NKzyb6KnpB0KKywReXu7Xnqng8pDEfFikTUlYpXtvhExT5LyFoAEMEDlDqmrIqI5bzH0o4j4ceVO/5I6Mv/zi/mflTtNT5W0rP7lpCOfFrKjssvYr1XtHxURd+SbpblBKPfBiHhLenv0vaKLpE8XU1Lh/lnSucoC6WS9E1KXSPpJQTUlw/bHJf1A2dxlS/qx7S9HxK8LLax4X5P0V9uVBVM+qKwNINatbAMDpVTaOam2H5T0X8reII6KiKdtT4uIPYutrFi274uI97e1ryxsj1EW3J9QNp/unIi4NT9WqpulUBvbZ7ME6ppsT5F0WGX0NO+o8n8RMaTYyopne1tJ71MWvB6IiJcKLqlwts+JiB+tbZ/twyPirmKqQ700FF1AgU6XtL+k7+QBtb+yfmxlt5ntt5eds32ApDI3lv6CpOERcaykgyV9PV+bXeKTPFq3yvaWlQ3bW9k+s8B6UtHQ4vL+yyr376Bqm0h6RdJiSU22P1hwPSlo7YrMZyoPCKjlUNqRVLTO9nBJ4yRtoWxe1GJJn42IRwotrCC2Z0REU9X25pJ+LWmGpEMiYp+iakOabD/W8t8FdyJLtn+gbP31X+S7TpL0eERcUFxVxcuXWz5J0nS9M587ytr1wPYpkj6hbDGMv1Qd6iFpZUQcWkhhKERp56Ta3l3S9yQ1SepW2V/2XoYRMVnSENs9lX2IWVx9vIRN6xfY3iciHpOkiHjN9pHKgvxehVaGVDXYdqVtW75oSOlXs4uIL+f3AFRW4roqIn7bxl8rg2MlDazM8Ybul/S8pG0lXVq1f6mkqYVUhMKUdiTV9l8lfVPS/1O29OXpyl6PbxZaWOLKNg/Tdh9l7ZYWtHLs/RFRthum0IZ8xLCfpLHKrkacIemZiPjXIusqWj6l6vmIeDPf3lRSr4iYW2hhBbP9B2V9Ul9r82SgZMocUidHxHDbj0fEXvm+v0TEB4quLWVctgTWLV8Q45/1zkIhdylbEGPlOv/iRs72JEkHRMTyfLurpPsiYt91/82Nm+3fSBoi6U9afZGDMYUVlYB81P0SSdsp+39UWbmtZ6GFoa5Ke7lf0pv5L5O/2z5L0rPK/jNg3cr5qQaoUUSssn29pD9HxKyi60lI50pAlaSIWJ4H1bIbn39hdd9X1nmn9CtBllmZQ+q5yhpvj5F0saRDVN7+juuDO9qBdbB9tLJ+oF0l9be9j6SLynojTJWFto+OiPGSZPsYSaVvtRQRN+RTH/ryoWY1LxBQUdrL/RX5DUIREUuLrqUjsH1FRJxVdB1AqmxPVvah957K1BjbU8u+alu+FPWNemf1rfmSPhkRTxZXVfFsHyXph5K6RgQfanK2fyRpe0m/0+rTIG4pqibUX2lHUm2PkHSdsrYWsl1ptTS50MIKli9b+F1JO0TEEbabJO0fEddKEgEVaFNzRCy2uehQLQ+j78vbuLnlwEAJO4dUfEvSSGUrcSkiHstvMiu7nspWOzy8al9IIqSWSGlDqrIWQmdGxF8kKW9gf52yPn5ldr2y1+Fr+fZsSf8r6dqiCgI6mGm2PyGpU97qboyytjpQ1sZtLYfOkVTGkNrah5pyX+KUFBGnF10Dilfm1T6WVgKqJEXEX5X1YSu7bSPil8qbSkdEs6RS35UMrKezJe2h7BLlTcoWxDi3yII6iLIOPa/2ocb2j8WHGtkeYPtPtqfl23vb/vei60J9lS6k2h5me5ikh2xfaftg2wfZ/m/ll1tK7nXb2yj/JG/7fcp+yQJoQ964f3xEfC0i9s2//r3SGxTrVNbRQz7UtO5qSf8maYUkRcRUSScXWhHqroyX+y9tsV3dvL+sb5LVvqSsHcqutu+T1CjpxGJLAjqGiFhpe5ntLVqu1oY2lXIkNSKWKZte9bXWjtv+cUScXd+qktA9Ih5qMQ2iuahiUIzShdSI+FAt55V1En9EPGL7IEkDlf3SmBURKwouC+hI3pT0uO0/Snq9srPszdkr8vn/IyVNi4i7qg6xelvr3l90AQV5Ke8IUbmqd6Ky5VJRIqVvQbU2JVz+8/h1HaftB1Ab2632Wy7jh15Jsv1QRIzMH39B0hcl/VbZXdu3RcR/Fllf6sr2u6jC9i6SrpJ0gKRFkp6WdFrZl9EtG0LqWpRt+U/b163jcETEZ+tWDICNRvV7qe2HJX00Ihba3kzS3yrLUqN1ZQ2pFfm/kwZ6mZdT6S73r4dSpXfafQDvje1fRsTHbT+uVt4/StzMv8H2Vspu1HVELJSkiHjdNnMM21bKubq2vyvp+xHxar69laR/jQju8C8RQuralfWNYRtlN5MdqOwX7V+VrX7ycqGFAek7J//zyEKrSM8WkiYre08N29tHxIJKU/9iS0uH7c0i4vVWDv2o7sWk4YiI+GplIyIW2f6oJEJqiZQypNoeJOkYSTsqC2LPKWsbU71OcFkn8d8s6V5JJ+Tbpypr5n9oYRUBHUBEPJ//+Y+ia0lJRPRby6FVko6rYylJsn2ApGskbS6pr+0hkv45Is6UpIi4vsDyitTJ9iYR8ZYk2d5U0iYF14Q6K92cVNtfkXSKsjA2P9/dR1n/tZvLPonf9uSIGN5i36SIGFFUTUBHYHup1jFNKCJ61rEcdBC2H1TW5m981dzdaRGxZ7GVFcv2BZKOVrYCYkj6rLLX6PuFFoa6KuNI6uck7dGyrZLtyyRNl1TqkCrpbtsnS/plvn2ipNsLrAfoECKihyTZvkjSAkk/U3Y5+1RJPQosDYmLiGda9AMt9Sp/zl6MX0iaquwqniVdHBF3FloY6q6MI6kzJX2k5SU52ztLuisiBhZTWRry0aDNlC+Lquxmh8o8qWA0CFg32w9GxH5t7QMkyfavJV0m6QpJ75M0RtKIiCj16kqtXdVD+ZRxJPVcSX+y/XdJz+T7+kraTdJZRRWVispoEIB3baXtU5VNKQpl04tKPTKGdTpD2c1ROyqbgnaXsl6yZfc32/tGxMNFF4LilG4kVZJsNyhb8WRHZZcR5kt6OCL4RSLJ9t6S+qnqQwzN/IHa2O6nLHS8X1lIvU/SuTQhB2pne4aylQ/nKruaZ2VX88rayq2UShlSsXa2x0naW9n83Molf5r5A8AGYPsGSee06Ad6adnfc/MpeGuge0a5lPFyP9btfRHRVHQRQEdj+4KI+L7tH6v1Zv5jCigL6du7ElClt/uBlma1w7WJiH/YPlDS7hFxne1GZW26UCKEVLT0gO2miJhRdCFAB1Ppszyp0CrQ0TTY3ioiFkmS7a3F72bZ/qakEcou+V8nqYuknyubRoOSKP1/BKzhBmVBdYGkt8Q8IKAmEXFb/ucNkmS7Z7bJmuNYp0sl3Z/f5S9J/yTpOwXWk4rjJA2V9IgkRcRztrmxt2QIqWhpnKRPSnpc78xJBVAj2yOUjfz0yDb9qqTPRsTkQgtDkiLip7YnS/qQskGB47mSJUlaHhFhO6Rs2diiC0L9EVLR0ryIGF90EUAHNk7SmRHxF0nK59Vdp+yGRKA1MyUtUv472XbfiJhXbEmF+6XtKyVtafsLylacurrgmlBn3N2P1dj+b0lbSrpN2eV+SbSgAmpl+76IeH9b+wBJsn22pG9KekFZP12mWOVsHybp8Hzzroj4Y5H1oP4YSUVLmyoLp4dX7QtJhFRgHWwPyx8+lI8A/ULZ/52TJN1TVF1I3jmSBkbEy0UXkqDHlf1OivwxSoaRVABoB7bvXsfhiIhD6lYMOoz8381hEdFcdC0psf15Sd+Q9Gdlo8sHSbooIsYVWhjqipAKSfR4BOrF9qcrHQAA29cqa7N0u1afYnVZYUUlwPYsSQdURphtbyPp/ogYWGxlqCcu96OCHo9AfZyjrNUbIEnz8q+u+Rcy8yVVt29bKumZgmpBQRhJxVrZbpC0eUQsKboWYGNh+9GIKP2KQlid7c0i4vWi60iF7Z9K2kvSrcqu7h0j6SFJsyVGmsuioegCkBbbN9numfekmyFplu0vF10XsBFhZABvs72/7RnKr2bZHpJ3WSm7JyX9Tu/8f7lV0vPK+g/T1L8kGEnFamw/FhH72D5V0nBJX5E0mXYoQPtgJBXVbD8o6URJ4yv/LmxPi4g9i60sbbZ/HBFnF10HNixGUtFSF9tdJB0r6daIWCFGfoB3Jb9k2dJ9dS8ESYuIlnMtVxZSSMdC3+ES4MYptHSlpLmSpki61/bOkpiTCrTBdsuV2izpQ7a3lKSIODr/86w6l4a0PWP7AElhu6ukMXrnRlag1Ljcj3WybUmdKj38aJ8DtM72I8rmcV+j7OqDlTX0P1mSImJicdUhVba3lfQjSYcq+zdzl6QxEfFKoYUlzvYjETGs7TPRkXG5H+sUmeom0+cUVgyQthGSJkv6mqTFEXGPpDciYiIBFeswMCJOjYheEbFdRJwmaXDRRXUALroAbHhc7sf64o0BaEVErJL0/2z/Kv/zBfEei7b9WFLLEcHW9mF1Pyq6AGx4vIFifTE/BFiHiJgv6Z9sf0zM58Za2N5f0gGSGm1/qepQT0mdiqmqeLY7S/qcpOMk7aDsd85zylpQXZvfzKuIuL6oGlE/hFSsL0ZSgRpExO3KlroEWtNV0ubKfg9X9/1coqwlVVn9TNKrkr6lbNUpSeoj6dOSfi7ppEKqQiG4cQptsn16RFyXP76Cu5MBoH3Y3jki/lF0HamwPSsiBq7l2OyIGFDvmlAcbpxCLb5deUBABYB2tYntq2zfZfvPla+iiyrQItv/lC/LLSlbotv2SZIWFVgXCsBIKiRJtqeu7ZCkARGxST3rAYAysD1F0lhlnSHebuIfEZMLK6pAtvtJukTSIXonlG4p6W5JF0bE08VUhiIQUiFJyu9E/ojW/KRqSfdHxA71rwoANm62J0fE8KLrSJHtbZTllJeKrgXF4MYpVPxe0uYR8VjLA7bvqXs1AFAOt9k+U9JvJb1V2UkzfykiXq7etn1YRPyxqHpQf4ykAgBQENutXb6OiNil7sUkzva8iOhbdB2oH0ZSAQAoSET0L7qGlNgev7ZDkrapZy0oHiEVAICC2O4u6UuS+kbEaNu7K1sq9fcFl1aUD0g6TdJrLfZb0sj6l4MiEVIBACjOdcru7D8g354v6VfK7hMoo79JWhYRE1sesD2rgHpQIOakAgBQENuTImKE7UcjYmi+b0pEDCm6NqBoNPMHAKA4y21vqmyNetneVVV3+aN1th8ougZseFzuBwCgON+UdIeknWzfKOn9kj5TaEUdQ7eiC8CGx+V+AAAKlDetf5+ym4P+RvP6ttl+JCKGFV0HNiwu9wMAUBDb75f0ZkTcrmz5z6/a3rnYqoA0EFIBACjO/0haZnuIpC9L+oeknxZbUofgogvAhkdIBQCgOM2Rzbs7RtLlEfEjST0KrikJtre3fbTto2xv3+LwJwspCnVFSAUAoDhLbf+bsgb2t9vuJKlLwTUVzvbnJT0k6XhJJ0r6m+3PVo5HxLSiakP9cOMUAAAFyUcIPyHp4Yj4i+2+kg6OiFJf8s8b9x8QES/n29tIuj8iBhZbGeqJkAoAQKJsPxAR+xddR73Z/pOkIyJieb7dVdKEiDi02MpQT/RJBQAgXaXqB2r7S/nDZyU9aPtWZQsdHKPs8j9KhJAKAEC6yna5s3LT2JP5V8WtBdSCgnG5HwCARNG0HmXGSCoAAOkqZT9Q23erlVHkiDikgHJQEEIqAAAFyu/wH6kslD0cEQuqDpe1H+j5VY+7STpBUnNBtaAgXO4HAKAgeT/Qb0j6s7JR04MkXRQR4wotLEG2J0bEQUXXgfphJBUAgOJ8WdLQlv1AJZU6pNreumqzQdJwSS1XncJGjpAKAEBx5ktaWrW9VNIzBdWSksnKpj9Y2WX+pyV9rtCKUHeEVAAA6ox+oOsWEf2LrgHFI6QCAFB/9ANtg+0DJPVTVVYp+3KxZcONUwAAICm2fyZpV0mPSVqZ746IGFNYUag7QioAAAWhH2jrbD8hqSkIKaXG5X4AAIpDP9DWTVN2N//zRReC4jCSCgBAQsrcD9T2bcpGlntI2kfZTWRvVY5HxNHFVIYiMJIKAEBB6Ae6hh8WXQDSQUgFAKA49AOtEhETaznP9gMRsf+GrgfFIqQCAFAQ+oG+a92KLgAbHiEVAIAC0Q/0XeGGmhIgpAIAUJC19QOVREhF6RFSAQAozgjRD/RttjeJiLfaPlPe4MWgcA1FFwAAQIlV+oEi84D09gjzunyyDrWgYIykAgBQZy36gc6wTT/QTFfbn5Z0gO3jWx6MiFvyP6fVvTLUHSEVAID6ox9o686QdKqkLSUd1eJYSLql3gWhOKw4BQBAosraD9T2WRFxRYt9tc5XxUaCOakAAKSrrP1AP9vKvgfqXgUKxeV+AADSVarLnba3l7SjpE1tD9U7d/H3lNS9sMJQCEIqAABIxUckfUZSH0mX6p2QukTSVwuqCQVhTioAAHVW6/xK249GxNB61JQS2ydExG/WcfzTEXFDPWtC/TEnFQCA+qMf6DqsK6DmzqlLISgUl/sBAKg/+oG+N6w4VQKEVAAA6o9+oO8NcxVLgJAKAECdRcRfJf3V9vTW+oEWVFZHwkhqCTAnFQCA4tAPtIrtMbZ3quHU+zZ4MSgcd/cDAFBnVf1Afy7pE1q9H+jYiBhUVG1Fsr1Y0uuSnpT0C0m/ioiFxVaFonC5HwCA+qMfaOuekjRc0qGSTpL0bduTlQXWWyJiaZHFob4YSQUAoCD0A12d7UciYljVdhdJR0g6RdKhEdFYWHGoO0IqAACJahnaNnbrWrzA9qYR8Ua9a0JxuHEKAIB0le0u9pPWdoCAWj6EVAAA0lWqy50RMbvoGpAOQioAAOkq20gq8DZCKgAAdWZ7P9s988eb2v627dtsX2J7i6pT6QeK0uLGKQAA6sz2dElDIqLZ9lWSlkn6taQP5/uPL7RAIAH0SQUAoP4aIqI5fzyi6g7+v9p+rKCagKRwuR8AgPqbZvv0/PEU2yMkyfYASSuKKwtIB5f7AQCos3ze6Y8kfUDSS5KGSXom/xoTEVMKLA9IAiEVAICC2O4haRdl0+/mR8QLBZcEJIOQCgAAgOQwJxUAAADJIaQCAAAgOYRUAAAAJIeQCgAAgOQQUgEAAJCc/w/vpO2pUHGidgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 720x504 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot and compare all of the model results\n",
    "all_model_results.plot(kind=\"bar\", figsize=(10, 7)).legend(bbox_to_anchor=(1.0, 1.0));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "avbdkiIuKNNr"
   },
   "source": [
    "Looks like our pretrained USE TensorFlow Hub models have the best performance, even the one with only 10% of the training data seems to outperform the other models. This goes to show the power of transfer learning.\n",
    "\n",
    "How about we drill down and get the F1-score's of each model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 546
    },
    "id": "yktdOiufmm3p",
    "outputId": "0b51f709-b672-4439-e479-663066c9e7c0"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x504 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Sort model results by f1-score\n",
    "all_model_results.sort_values(\"f1\", ascending=False)[\"f1\"].plot(kind=\"bar\", figsize=(10, 7));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pv2iE0TPGdNy"
   },
   "source": [
    "Drilling down into a single metric we see our USE TensorFlow Hub models performing  better than all of the other models. Interestingly, the baseline's F1-score isn't too far off the rest of the deeper models.\n",
    "\n",
    "We can also visualize all of our model's training logs using TensorBoard.dev."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2Ca8TalwGhPf"
   },
   "outputs": [],
   "source": [
    "# # View tensorboard logs of transfer learning modelling experiments (should be 4 models)\n",
    "# # Upload TensorBoard dev records\n",
    "# !tensorboard dev upload --logdir ./model_logs \\\n",
    "#   --name \"NLP modelling experiments\" \\\n",
    "#   --description \"A series of different NLP modellings experiments with various models\" \\\n",
    "#   --one_shot # exits the uploader when upload has finished"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uIYVXCUJ3FBn"
   },
   "source": [
    "The TensorBoard logs of the different modelling experiments we ran can be viewed here: https://tensorboard.dev/experiment/LkoAakb7QIKBZ0RL97cXbw/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Os7dv00u21jg"
   },
   "outputs": [],
   "source": [
    "# If you need to remove previous experiments, you can do so using the following command\n",
    "# !tensorboard dev delete --experiment_id EXPERIMENT_ID_TO_DELETE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GGVZhTTiGdd5"
   },
   "source": [
    "## Combining our models (model ensembling/stacking)\n",
    "\n",
    "Many production systems use an **ensemble** (multiple different models combined) of models to make a prediction.\n",
    "\n",
    "The idea behind model stacking is that if several uncorrelated models agree on a prediction, then the prediction must be more robust than a prediction made by a singular model.\n",
    "\n",
    "The keyword in the sentence above is **uncorrelated**, which is another way of saying, different types of models. For example, in our case, we might combine our baseline, our bidirectional model and our TensorFlow Hub USE model.\n",
    "\n",
    "Although these models are all trained on the same data, they all have a different way of finding patterns.\n",
    "\n",
    "If we were to use three similarly trained models, such as three LSTM models, the predictions they output will likely be very similar.\n",
    "\n",
    "Think of it as trying to decide where to eat with your friends. If you all have similar tastes, you'll probably all pick the same restaurant. But if you've all got different tastes and still end up picking the same restaurant, the restaurant must be good.\n",
    "\n",
    "Since we're working with a classification problem, there are a few of ways we can combine our models:\n",
    "1. **Averaging** - Take the output prediction probabilities of each model for each sample, combine them and then average them.\n",
    "2. **Majority vote (mode)** - Make class predictions with each of your models on all samples, the predicted class is the one in majority. For example, if three different models predict `[1, 0, 1]` respectively, the majority class is `1`, therefore, that would be the predicted label.\n",
    "3. **Model stacking** - Take the outputs of each of your chosen models and use them as inputs to another model.\n",
    "\n",
    "> 📖 **Resource:** The above methods for model stacking/ensembling were adapted from Chapter 6 of the [Machine Learning Engineering Book](http://www.mlebook.com/wiki/doku.php) by Andriy Burkov. If you're looking to enter the field of machine learning engineering, not only building models but production-scale machine learning systems, I'd highly recommend reading it in its entirety.\n",
    "\n",
    "Again, the concept of model stacking is best seen in action.\n",
    "\n",
    "We're going to combine our baseline model (`model_0`), LSTM model (`model_2`) and our USE model trained on the full training data (`model_6`) by averaging the combined prediction probabilities of each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "t63u8PCCm-yo",
    "outputId": "770d8969-088c-441e-d28c-a3970c4c5298"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(20,), dtype=float32, numpy=\n",
       "array([0., 1., 1., 0., 0., 1., 1., 1., 1., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 1.], dtype=float32)>"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get mean pred probs for 3 models\n",
    "baseline_pred_probs = np.max(model_0.predict_proba(val_sentences), axis=1) # get the prediction probabilities from baseline model\n",
    "combined_pred_probs = baseline_pred_probs + tf.squeeze(model_2_pred_probs, axis=1) + tf.squeeze(model_6_pred_probs)\n",
    "combined_preds = tf.round(combined_pred_probs/3) # average and round the prediction probabilities to get prediction classes\n",
    "combined_preds[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6abZa7wqlXSI"
   },
   "source": [
    "Wonderful! We've got a combined predictions array of different classes, let's evaluate them against the true labels and add our stacked model's results to our `all_model_results` DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ieYvhDiev8Et",
    "outputId": "f6a5f11e-75db-4a84-9f6a-2d9b326c7bfd"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 77.82152230971128,\n",
       " 'precision': 0.7778881584643952,\n",
       " 'recall': 0.7782152230971129,\n",
       " 'f1': 0.7778883624687377}"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate results from averaging the prediction probabilities\n",
    "ensemble_results = calculate_results(val_labels, combined_preds)\n",
    "ensemble_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "id": "132EHlUUpRrP"
   },
   "outputs": [],
   "source": [
    "# Add our combined model's results to the results DataFrame\n",
    "all_model_results.loc[\"ensemble_results\"] = ensemble_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "id": "Pm2P1zsvpZ3D"
   },
   "outputs": [],
   "source": [
    "# Convert the accuracy to the same scale as the rest of the results\n",
    "all_model_results.loc[\"ensemble_results\"][\"accuracy\"] = all_model_results.loc[\"ensemble_results\"][\"accuracy\"]/100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 332
    },
    "id": "trmdZ6eEpwHI",
    "outputId": "fac1d5dc-65c1-4c49-aaa9-8054abd4a043"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0_baseline</th>\n",
       "      <td>0.792651</td>\n",
       "      <td>0.811139</td>\n",
       "      <td>0.792651</td>\n",
       "      <td>0.786219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1_simple_dense</th>\n",
       "      <td>0.787402</td>\n",
       "      <td>0.791492</td>\n",
       "      <td>0.787402</td>\n",
       "      <td>0.784697</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2_lstm</th>\n",
       "      <td>0.750656</td>\n",
       "      <td>0.751008</td>\n",
       "      <td>0.750656</td>\n",
       "      <td>0.748927</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3_gru</th>\n",
       "      <td>0.767717</td>\n",
       "      <td>0.767545</td>\n",
       "      <td>0.767717</td>\n",
       "      <td>0.766793</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4_bidirectional</th>\n",
       "      <td>0.766404</td>\n",
       "      <td>0.766590</td>\n",
       "      <td>0.766404</td>\n",
       "      <td>0.765121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5_conv1d</th>\n",
       "      <td>0.778215</td>\n",
       "      <td>0.780752</td>\n",
       "      <td>0.778215</td>\n",
       "      <td>0.775881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6_tf_hub_sentence_encoder</th>\n",
       "      <td>0.812336</td>\n",
       "      <td>0.815211</td>\n",
       "      <td>0.812336</td>\n",
       "      <td>0.810581</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7_tf_hub_10_percent_data</th>\n",
       "      <td>0.783465</td>\n",
       "      <td>0.788873</td>\n",
       "      <td>0.783465</td>\n",
       "      <td>0.780192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ensemble_results</th>\n",
       "      <td>0.778215</td>\n",
       "      <td>0.777888</td>\n",
       "      <td>0.778215</td>\n",
       "      <td>0.777888</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           accuracy  precision    recall        f1\n",
       "0_baseline                 0.792651   0.811139  0.792651  0.786219\n",
       "1_simple_dense             0.787402   0.791492  0.787402  0.784697\n",
       "2_lstm                     0.750656   0.751008  0.750656  0.748927\n",
       "3_gru                      0.767717   0.767545  0.767717  0.766793\n",
       "4_bidirectional            0.766404   0.766590  0.766404  0.765121\n",
       "5_conv1d                   0.778215   0.780752  0.778215  0.775881\n",
       "6_tf_hub_sentence_encoder  0.812336   0.815211  0.812336  0.810581\n",
       "7_tf_hub_10_percent_data   0.783465   0.788873  0.783465  0.780192\n",
       "ensemble_results           0.778215   0.777888  0.778215  0.777888"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_model_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HZwqwF_swdIA"
   },
   "source": [
    "How did the stacked model go against the other models?\n",
    "\n",
    "> 🔑 **Note:** It seems many of our model's results are similar. This may mean there are some limitations to what can be learned from our data. When many of your modelling experiments return similar results, it's a good idea to revisit your data, we'll do this shortly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UpwErZOgX_nC"
   },
   "source": [
    "## Saving and loading a trained model\n",
    "\n",
    "Although training time didn't take very long, it's good practice to save your trained models to avoid having to retrain them.\n",
    "\n",
    "Saving your models also enables you to export them for use elsewhere outside of your notebooks, such as in a web application.\n",
    "\n",
    "There are two main ways of [saving a model in TensorFlow](https://www.tensorflow.org/tutorials/keras/save_and_load#save_the_entire_model):\n",
    "1. The `HDF5` format. \n",
    "2. The `SavedModel` format (default).\n",
    "\n",
    "Let's take a look at both."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "id": "SlwjGFVyX-_T"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-22 19:03:52.871074: W tensorflow/core/common_runtime/bfc_allocator.cc:433] Allocator (GPU_0_bfc) ran out of memory trying to allocate 32.55MiB (rounded to 34133760)requested by op ReadVariableOp\n",
      "Current allocation summary follows.\n",
      "2022-08-22 19:03:52.871137: I tensorflow/core/common_runtime/bfc_allocator.cc:972] BFCAllocator dump for GPU_0_bfc\n",
      "2022-08-22 19:03:52.871149: I tensorflow/core/common_runtime/bfc_allocator.cc:979] Bin (256): \tTotal Chunks: 318, Chunks in use: 318. 79.5KiB allocated for chunks. 79.5KiB in use in bin. 11.9KiB client-requested in use in bin.\n",
      "2022-08-22 19:03:52.871155: I tensorflow/core/common_runtime/bfc_allocator.cc:979] Bin (512): \tTotal Chunks: 6, Chunks in use: 6. 3.0KiB allocated for chunks. 3.0KiB in use in bin. 3.0KiB client-requested in use in bin.\n",
      "2022-08-22 19:03:52.871162: I tensorflow/core/common_runtime/bfc_allocator.cc:979] Bin (1024): \tTotal Chunks: 13, Chunks in use: 13. 16.0KiB allocated for chunks. 16.0KiB in use in bin. 14.5KiB client-requested in use in bin.\n",
      "2022-08-22 19:03:52.871168: I tensorflow/core/common_runtime/bfc_allocator.cc:979] Bin (2048): \tTotal Chunks: 10, Chunks in use: 8. 29.0KiB allocated for chunks. 23.5KiB in use in bin. 22.2KiB client-requested in use in bin.\n",
      "2022-08-22 19:03:52.871174: I tensorflow/core/common_runtime/bfc_allocator.cc:979] Bin (4096): \tTotal Chunks: 5, Chunks in use: 5. 31.2KiB allocated for chunks. 31.2KiB in use in bin. 25.0KiB client-requested in use in bin.\n",
      "2022-08-22 19:03:52.871179: I tensorflow/core/common_runtime/bfc_allocator.cc:979] Bin (8192): \tTotal Chunks: 159, Chunks in use: 159. 2.08MiB allocated for chunks. 2.08MiB in use in bin. 2.07MiB client-requested in use in bin.\n",
      "2022-08-22 19:03:52.871184: I tensorflow/core/common_runtime/bfc_allocator.cc:979] Bin (16384): \tTotal Chunks: 171, Chunks in use: 171. 3.56MiB allocated for chunks. 3.56MiB in use in bin. 3.53MiB client-requested in use in bin.\n",
      "2022-08-22 19:03:52.871190: I tensorflow/core/common_runtime/bfc_allocator.cc:979] Bin (32768): \tTotal Chunks: 97, Chunks in use: 96. 3.30MiB allocated for chunks. 3.26MiB in use in bin. 3.21MiB client-requested in use in bin.\n",
      "2022-08-22 19:03:52.871195: I tensorflow/core/common_runtime/bfc_allocator.cc:979] Bin (65536): \tTotal Chunks: 16, Chunks in use: 16. 1.16MiB allocated for chunks. 1.16MiB in use in bin. 1.16MiB client-requested in use in bin.\n",
      "2022-08-22 19:03:52.871200: I tensorflow/core/common_runtime/bfc_allocator.cc:979] Bin (131072): \tTotal Chunks: 22, Chunks in use: 22. 2.87MiB allocated for chunks. 2.87MiB in use in bin. 2.75MiB client-requested in use in bin.\n",
      "2022-08-22 19:03:52.871204: I tensorflow/core/common_runtime/bfc_allocator.cc:979] Bin (262144): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2022-08-22 19:03:52.871209: I tensorflow/core/common_runtime/bfc_allocator.cc:979] Bin (524288): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2022-08-22 19:03:52.871213: I tensorflow/core/common_runtime/bfc_allocator.cc:979] Bin (1048576): \tTotal Chunks: 1, Chunks in use: 0. 1.42MiB allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2022-08-22 19:03:52.871217: I tensorflow/core/common_runtime/bfc_allocator.cc:979] Bin (2097152): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2022-08-22 19:03:52.871221: I tensorflow/core/common_runtime/bfc_allocator.cc:979] Bin (4194304): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2022-08-22 19:03:52.871225: I tensorflow/core/common_runtime/bfc_allocator.cc:979] Bin (8388608): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2022-08-22 19:03:52.871230: I tensorflow/core/common_runtime/bfc_allocator.cc:979] Bin (16777216): \tTotal Chunks: 1, Chunks in use: 0. 23.67MiB allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2022-08-22 19:03:52.871237: I tensorflow/core/common_runtime/bfc_allocator.cc:979] Bin (33554432): \tTotal Chunks: 87, Chunks in use: 87. 2.78GiB allocated for chunks. 2.78GiB in use in bin. 2.77GiB client-requested in use in bin.\n",
      "2022-08-22 19:03:52.871245: I tensorflow/core/common_runtime/bfc_allocator.cc:979] Bin (67108864): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2022-08-22 19:03:52.871250: I tensorflow/core/common_runtime/bfc_allocator.cc:979] Bin (134217728): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2022-08-22 19:03:52.871253: I tensorflow/core/common_runtime/bfc_allocator.cc:979] Bin (268435456): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2022-08-22 19:03:52.871258: I tensorflow/core/common_runtime/bfc_allocator.cc:995] Bin for 32.55MiB was 32.00MiB, Chunk State: \n",
      "2022-08-22 19:03:52.871262: I tensorflow/core/common_runtime/bfc_allocator.cc:1008] Next region of size 3021275136\n",
      "2022-08-22 19:03:52.871271: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 803dc0000 of size 256 next 4\n",
      "2022-08-22 19:03:52.871275: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 803dc0100 of size 256 next 5\n",
      "2022-08-22 19:03:52.871279: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 803dc0200 of size 256 next 11\n",
      "2022-08-22 19:03:52.871282: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 803dc0300 of size 256 next 7\n",
      "2022-08-22 19:03:52.871285: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 803dc0400 of size 256 next 12\n",
      "2022-08-22 19:03:52.871289: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 803dc0500 of size 256 next 3\n",
      "2022-08-22 19:03:52.871292: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 803dc0600 of size 256 next 6\n",
      "2022-08-22 19:03:52.871295: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 803dc0700 of size 256 next 13\n",
      "2022-08-22 19:03:52.871299: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 803dc0800 of size 512 next 9\n",
      "2022-08-22 19:03:52.871302: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 803dc0a00 of size 7680 next 8\n",
      "2022-08-22 19:03:52.871306: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 803dc2800 of size 256 next 10\n",
      "2022-08-22 19:03:52.871309: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 803dc2900 of size 256 next 14\n",
      "2022-08-22 19:03:52.871312: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 803dc2a00 of size 256 next 15\n",
      "2022-08-22 19:03:52.871315: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 803dc2b00 of size 256 next 16\n",
      "2022-08-22 19:03:52.871319: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 803dc2c00 of size 256 next 17\n",
      "2022-08-22 19:03:52.871322: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 803dc2d00 of size 256 next 18\n",
      "2022-08-22 19:03:52.871325: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 803dc2e00 of size 256 next 19\n",
      "2022-08-22 19:03:52.871328: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 803dc2f00 of size 256 next 20\n",
      "2022-08-22 19:03:52.871331: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 803dc3000 of size 512 next 21\n",
      "2022-08-22 19:03:52.871334: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 803dc3200 of size 256 next 22\n",
      "2022-08-22 19:03:52.871338: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 803dc3300 of size 512 next 23\n",
      "2022-08-22 19:03:52.871341: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 803dc3500 of size 256 next 24\n",
      "2022-08-22 19:03:52.871344: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 803dc3600 of size 256 next 25\n",
      "2022-08-22 19:03:52.871347: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 803dc3700 of size 256 next 66\n",
      "2022-08-22 19:03:52.871351: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 803dc3800 of size 256 next 27\n",
      "2022-08-22 19:03:52.871355: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 803dc3900 of size 1792 next 34\n",
      "2022-08-22 19:03:52.871358: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 803dc4000 of size 256 next 35\n",
      "2022-08-22 19:03:52.871361: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 803dc4100 of size 256 next 33\n",
      "2022-08-22 19:03:52.871364: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 803dc4200 of size 256 next 62\n",
      "2022-08-22 19:03:52.871368: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 803dc4300 of size 256 next 37\n",
      "2022-08-22 19:03:52.871371: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 803dc4400 of size 256 next 52\n",
      "2022-08-22 19:03:52.871374: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 803dc4500 of size 256 next 60\n",
      "2022-08-22 19:03:52.871377: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 803dc4600 of size 256 next 30\n",
      "2022-08-22 19:03:52.871380: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 803dc4700 of size 256 next 28\n",
      "2022-08-22 19:03:52.871383: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 803dc4800 of size 256 next 26\n",
      "2022-08-22 19:03:52.871387: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 803dc4900 of size 256 next 40\n",
      "2022-08-22 19:03:52.871390: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 803dc4a00 of size 256 next 49\n",
      "2022-08-22 19:03:52.871393: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 803dc4b00 of size 256 next 50\n",
      "2022-08-22 19:03:52.871396: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 803dc4c00 of size 256 next 41\n",
      "2022-08-22 19:03:52.871399: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 803dc4d00 of size 256 next 55\n",
      "2022-08-22 19:03:52.871402: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 803dc4e00 of size 256 next 48\n",
      "2022-08-22 19:03:52.871405: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 803dc4f00 of size 1024 next 38\n",
      "2022-08-22 19:03:52.871409: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 803dc5300 of size 256 next 54\n",
      "2022-08-22 19:03:52.871412: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 803dc5400 of size 256 next 69\n",
      "2022-08-22 19:03:52.871415: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 803dc5500 of size 256 next 58\n",
      "2022-08-22 19:03:52.871418: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 803dc5600 of size 256 next 39\n",
      "2022-08-22 19:03:52.871421: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 803dc5700 of size 256 next 42\n",
      "2022-08-22 19:03:52.871426: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 803dc5800 of size 3072 next 57\n",
      "2022-08-22 19:03:52.871430: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 803dc6400 of size 65536 next 44\n",
      "2022-08-22 19:03:52.871433: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 803dd6400 of size 65536 next 29\n",
      "2022-08-22 19:03:52.871437: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 803de6400 of size 131072 next 31\n",
      "2022-08-22 19:03:52.871440: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 803e06400 of size 131072 next 32\n",
      "2022-08-22 19:03:52.871444: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 803e26400 of size 131072 next 64\n",
      "2022-08-22 19:03:52.871447: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 803e46400 of size 65536 next 46\n",
      "2022-08-22 19:03:52.871451: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 803e56400 of size 1024 next 45\n",
      "2022-08-22 19:03:52.871454: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 803e56800 of size 1536 next 92\n",
      "2022-08-22 19:03:52.871457: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 803e56e00 of size 256 next 103\n",
      "2022-08-22 19:03:52.871461: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 803e56f00 of size 256 next 88\n",
      "2022-08-22 19:03:52.871464: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 803e57000 of size 512 next 114\n",
      "2022-08-22 19:03:52.871468: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 803e57200 of size 256 next 90\n",
      "2022-08-22 19:03:52.871471: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 803e57300 of size 512 next 85\n",
      "2022-08-22 19:03:52.871474: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 803e57500 of size 256 next 75\n",
      "2022-08-22 19:03:52.871477: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 803e57600 of size 1792 next 67\n",
      "2022-08-22 19:03:52.871480: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 803e57d00 of size 256 next 76\n",
      "2022-08-22 19:03:52.871483: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 803e57e00 of size 256 next 97\n",
      "2022-08-22 19:03:52.871487: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 803e57f00 of size 256 next 87\n",
      "2022-08-22 19:03:52.871490: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 803e58000 of size 256 next 80\n",
      "2022-08-22 19:03:52.871493: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 803e58100 of size 256 next 105\n",
      "2022-08-22 19:03:52.871496: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 803e58200 of size 256 next 100\n",
      "2022-08-22 19:03:52.871499: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 803e58300 of size 256 next 47\n",
      "2022-08-22 19:03:52.871502: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 803e58400 of size 256 next 86\n",
      "2022-08-22 19:03:52.871505: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 803e58500 of size 256 next 70\n",
      "2022-08-22 19:03:52.871509: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 803e58600 of size 256 next 98\n",
      "2022-08-22 19:03:52.871512: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 803e58700 of size 256 next 107\n",
      "2022-08-22 19:03:52.871515: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 803e58800 of size 1536 next 53\n",
      "2022-08-22 19:03:52.871518: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 803e58e00 of size 256 next 68\n",
      "2022-08-22 19:03:52.871522: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 803e58f00 of size 256 next 36\n",
      "2022-08-22 19:03:52.871525: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 803e59000 of size 256 next 111\n",
      "2022-08-22 19:03:52.871528: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 803e59100 of size 256 next 120\n",
      "2022-08-22 19:03:52.871531: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 803e59200 of size 256 next 99\n",
      "2022-08-22 19:03:52.871534: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 803e59300 of size 256 next 123\n",
      "2022-08-22 19:03:52.871537: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 803e59400 of size 3072 next 108\n",
      "2022-08-22 19:03:52.871541: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 803e5a000 of size 49152 next 51\n",
      "2022-08-22 19:03:52.871545: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 803e66000 of size 49152 next 59\n",
      "2022-08-22 19:03:52.871548: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 803e72000 of size 98304 next 56\n",
      "2022-08-22 19:03:52.871552: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 803e8a000 of size 98304 next 43\n",
      "2022-08-22 19:03:52.871555: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 803ea2000 of size 98304 next 63\n",
      "2022-08-22 19:03:52.871558: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 803eba000 of size 49152 next 65\n",
      "2022-08-22 19:03:52.871561: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 803ec6000 of size 1280 next 83\n",
      "2022-08-22 19:03:52.871565: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 803ec6500 of size 256 next 106\n",
      "2022-08-22 19:03:52.871568: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 803ec6600 of size 256 next 91\n",
      "2022-08-22 19:03:52.871571: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 803ec6700 of size 1024 next 79\n",
      "2022-08-22 19:03:52.871575: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 803ec6b00 of size 256 next 95\n",
      "2022-08-22 19:03:52.871578: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 803ec6c00 of size 512 next 61\n",
      "2022-08-22 19:03:52.871581: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 803ec6e00 of size 256 next 128\n",
      "2022-08-22 19:03:52.871584: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 803ec6f00 of size 256 next 130\n",
      "2022-08-22 19:03:52.871587: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 803ec7000 of size 256 next 137\n",
      "2022-08-22 19:03:52.871590: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 803ec7100 of size 1024 next 93\n",
      "2022-08-22 19:03:52.871594: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 803ec7500 of size 1024 next 104\n",
      "2022-08-22 19:03:52.871597: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 803ec7900 of size 1024 next 142\n",
      "2022-08-22 19:03:52.871600: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 803ec7d00 of size 3072 next 143\n",
      "2022-08-22 19:03:52.871603: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 803ec8900 of size 65536 next 78\n",
      "2022-08-22 19:03:52.871607: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 803ed8900 of size 65536 next 127\n",
      "2022-08-22 19:03:52.871610: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 803ee8900 of size 131072 next 84\n",
      "2022-08-22 19:03:52.871613: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 803f08900 of size 258816 next 1\n",
      "2022-08-22 19:03:52.871616: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 803f47c00 of size 1280 next 2\n",
      "2022-08-22 19:03:52.871620: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 803f48100 of size 65536 next 94\n",
      "2022-08-22 19:03:52.871623: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 803f58100 of size 65536 next 82\n",
      "2022-08-22 19:03:52.871626: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 803f68100 of size 131072 next 74\n",
      "2022-08-22 19:03:52.871629: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 803f88100 of size 131072 next 89\n",
      "2022-08-22 19:03:52.871633: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 803fa8100 of size 131072 next 125\n",
      "2022-08-22 19:03:52.871636: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 803fc8100 of size 65536 next 141\n",
      "2022-08-22 19:03:52.871639: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 803fd8100 of size 131072 next 72\n",
      "2022-08-22 19:03:52.871642: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 803ff8100 of size 65536 next 112\n",
      "2022-08-22 19:03:52.871645: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 804008100 of size 1024 next 121\n",
      "2022-08-22 19:03:52.871648: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 804008500 of size 256 next 126\n",
      "2022-08-22 19:03:52.871651: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 804008600 of size 256 next 134\n",
      "2022-08-22 19:03:52.871655: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 804008700 of size 256 next 151\n",
      "2022-08-22 19:03:52.871658: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 804008800 of size 256 next 159\n",
      "2022-08-22 19:03:52.871661: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 804008900 of size 256 next 161\n",
      "2022-08-22 19:03:52.871664: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 804008a00 of size 256 next 188\n",
      "2022-08-22 19:03:52.871667: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 804008b00 of size 256 next 210\n",
      "2022-08-22 19:03:52.871671: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 804008c00 of size 256 next 147\n",
      "2022-08-22 19:03:52.871675: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 804008d00 of size 256 next 140\n",
      "2022-08-22 19:03:52.871678: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 804008e00 of size 256 next 351\n",
      "2022-08-22 19:03:52.871681: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 804008f00 of size 256 next 357\n",
      "2022-08-22 19:03:52.871684: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 804009000 of size 256 next 109\n",
      "2022-08-22 19:03:52.871687: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 804009100 of size 256 next 131\n",
      "2022-08-22 19:03:52.871692: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 804009200 of size 256 next 124\n",
      "2022-08-22 19:03:52.871698: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 804009300 of size 256 next 96\n",
      "2022-08-22 19:03:52.871704: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 804009400 of size 256 next 136\n",
      "2022-08-22 19:03:52.871707: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 804009500 of size 256 next 81\n",
      "2022-08-22 19:03:52.871710: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 804009600 of size 256 next 71\n",
      "2022-08-22 19:03:52.871713: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 804009700 of size 256 next 164\n",
      "2022-08-22 19:03:52.871716: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 804009800 of size 256 next 172\n",
      "2022-08-22 19:03:52.871720: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 804009900 of size 256 next 148\n",
      "2022-08-22 19:03:52.871723: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 804009a00 of size 256 next 157\n",
      "2022-08-22 19:03:52.871726: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 804009b00 of size 256 next 122\n",
      "2022-08-22 19:03:52.871729: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 804009c00 of size 256 next 102\n",
      "2022-08-22 19:03:52.871732: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 804009d00 of size 256 next 117\n",
      "2022-08-22 19:03:52.871735: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 804009e00 of size 256 next 156\n",
      "2022-08-22 19:03:52.871739: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 804009f00 of size 256 next 144\n",
      "2022-08-22 19:03:52.871742: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 80400a000 of size 256 next 139\n",
      "2022-08-22 19:03:52.871745: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 80400a100 of size 256 next 192\n",
      "2022-08-22 19:03:52.871748: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 80400a200 of size 256 next 167\n",
      "2022-08-22 19:03:52.871751: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 80400a300 of size 256 next 163\n",
      "2022-08-22 19:03:52.871754: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 80400a400 of size 256 next 180\n",
      "2022-08-22 19:03:52.871757: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 80400a500 of size 256 next 154\n",
      "2022-08-22 19:03:52.871761: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 80400a600 of size 256 next 145\n",
      "2022-08-22 19:03:52.871764: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 80400a700 of size 256 next 189\n",
      "2022-08-22 19:03:52.871767: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 80400a800 of size 256 next 173\n",
      "2022-08-22 19:03:52.871780: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 80400a900 of size 256 next 183\n",
      "2022-08-22 19:03:52.871783: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 80400aa00 of size 256 next 129\n",
      "2022-08-22 19:03:52.871786: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 80400ab00 of size 256 next 171\n",
      "2022-08-22 19:03:52.871789: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 80400ac00 of size 256 next 176\n",
      "2022-08-22 19:03:52.871792: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 80400ad00 of size 81920 next 73\n",
      "2022-08-22 19:03:52.871796: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 80401ed00 of size 256 next 152\n",
      "2022-08-22 19:03:52.871799: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 80401ee00 of size 81920 next 77\n",
      "2022-08-22 19:03:52.871802: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 804032e00 of size 90880 next 177\n",
      "2022-08-22 19:03:52.871806: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 804049100 of size 256 next 185\n",
      "2022-08-22 19:03:52.871809: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 804049200 of size 256 next 149\n",
      "2022-08-22 19:03:52.871812: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 804049300 of size 2560 next 178\n",
      "2022-08-22 19:03:52.871815: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 804049d00 of size 3072 next 207\n",
      "2022-08-22 19:03:52.871818: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 80404a900 of size 7680 next 205\n",
      "2022-08-22 19:03:52.871822: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 80404c700 of size 81920 next 101\n",
      "2022-08-22 19:03:52.871835: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 804060700 of size 256 next 186\n",
      "2022-08-22 19:03:52.871838: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 804060800 of size 256 next 200\n",
      "2022-08-22 19:03:52.871841: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 804060900 of size 256 next 193\n",
      "2022-08-22 19:03:52.871844: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 804060a00 of size 256 next 162\n",
      "2022-08-22 19:03:52.871847: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 804060b00 of size 256 next 358\n",
      "2022-08-22 19:03:52.871850: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 804060c00 of size 256 next 359\n",
      "2022-08-22 19:03:52.871854: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 804060d00 of size 256 next 360\n",
      "2022-08-22 19:03:52.871857: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 804060e00 of size 256 next 361\n",
      "2022-08-22 19:03:52.871860: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 804060f00 of size 256 next 367\n",
      "2022-08-22 19:03:52.871863: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 804061000 of size 256 next 541\n",
      "2022-08-22 19:03:52.871867: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 804061100 of size 256 next 545\n",
      "2022-08-22 19:03:52.871872: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 804061200 of size 256 next 385\n",
      "2022-08-22 19:03:52.871886: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 804061300 of size 256 next 548\n",
      "2022-08-22 19:03:52.871889: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 804061400 of size 256 next 549\n",
      "2022-08-22 19:03:52.871892: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 804061500 of size 256 next 550\n",
      "2022-08-22 19:03:52.871896: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 804061600 of size 256 next 551\n",
      "2022-08-22 19:03:52.871899: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 804061700 of size 256 next 552\n",
      "2022-08-22 19:03:52.871902: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 804061800 of size 256 next 554\n",
      "2022-08-22 19:03:52.871905: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 804061900 of size 256 next 201\n",
      "2022-08-22 19:03:52.871909: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 804061a00 of size 256 next 133\n",
      "2022-08-22 19:03:52.871912: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 804061b00 of size 3072 next 175\n",
      "2022-08-22 19:03:52.871915: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 804062700 of size 34133760 next 116\n",
      "2022-08-22 19:03:52.871919: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 8060efe00 of size 34133760 next 202\n",
      "2022-08-22 19:03:52.871923: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 80817d500 of size 34133760 next 135\n",
      "2022-08-22 19:03:52.871926: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 80a20ac00 of size 22528 next 158\n",
      "2022-08-22 19:03:52.871930: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 80a210400 of size 22528 next 174\n",
      "2022-08-22 19:03:52.871942: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 80a215c00 of size 22528 next 179\n",
      "2022-08-22 19:03:52.871946: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 80a21b400 of size 22528 next 113\n",
      "2022-08-22 19:03:52.871949: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 80a220c00 of size 22528 next 208\n",
      "2022-08-22 19:03:52.871952: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 80a226400 of size 22528 next 118\n",
      "2022-08-22 19:03:52.871955: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 80a22bc00 of size 22528 next 110\n",
      "2022-08-22 19:03:52.871958: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 80a231400 of size 22528 next 204\n",
      "2022-08-22 19:03:52.871961: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 80a236c00 of size 22528 next 132\n",
      "2022-08-22 19:03:52.871965: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 80a23c400 of size 22528 next 206\n",
      "2022-08-22 19:03:52.871968: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 80a241c00 of size 34133760 next 190\n",
      "2022-08-22 19:03:52.871971: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 80c2cf300 of size 20480 next 197\n",
      "2022-08-22 19:03:52.871974: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 80c2d4300 of size 20480 next 184\n",
      "2022-08-22 19:03:52.871978: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 80c2d9300 of size 20480 next 203\n",
      "2022-08-22 19:03:52.871981: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 80c2de300 of size 20480 next 195\n",
      "2022-08-22 19:03:52.871984: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 80c2e3300 of size 20480 next 169\n",
      "2022-08-22 19:03:52.871987: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 80c2e8300 of size 20480 next 194\n",
      "2022-08-22 19:03:52.871991: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 80c2ed300 of size 20480 next 199\n",
      "2022-08-22 19:03:52.871994: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 80c2f2300 of size 20480 next 138\n",
      "2022-08-22 19:03:52.871997: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 80c2f7300 of size 20480 next 155\n",
      "2022-08-22 19:03:52.872000: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 80c2fc300 of size 20480 next 160\n",
      "2022-08-22 19:03:52.872003: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 80c301300 of size 34133760 next 170\n",
      "2022-08-22 19:03:52.872006: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 80e38ea00 of size 36864 next 168\n",
      "2022-08-22 19:03:52.872010: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 80e397a00 of size 36864 next 146\n",
      "2022-08-22 19:03:52.872013: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 80e3a0a00 of size 34816 next 191\n",
      "2022-08-22 19:03:52.872017: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 80e3a9200 of size 34816 next 115\n",
      "2022-08-22 19:03:52.872033: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 80e3b1a00 of size 34816 next 181\n",
      "2022-08-22 19:03:52.872037: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 80e3ba200 of size 34816 next 153\n",
      "2022-08-22 19:03:52.872040: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 80e3c2a00 of size 34816 next 165\n",
      "2022-08-22 19:03:52.872043: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 80e3cb200 of size 34816 next 119\n",
      "2022-08-22 19:03:52.872047: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 80e3d3a00 of size 34816 next 209\n",
      "2022-08-22 19:03:52.872050: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 80e3dc200 of size 34816 next 198\n",
      "2022-08-22 19:03:52.872054: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 80e3e4a00 of size 34133760 next 166\n",
      "2022-08-22 19:03:52.872057: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 810472100 of size 34816 next 187\n",
      "2022-08-22 19:03:52.872060: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 81047a900 of size 34816 next 150\n",
      "2022-08-22 19:03:52.872063: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 810483100 of size 34816 next 196\n",
      "2022-08-22 19:03:52.872067: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 81048b900 of size 34816 next 182\n",
      "2022-08-22 19:03:52.872070: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 810494100 of size 34816 next 211\n",
      "2022-08-22 19:03:52.872073: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 81049c900 of size 34816 next 212\n",
      "2022-08-22 19:03:52.872086: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 8104a5100 of size 34816 next 213\n",
      "2022-08-22 19:03:52.872089: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 8104ad900 of size 34816 next 214\n",
      "2022-08-22 19:03:52.872092: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 8104b6100 of size 34816 next 215\n",
      "2022-08-22 19:03:52.872095: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 8104be900 of size 34816 next 216\n",
      "2022-08-22 19:03:52.872098: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 8104c7100 of size 34133760 next 217\n",
      "2022-08-22 19:03:52.872101: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 812554800 of size 34816 next 218\n",
      "2022-08-22 19:03:52.872104: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 81255d000 of size 34816 next 219\n",
      "2022-08-22 19:03:52.872107: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 812565800 of size 34816 next 220\n",
      "2022-08-22 19:03:52.872111: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 81256e000 of size 34816 next 221\n",
      "2022-08-22 19:03:52.872114: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 812576800 of size 34816 next 222\n",
      "2022-08-22 19:03:52.872117: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 81257f000 of size 34816 next 223\n",
      "2022-08-22 19:03:52.872120: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 812587800 of size 34816 next 224\n",
      "2022-08-22 19:03:52.872123: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 812590000 of size 34816 next 225\n",
      "2022-08-22 19:03:52.872126: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 812598800 of size 34816 next 226\n",
      "2022-08-22 19:03:52.872129: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 8125a1000 of size 34816 next 227\n",
      "2022-08-22 19:03:52.872132: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 8125a9800 of size 34133760 next 228\n",
      "2022-08-22 19:03:52.872136: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 814636f00 of size 22528 next 229\n",
      "2022-08-22 19:03:52.872139: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 81463c700 of size 22528 next 230\n",
      "2022-08-22 19:03:52.872142: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 814641f00 of size 22528 next 231\n",
      "2022-08-22 19:03:52.872145: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 814647700 of size 22528 next 232\n",
      "2022-08-22 19:03:52.872148: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 81464cf00 of size 22528 next 233\n",
      "2022-08-22 19:03:52.872151: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 814652700 of size 22528 next 234\n",
      "2022-08-22 19:03:52.872154: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 814657f00 of size 22528 next 235\n",
      "2022-08-22 19:03:52.872159: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 81465d700 of size 22528 next 236\n",
      "2022-08-22 19:03:52.872174: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 814662f00 of size 22528 next 237\n",
      "2022-08-22 19:03:52.872178: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 814668700 of size 22528 next 238\n",
      "2022-08-22 19:03:52.872182: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 81466df00 of size 34133760 next 239\n",
      "2022-08-22 19:03:52.872185: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 8166fb600 of size 22528 next 240\n",
      "2022-08-22 19:03:52.872188: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 816700e00 of size 22528 next 241\n",
      "2022-08-22 19:03:52.872191: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 816706600 of size 22528 next 242\n",
      "2022-08-22 19:03:52.872195: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 81670be00 of size 22528 next 243\n",
      "2022-08-22 19:03:52.872198: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 816711600 of size 22528 next 244\n",
      "2022-08-22 19:03:52.872201: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 816716e00 of size 22528 next 245\n",
      "2022-08-22 19:03:52.872205: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 81671c600 of size 22528 next 246\n",
      "2022-08-22 19:03:52.872208: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 816721e00 of size 22528 next 247\n",
      "2022-08-22 19:03:52.872211: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 816727600 of size 22528 next 248\n",
      "2022-08-22 19:03:52.872214: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 81672ce00 of size 22528 next 249\n",
      "2022-08-22 19:03:52.872227: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 816732600 of size 34133760 next 250\n",
      "2022-08-22 19:03:52.872230: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 8187bfd00 of size 20480 next 251\n",
      "2022-08-22 19:03:52.872233: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 8187c4d00 of size 20480 next 252\n",
      "2022-08-22 19:03:52.872236: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 8187c9d00 of size 20480 next 253\n",
      "2022-08-22 19:03:52.872239: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 8187ced00 of size 20480 next 254\n",
      "2022-08-22 19:03:52.872242: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 8187d3d00 of size 20480 next 255\n",
      "2022-08-22 19:03:52.872245: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 8187d8d00 of size 20480 next 256\n",
      "2022-08-22 19:03:52.872249: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 8187ddd00 of size 20480 next 257\n",
      "2022-08-22 19:03:52.872252: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 8187e2d00 of size 20480 next 258\n",
      "2022-08-22 19:03:52.872255: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 8187e7d00 of size 20480 next 259\n",
      "2022-08-22 19:03:52.872258: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 8187ecd00 of size 20480 next 260\n",
      "2022-08-22 19:03:52.872261: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 8187f1d00 of size 34133760 next 261\n",
      "2022-08-22 19:03:52.872264: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 81a87f400 of size 34133760 next 262\n",
      "2022-08-22 19:03:52.872267: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 81c90cb00 of size 34133760 next 263\n",
      "2022-08-22 19:03:52.872270: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 81e99a200 of size 34133760 next 264\n",
      "2022-08-22 19:03:52.872274: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 820a27900 of size 34133760 next 265\n",
      "2022-08-22 19:03:52.872277: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 822ab5000 of size 34133760 next 266\n",
      "2022-08-22 19:03:52.872280: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 824b42700 of size 34133760 next 267\n",
      "2022-08-22 19:03:52.872283: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 826bcfe00 of size 34133760 next 268\n",
      "2022-08-22 19:03:52.872287: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 828c5d500 of size 34133760 next 269\n",
      "2022-08-22 19:03:52.872290: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 82aceac00 of size 34133760 next 270\n",
      "2022-08-22 19:03:52.872294: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 82cd78300 of size 34133760 next 271\n",
      "2022-08-22 19:03:52.872310: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 82ee05a00 of size 34133760 next 272\n",
      "2022-08-22 19:03:52.872314: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 830e93100 of size 14080 next 353\n",
      "2022-08-22 19:03:52.872317: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 830e96800 of size 14080 next 354\n",
      "2022-08-22 19:03:52.872321: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 830e99f00 of size 22528 next 355\n",
      "2022-08-22 19:03:52.872324: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 830e9f700 of size 36864 next 356\n",
      "2022-08-22 19:03:52.872327: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 830ea8700 of size 22528 next 372\n",
      "2022-08-22 19:03:52.872330: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 830eadf00 of size 22528 next 373\n",
      "2022-08-22 19:03:52.872334: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 830eb3700 of size 22528 next 365\n",
      "2022-08-22 19:03:52.872337: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 830eb8f00 of size 22528 next 364\n",
      "2022-08-22 19:03:52.872340: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 830ebe700 of size 22528 next 381\n",
      "2022-08-22 19:03:52.872344: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 830ec3f00 of size 22528 next 371\n",
      "2022-08-22 19:03:52.872347: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 830ec9700 of size 22528 next 370\n",
      "2022-08-22 19:03:52.872350: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 830ecef00 of size 22528 next 374\n",
      "2022-08-22 19:03:52.872362: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 830ed4700 of size 22528 next 376\n",
      "2022-08-22 19:03:52.872365: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 830ed9f00 of size 20480 next 379\n",
      "2022-08-22 19:03:52.872369: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 830edef00 of size 20480 next 363\n",
      "2022-08-22 19:03:52.872372: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 830ee3f00 of size 20480 next 380\n",
      "2022-08-22 19:03:52.872375: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 830ee8f00 of size 20480 next 377\n",
      "2022-08-22 19:03:52.872378: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 830eedf00 of size 20480 next 378\n",
      "2022-08-22 19:03:52.872381: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 830ef2f00 of size 20480 next 375\n",
      "2022-08-22 19:03:52.872385: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 830ef7f00 of size 20480 next 383\n",
      "2022-08-22 19:03:52.872398: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 830efcf00 of size 20480 next 384\n",
      "2022-08-22 19:03:52.872401: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 830f01f00 of size 57344 next 387\n",
      "2022-08-22 19:03:52.872405: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 830f0ff00 of size 36864 next 388\n",
      "2022-08-22 19:03:52.872409: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 830f18f00 of size 34816 next 389\n",
      "2022-08-22 19:03:52.872412: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 830f21700 of size 34816 next 390\n",
      "2022-08-22 19:03:52.872415: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 830f29f00 of size 34816 next 391\n",
      "2022-08-22 19:03:52.872418: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 830f32700 of size 34816 next 392\n",
      "2022-08-22 19:03:52.872421: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 830f3af00 of size 34816 next 393\n",
      "2022-08-22 19:03:52.872425: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 830f43700 of size 34816 next 394\n",
      "2022-08-22 19:03:52.872428: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 830f4bf00 of size 34816 next 395\n",
      "2022-08-22 19:03:52.872432: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 830f54700 of size 34816 next 396\n",
      "2022-08-22 19:03:52.872435: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 830f5cf00 of size 34816 next 398\n",
      "2022-08-22 19:03:52.872438: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 830f65700 of size 34816 next 399\n",
      "2022-08-22 19:03:52.872441: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 830f6df00 of size 34816 next 400\n",
      "2022-08-22 19:03:52.872445: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 830f76700 of size 34816 next 401\n",
      "2022-08-22 19:03:52.872448: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 830f7ef00 of size 34816 next 402\n",
      "2022-08-22 19:03:52.872451: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 830f87700 of size 34816 next 403\n",
      "2022-08-22 19:03:52.872454: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 830f8ff00 of size 34816 next 404\n",
      "2022-08-22 19:03:52.872458: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 830f98700 of size 34816 next 405\n",
      "2022-08-22 19:03:52.872463: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 830fa0f00 of size 34816 next 406\n",
      "2022-08-22 19:03:52.872469: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 830fa9700 of size 34816 next 407\n",
      "2022-08-22 19:03:52.872473: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 830fb1f00 of size 34816 next 409\n",
      "2022-08-22 19:03:52.872479: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 830fba700 of size 34816 next 410\n",
      "2022-08-22 19:03:52.872484: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 830fc2f00 of size 34816 next 411\n",
      "2022-08-22 19:03:52.872490: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 830fcb700 of size 34816 next 412\n",
      "2022-08-22 19:03:52.872495: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 830fd3f00 of size 34816 next 413\n",
      "2022-08-22 19:03:52.872501: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 830fdc700 of size 34816 next 414\n",
      "2022-08-22 19:03:52.872507: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 830fe4f00 of size 34816 next 415\n",
      "2022-08-22 19:03:52.872512: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 830fed700 of size 34816 next 416\n",
      "2022-08-22 19:03:52.872518: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 830ff5f00 of size 34816 next 417\n",
      "2022-08-22 19:03:52.872523: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 830ffe700 of size 34816 next 418\n",
      "2022-08-22 19:03:52.872528: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 831006f00 of size 22528 next 420\n",
      "2022-08-22 19:03:52.872533: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 83100c700 of size 22528 next 421\n",
      "2022-08-22 19:03:52.872539: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 831011f00 of size 22528 next 422\n",
      "2022-08-22 19:03:52.872553: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 831017700 of size 22528 next 423\n",
      "2022-08-22 19:03:52.872568: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 83101cf00 of size 22528 next 424\n",
      "2022-08-22 19:03:52.872574: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 831022700 of size 22528 next 425\n",
      "2022-08-22 19:03:52.872580: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 831027f00 of size 22528 next 426\n",
      "2022-08-22 19:03:52.872586: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 83102d700 of size 22528 next 427\n",
      "2022-08-22 19:03:52.872591: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 831032f00 of size 22528 next 428\n",
      "2022-08-22 19:03:52.872598: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 831038700 of size 22528 next 429\n",
      "2022-08-22 19:03:52.872604: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 83103df00 of size 22528 next 431\n",
      "2022-08-22 19:03:52.872609: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 831043700 of size 22528 next 432\n",
      "2022-08-22 19:03:52.872614: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 831048f00 of size 22528 next 433\n",
      "2022-08-22 19:03:52.872620: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 83104e700 of size 22528 next 434\n",
      "2022-08-22 19:03:52.872636: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 831053f00 of size 22528 next 435\n",
      "2022-08-22 19:03:52.872642: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 831059700 of size 22528 next 436\n",
      "2022-08-22 19:03:52.872648: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 83105ef00 of size 22528 next 437\n",
      "2022-08-22 19:03:52.872653: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 831064700 of size 22528 next 438\n",
      "2022-08-22 19:03:52.872659: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 831069f00 of size 22528 next 439\n",
      "2022-08-22 19:03:52.872666: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 83106f700 of size 22528 next 440\n",
      "2022-08-22 19:03:52.872673: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 831074f00 of size 20480 next 442\n",
      "2022-08-22 19:03:52.872677: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 831079f00 of size 20480 next 443\n",
      "2022-08-22 19:03:52.872681: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 83107ef00 of size 20480 next 444\n",
      "2022-08-22 19:03:52.872684: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 831083f00 of size 20480 next 445\n",
      "2022-08-22 19:03:52.872688: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 831088f00 of size 20480 next 446\n",
      "2022-08-22 19:03:52.872691: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 83108df00 of size 20480 next 447\n",
      "2022-08-22 19:03:52.872695: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 831092f00 of size 20480 next 448\n",
      "2022-08-22 19:03:52.872698: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 831097f00 of size 20480 next 449\n",
      "2022-08-22 19:03:52.872702: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 83109cf00 of size 20480 next 450\n",
      "2022-08-22 19:03:52.872709: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 8310a1f00 of size 256 next 652\n",
      "2022-08-22 19:03:52.872713: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 8310a2000 of size 256 next 746\n",
      "2022-08-22 19:03:52.872717: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 8310a2100 of size 256 next 749\n",
      "2022-08-22 19:03:52.872721: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 8310a2200 of size 256 next 750\n",
      "2022-08-22 19:03:52.872724: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 8310a2300 of size 256 next 751\n",
      "2022-08-22 19:03:52.872727: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 8310a2400 of size 256 next 752\n",
      "2022-08-22 19:03:52.872732: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 8310a2500 of size 256 next 754\n",
      "2022-08-22 19:03:52.872737: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 8310a2600 of size 256 next 755\n",
      "2022-08-22 19:03:52.872740: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 8310a2700 of size 256 next 756\n",
      "2022-08-22 19:03:52.872744: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 8310a2800 of size 256 next 646\n",
      "2022-08-22 19:03:52.872747: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 8310a2900 of size 256 next 581\n",
      "2022-08-22 19:03:52.872750: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 8310a2a00 of size 256 next 757\n",
      "2022-08-22 19:03:52.872754: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 8310a2b00 of size 256 next 758\n",
      "2022-08-22 19:03:52.872757: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 8310a2c00 of size 256 next 759\n",
      "2022-08-22 19:03:52.872761: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 8310a2d00 of size 256 next 760\n",
      "2022-08-22 19:03:52.872764: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 8310a2e00 of size 256 next 761\n",
      "2022-08-22 19:03:52.872768: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 8310a2f00 of size 256 next 762\n",
      "2022-08-22 19:03:52.872771: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 8310a3000 of size 256 next 763\n",
      "2022-08-22 19:03:52.872775: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 8310a3100 of size 256 next 764\n",
      "2022-08-22 19:03:52.872778: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 8310a3200 of size 256 next 765\n",
      "2022-08-22 19:03:52.872781: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 8310a3300 of size 256 next 766\n",
      "2022-08-22 19:03:52.872785: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 8310a3400 of size 256 next 767\n",
      "2022-08-22 19:03:52.872788: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 8310a3500 of size 256 next 768\n",
      "2022-08-22 19:03:52.872791: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 8310a3600 of size 256 next 628\n",
      "2022-08-22 19:03:52.872795: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 8310a3700 of size 4864 next 600\n",
      "2022-08-22 19:03:52.872799: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 8310a4a00 of size 256 next 612\n",
      "2022-08-22 19:03:52.872803: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 8310a4b00 of size 256 next 590\n",
      "2022-08-22 19:03:52.872806: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 8310a4c00 of size 256 next 580\n",
      "2022-08-22 19:03:52.872809: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 8310a4d00 of size 256 next 624\n",
      "2022-08-22 19:03:52.872813: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 8310a4e00 of size 256 next 589\n",
      "2022-08-22 19:03:52.872816: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 8310a4f00 of size 256 next 622\n",
      "2022-08-22 19:03:52.872819: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 8310a5000 of size 256 next 629\n",
      "2022-08-22 19:03:52.872823: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 8310a5100 of size 256 next 574\n",
      "2022-08-22 19:03:52.872826: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 8310a5200 of size 256 next 627\n",
      "2022-08-22 19:03:52.872830: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 8310a5300 of size 256 next 598\n",
      "2022-08-22 19:03:52.872833: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 8310a5400 of size 256 next 584\n",
      "2022-08-22 19:03:52.872836: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 8310a5500 of size 256 next 620\n",
      "2022-08-22 19:03:52.872840: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 8310a5600 of size 256 next 641\n",
      "2022-08-22 19:03:52.872843: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 8310a5700 of size 256 next 610\n",
      "2022-08-22 19:03:52.872848: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 8310a5800 of size 256 next 586\n",
      "2022-08-22 19:03:52.872852: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 8310a5900 of size 256 next 577\n",
      "2022-08-22 19:03:52.872856: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 8310a5a00 of size 256 next 587\n",
      "2022-08-22 19:03:52.872860: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 8310a5b00 of size 256 next 634\n",
      "2022-08-22 19:03:52.872863: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 8310a5c00 of size 256 next 592\n",
      "2022-08-22 19:03:52.872866: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 8310a5d00 of size 256 next 576\n",
      "2022-08-22 19:03:52.872871: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 8310a5e00 of size 256 next 602\n",
      "2022-08-22 19:03:52.872874: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 8310a5f00 of size 256 next 769\n",
      "2022-08-22 19:03:52.872877: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 8310a6000 of size 256 next 770\n",
      "2022-08-22 19:03:52.872881: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 8310a6100 of size 256 next 771\n",
      "2022-08-22 19:03:52.872884: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 8310a6200 of size 256 next 772\n",
      "2022-08-22 19:03:52.872888: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 8310a6300 of size 256 next 793\n",
      "2022-08-22 19:03:52.872891: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 8310a6400 of size 256 next 816\n",
      "2022-08-22 19:03:52.872895: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 8310a6500 of size 256 next 815\n",
      "2022-08-22 19:03:52.872898: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 8310a6600 of size 256 next 777\n",
      "2022-08-22 19:03:52.872901: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 8310a6700 of size 256 next 773\n",
      "2022-08-22 19:03:52.872905: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 8310a6800 of size 256 next 782\n",
      "2022-08-22 19:03:52.872908: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 8310a6900 of size 256 next 785\n",
      "2022-08-22 19:03:52.872912: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 8310a6a00 of size 256 next 811\n",
      "2022-08-22 19:03:52.872915: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 8310a6b00 of size 256 next 788\n",
      "2022-08-22 19:03:52.872918: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 8310a6c00 of size 256 next 801\n",
      "2022-08-22 19:03:52.872922: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 8310a6d00 of size 256 next 820\n",
      "2022-08-22 19:03:52.872925: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 8310a6e00 of size 256 next 451\n",
      "2022-08-22 19:03:52.872929: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 8310a6f00 of size 14080 next 466\n",
      "2022-08-22 19:03:52.872932: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 8310aa600 of size 14080 next 467\n",
      "2022-08-22 19:03:52.872936: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 8310add00 of size 14080 next 468\n",
      "2022-08-22 19:03:52.872939: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 8310b1400 of size 14080 next 469\n",
      "2022-08-22 19:03:52.872943: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 8310b4b00 of size 14080 next 470\n",
      "2022-08-22 19:03:52.872946: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 8310b8200 of size 14080 next 471\n",
      "2022-08-22 19:03:52.872949: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 8310bb900 of size 14080 next 472\n",
      "2022-08-22 19:03:52.872953: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 8310bf000 of size 14080 next 473\n",
      "2022-08-22 19:03:52.872956: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 8310c2700 of size 14080 next 474\n",
      "2022-08-22 19:03:52.872960: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 8310c5e00 of size 14080 next 476\n",
      "2022-08-22 19:03:52.872965: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 8310c9500 of size 14080 next 477\n",
      "2022-08-22 19:03:52.872969: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 8310ccc00 of size 14080 next 478\n",
      "2022-08-22 19:03:52.872973: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 8310d0300 of size 14080 next 479\n",
      "2022-08-22 19:03:52.872976: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 8310d3a00 of size 14080 next 480\n",
      "2022-08-22 19:03:52.872980: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 8310d7100 of size 14080 next 481\n",
      "2022-08-22 19:03:52.872984: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 8310da800 of size 14080 next 482\n",
      "2022-08-22 19:03:52.872987: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 8310ddf00 of size 14080 next 483\n",
      "2022-08-22 19:03:52.872990: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 8310e1600 of size 14080 next 484\n",
      "2022-08-22 19:03:52.872994: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 8310e4d00 of size 14080 next 485\n",
      "2022-08-22 19:03:52.872997: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 8310e8400 of size 12800 next 487\n",
      "2022-08-22 19:03:52.873001: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 8310eb600 of size 12800 next 488\n",
      "2022-08-22 19:03:52.873005: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 8310ee800 of size 12800 next 489\n",
      "2022-08-22 19:03:52.873009: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 8310f1a00 of size 12800 next 490\n",
      "2022-08-22 19:03:52.873012: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 8310f4c00 of size 12800 next 491\n",
      "2022-08-22 19:03:52.873015: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 8310f7e00 of size 12800 next 492\n",
      "2022-08-22 19:03:52.873019: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 8310fb000 of size 12800 next 493\n",
      "2022-08-22 19:03:52.873022: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 8310fe200 of size 12800 next 494\n",
      "2022-08-22 19:03:52.873026: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 831101400 of size 12800 next 495\n",
      "2022-08-22 19:03:52.873029: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 831104600 of size 26880 next 498\n",
      "2022-08-22 19:03:52.873033: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 83110af00 of size 14080 next 499\n",
      "2022-08-22 19:03:52.873036: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 83110e600 of size 14080 next 500\n",
      "2022-08-22 19:03:52.873040: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 831111d00 of size 14080 next 501\n",
      "2022-08-22 19:03:52.873043: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 831115400 of size 14080 next 502\n",
      "2022-08-22 19:03:52.873046: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 831118b00 of size 14080 next 503\n",
      "2022-08-22 19:03:52.873050: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 83111c200 of size 14080 next 504\n",
      "2022-08-22 19:03:52.873053: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 83111f900 of size 14080 next 505\n",
      "2022-08-22 19:03:52.873057: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 831123000 of size 14080 next 506\n",
      "2022-08-22 19:03:52.873060: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 831126700 of size 14080 next 507\n",
      "2022-08-22 19:03:52.873063: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 831129e00 of size 14080 next 509\n",
      "2022-08-22 19:03:52.873067: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 83112d500 of size 14080 next 510\n",
      "2022-08-22 19:03:52.873070: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 831130c00 of size 14080 next 511\n",
      "2022-08-22 19:03:52.873074: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 831134300 of size 14080 next 512\n",
      "2022-08-22 19:03:52.873077: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 831137a00 of size 14080 next 513\n",
      "2022-08-22 19:03:52.873080: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 83113b100 of size 14080 next 514\n",
      "2022-08-22 19:03:52.873084: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 83113e800 of size 14080 next 515\n",
      "2022-08-22 19:03:52.873087: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 831141f00 of size 14080 next 516\n",
      "2022-08-22 19:03:52.873091: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 831145600 of size 14080 next 517\n",
      "2022-08-22 19:03:52.873095: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 831148d00 of size 14080 next 518\n",
      "2022-08-22 19:03:52.873098: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 83114c400 of size 12800 next 520\n",
      "2022-08-22 19:03:52.873102: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 83114f600 of size 12800 next 521\n",
      "2022-08-22 19:03:52.873105: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 831152800 of size 12800 next 522\n",
      "2022-08-22 19:03:52.873108: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 831155a00 of size 12800 next 523\n",
      "2022-08-22 19:03:52.873112: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 831158c00 of size 12800 next 524\n",
      "2022-08-22 19:03:52.873115: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 83115be00 of size 12800 next 525\n",
      "2022-08-22 19:03:52.873119: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 83115f000 of size 12800 next 526\n",
      "2022-08-22 19:03:52.873122: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 831162200 of size 12800 next 527\n",
      "2022-08-22 19:03:52.873126: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 831165400 of size 12800 next 528\n",
      "2022-08-22 19:03:52.873129: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 831168600 of size 35328 next 531\n",
      "2022-08-22 19:03:52.873133: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 831171000 of size 22528 next 532\n",
      "2022-08-22 19:03:52.873136: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 831176800 of size 22528 next 533\n",
      "2022-08-22 19:03:52.873140: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 83117c000 of size 22528 next 534\n",
      "2022-08-22 19:03:52.873143: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 831181800 of size 22528 next 535\n",
      "2022-08-22 19:03:52.873147: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 831187000 of size 22528 next 536\n",
      "2022-08-22 19:03:52.873150: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 83118c800 of size 22528 next 537\n",
      "2022-08-22 19:03:52.873153: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 831192000 of size 22528 next 538\n",
      "2022-08-22 19:03:52.873157: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 831197800 of size 22528 next 539\n",
      "2022-08-22 19:03:52.873160: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 83119d000 of size 22528 next 540\n",
      "2022-08-22 19:03:52.873164: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 8311a2800 of size 14080 next 543\n",
      "2022-08-22 19:03:52.873167: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 8311a5f00 of size 14080 next 544\n",
      "2022-08-22 19:03:52.873171: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 8311a9600 of size 22528 next 496\n",
      "2022-08-22 19:03:52.873174: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 8311aee00 of size 36864 next 529\n",
      "2022-08-22 19:03:52.873178: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 8311b7e00 of size 131072 next 553\n",
      "2022-08-22 19:03:52.873181: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 8311d7e00 of size 131072 next 547\n",
      "2022-08-22 19:03:52.873185: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 8311f7e00 of size 131072 next 546\n",
      "2022-08-22 19:03:52.873188: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 831217e00 of size 22528 next 631\n",
      "2022-08-22 19:03:52.873192: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 83121d600 of size 22528 next 650\n",
      "2022-08-22 19:03:52.873195: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 831222e00 of size 22528 next 656\n",
      "2022-08-22 19:03:52.873199: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 831228600 of size 22528 next 621\n",
      "2022-08-22 19:03:52.873202: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 83122de00 of size 22528 next 578\n",
      "2022-08-22 19:03:52.873206: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 831233600 of size 22528 next 655\n",
      "2022-08-22 19:03:52.873210: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 831238e00 of size 22528 next 626\n",
      "2022-08-22 19:03:52.873213: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 83123e600 of size 22528 next 594\n",
      "2022-08-22 19:03:52.873216: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 831243e00 of size 22528 next 653\n",
      "2022-08-22 19:03:52.873220: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 831249600 of size 22528 next 618\n",
      "2022-08-22 19:03:52.873223: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 83124ee00 of size 20480 next 617\n",
      "2022-08-22 19:03:52.873227: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 831253e00 of size 20480 next 605\n",
      "2022-08-22 19:03:52.873230: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 831258e00 of size 20480 next 603\n",
      "2022-08-22 19:03:52.873233: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 83125de00 of size 20480 next 638\n",
      "2022-08-22 19:03:52.873237: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 831262e00 of size 20480 next 636\n",
      "2022-08-22 19:03:52.873240: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 831267e00 of size 20480 next 643\n",
      "2022-08-22 19:03:52.873244: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 83126ce00 of size 20480 next 649\n",
      "2022-08-22 19:03:52.873247: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 831271e00 of size 20480 next 609\n",
      "2022-08-22 19:03:52.873251: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 831276e00 of size 20480 next 632\n",
      "2022-08-22 19:03:52.873254: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 83127be00 of size 20480 next 651\n",
      "2022-08-22 19:03:52.873257: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 831280e00 of size 36864 next 630\n",
      "2022-08-22 19:03:52.873261: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 831289e00 of size 36864 next 633\n",
      "2022-08-22 19:03:52.873264: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 831292e00 of size 34816 next 591\n",
      "2022-08-22 19:03:52.873268: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 83129b600 of size 34816 next 625\n",
      "2022-08-22 19:03:52.873271: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 8312a3e00 of size 34816 next 637\n",
      "2022-08-22 19:03:52.873275: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 8312ac600 of size 34816 next 619\n",
      "2022-08-22 19:03:52.873278: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 8312b4e00 of size 34816 next 593\n",
      "2022-08-22 19:03:52.873281: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 8312bd600 of size 34816 next 615\n",
      "2022-08-22 19:03:52.873285: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 8312c5e00 of size 34816 next 647\n",
      "2022-08-22 19:03:52.873288: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 8312ce600 of size 34816 next 604\n",
      "2022-08-22 19:03:52.873292: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 8312d6e00 of size 34816 next 614\n",
      "2022-08-22 19:03:52.873295: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 8312df600 of size 34816 next 597\n",
      "2022-08-22 19:03:52.873298: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 8312e7e00 of size 34816 next 613\n",
      "2022-08-22 19:03:52.873302: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 8312f0600 of size 34816 next 579\n",
      "2022-08-22 19:03:52.873305: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 8312f8e00 of size 34816 next 642\n",
      "2022-08-22 19:03:52.873309: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 831301600 of size 34816 next 601\n",
      "2022-08-22 19:03:52.873312: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 831309e00 of size 34816 next 644\n",
      "2022-08-22 19:03:52.873316: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 831312600 of size 34816 next 645\n",
      "2022-08-22 19:03:52.873319: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 83131ae00 of size 34816 next 639\n",
      "2022-08-22 19:03:52.873322: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 831323600 of size 34816 next 582\n",
      "2022-08-22 19:03:52.873326: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 83132be00 of size 34816 next 654\n",
      "2022-08-22 19:03:52.873329: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 831334600 of size 34816 next 608\n",
      "2022-08-22 19:03:52.873333: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 83133ce00 of size 34816 next 635\n",
      "2022-08-22 19:03:52.873336: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 831345600 of size 34816 next 595\n",
      "2022-08-22 19:03:52.873339: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 83134de00 of size 34816 next 596\n",
      "2022-08-22 19:03:52.873343: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 831356600 of size 34816 next 607\n",
      "2022-08-22 19:03:52.873346: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 83135ee00 of size 34816 next 611\n",
      "2022-08-22 19:03:52.873350: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 831367600 of size 34816 next 640\n",
      "2022-08-22 19:03:52.873353: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 83136fe00 of size 34816 next 599\n",
      "2022-08-22 19:03:52.873356: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 831378600 of size 34816 next 585\n",
      "2022-08-22 19:03:52.873360: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 831380e00 of size 22528 next 658\n",
      "2022-08-22 19:03:52.873363: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 831386600 of size 22528 next 659\n",
      "2022-08-22 19:03:52.873367: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 83138be00 of size 22528 next 660\n",
      "2022-08-22 19:03:52.873370: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 831391600 of size 22528 next 661\n",
      "2022-08-22 19:03:52.873373: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 831396e00 of size 22528 next 662\n",
      "2022-08-22 19:03:52.873377: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 83139c600 of size 22528 next 663\n",
      "2022-08-22 19:03:52.873380: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 8313a1e00 of size 22528 next 664\n",
      "2022-08-22 19:03:52.873384: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 8313a7600 of size 22528 next 665\n",
      "2022-08-22 19:03:52.873387: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 8313ace00 of size 22528 next 666\n",
      "2022-08-22 19:03:52.873390: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 8313b2600 of size 22528 next 667\n",
      "2022-08-22 19:03:52.873394: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 8313b7e00 of size 22528 next 669\n",
      "2022-08-22 19:03:52.873397: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 8313bd600 of size 22528 next 670\n",
      "2022-08-22 19:03:52.873401: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 8313c2e00 of size 22528 next 671\n",
      "2022-08-22 19:03:52.873404: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 8313c8600 of size 22528 next 672\n",
      "2022-08-22 19:03:52.873408: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 8313cde00 of size 22528 next 673\n",
      "2022-08-22 19:03:52.873411: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 8313d3600 of size 22528 next 674\n",
      "2022-08-22 19:03:52.873414: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 8313d8e00 of size 22528 next 675\n",
      "2022-08-22 19:03:52.873418: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 8313de600 of size 22528 next 676\n",
      "2022-08-22 19:03:52.873421: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 8313e3e00 of size 22528 next 677\n",
      "2022-08-22 19:03:52.873788: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 8313e9600 of size 22528 next 678\n",
      "2022-08-22 19:03:52.873796: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 8313eee00 of size 20480 next 680\n",
      "2022-08-22 19:03:52.873800: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 8313f3e00 of size 20480 next 681\n",
      "2022-08-22 19:03:52.873803: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 8313f8e00 of size 20480 next 682\n",
      "2022-08-22 19:03:52.873806: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 8313fde00 of size 20480 next 683\n",
      "2022-08-22 19:03:52.873810: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 831402e00 of size 20480 next 684\n",
      "2022-08-22 19:03:52.873813: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 831407e00 of size 20480 next 685\n",
      "2022-08-22 19:03:52.873816: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 83140ce00 of size 20480 next 686\n",
      "2022-08-22 19:03:52.873819: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 831411e00 of size 20480 next 687\n",
      "2022-08-22 19:03:52.873823: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 831416e00 of size 20480 next 688\n",
      "2022-08-22 19:03:52.873826: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 83141be00 of size 20480 next 689\n",
      "2022-08-22 19:03:52.873829: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 831420e00 of size 14080 next 704\n",
      "2022-08-22 19:03:52.873833: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 831424500 of size 14080 next 705\n",
      "2022-08-22 19:03:52.873836: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 831427c00 of size 14080 next 706\n",
      "2022-08-22 19:03:52.873839: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 83142b300 of size 14080 next 707\n",
      "2022-08-22 19:03:52.873842: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 83142ea00 of size 14080 next 708\n",
      "2022-08-22 19:03:52.873845: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 831432100 of size 14080 next 709\n",
      "2022-08-22 19:03:52.873849: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 831435800 of size 14080 next 710\n",
      "2022-08-22 19:03:52.873852: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 831438f00 of size 14080 next 711\n",
      "2022-08-22 19:03:52.873855: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 83143c600 of size 14080 next 712\n",
      "2022-08-22 19:03:52.873858: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 83143fd00 of size 14080 next 713\n",
      "2022-08-22 19:03:52.873861: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 831443400 of size 14080 next 715\n",
      "2022-08-22 19:03:52.873865: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 831446b00 of size 14080 next 716\n",
      "2022-08-22 19:03:52.873868: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 83144a200 of size 14080 next 717\n",
      "2022-08-22 19:03:52.873871: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 83144d900 of size 14080 next 718\n",
      "2022-08-22 19:03:52.873874: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 831451000 of size 14080 next 719\n",
      "2022-08-22 19:03:52.873878: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 831454700 of size 14080 next 720\n",
      "2022-08-22 19:03:52.873881: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 831457e00 of size 14080 next 721\n",
      "2022-08-22 19:03:52.873884: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 83145b500 of size 14080 next 722\n",
      "2022-08-22 19:03:52.873887: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 83145ec00 of size 14080 next 723\n",
      "2022-08-22 19:03:52.873890: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 831462300 of size 14080 next 724\n",
      "2022-08-22 19:03:52.873894: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 831465a00 of size 12800 next 726\n",
      "2022-08-22 19:03:52.874239: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 831468c00 of size 12800 next 727\n",
      "2022-08-22 19:03:52.874248: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 83146be00 of size 12800 next 728\n",
      "2022-08-22 19:03:52.874252: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 83146f000 of size 12800 next 729\n",
      "2022-08-22 19:03:52.874255: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 831472200 of size 12800 next 730\n",
      "2022-08-22 19:03:52.874259: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 831475400 of size 12800 next 731\n",
      "2022-08-22 19:03:52.874262: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 831478600 of size 12800 next 732\n",
      "2022-08-22 19:03:52.874265: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 83147b800 of size 12800 next 733\n",
      "2022-08-22 19:03:52.874269: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 83147ea00 of size 12800 next 734\n",
      "2022-08-22 19:03:52.874272: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 831481c00 of size 12800 next 735\n",
      "2022-08-22 19:03:52.874275: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 831484e00 of size 14080 next 736\n",
      "2022-08-22 19:03:52.874278: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 831488500 of size 14080 next 737\n",
      "2022-08-22 19:03:52.874281: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 83148bc00 of size 14080 next 738\n",
      "2022-08-22 19:03:52.874285: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 83148f300 of size 14080 next 739\n",
      "2022-08-22 19:03:52.874288: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 831492a00 of size 14080 next 740\n",
      "2022-08-22 19:03:52.874291: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 831496100 of size 14080 next 741\n",
      "2022-08-22 19:03:52.874295: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 831499800 of size 14080 next 742\n",
      "2022-08-22 19:03:52.874298: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 83149cf00 of size 14080 next 743\n",
      "2022-08-22 19:03:52.874301: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 8314a0600 of size 14080 next 744\n",
      "2022-08-22 19:03:52.874305: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 8314a3d00 of size 14080 next 745\n",
      "2022-08-22 19:03:52.874308: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 8314a7400 of size 131072 next 753\n",
      "2022-08-22 19:03:52.874311: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 8314c7400 of size 131072 next 747\n",
      "2022-08-22 19:03:52.874314: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 8314e7400 of size 131072 next 748\n",
      "2022-08-22 19:03:52.874318: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 831507400 of size 256 next 825\n",
      "2022-08-22 19:03:52.874321: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 831507500 of size 256 next 808\n",
      "2022-08-22 19:03:52.874325: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 831507600 of size 256 next 804\n",
      "2022-08-22 19:03:52.874328: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 831507700 of size 256 next 775\n",
      "2022-08-22 19:03:52.874331: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 831507800 of size 256 next 834\n",
      "2022-08-22 19:03:52.874334: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 831507900 of size 256 next 776\n",
      "2022-08-22 19:03:52.874338: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 831507a00 of size 256 next 783\n",
      "2022-08-22 19:03:52.874341: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 831507b00 of size 256 next 807\n",
      "2022-08-22 19:03:52.874344: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 831507c00 of size 256 next 837\n",
      "2022-08-22 19:03:52.874347: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 831507d00 of size 256 next 809\n",
      "2022-08-22 19:03:52.874675: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 831507e00 of size 256 next 822\n",
      "2022-08-22 19:03:52.874679: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 831507f00 of size 256 next 831\n",
      "2022-08-22 19:03:52.874682: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 831508000 of size 256 next 839\n",
      "2022-08-22 19:03:52.874688: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 831508100 of size 256 next 819\n",
      "2022-08-22 19:03:52.874692: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 831508200 of size 256 next 792\n",
      "2022-08-22 19:03:52.874696: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 831508300 of size 256 next 832\n",
      "2022-08-22 19:03:52.874699: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 831508400 of size 256 next 851\n",
      "2022-08-22 19:03:52.874702: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 831508500 of size 256 next 813\n",
      "2022-08-22 19:03:52.874705: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 831508600 of size 256 next 835\n",
      "2022-08-22 19:03:52.874709: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 831508700 of size 256 next 931\n",
      "2022-08-22 19:03:52.874712: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 831508800 of size 256 next 932\n",
      "2022-08-22 19:03:52.874715: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] Free  at 831508900 of size 2560 next 800\n",
      "2022-08-22 19:03:52.874718: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 831509300 of size 131072 next 827\n",
      "2022-08-22 19:03:52.874722: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 831529300 of size 256 next 794\n",
      "2022-08-22 19:03:52.874725: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 831529400 of size 256 next 812\n",
      "2022-08-22 19:03:52.874728: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 831529500 of size 256 next 779\n",
      "2022-08-22 19:03:52.874731: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 831529600 of size 256 next 784\n",
      "2022-08-22 19:03:52.874735: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 831529700 of size 256 next 774\n",
      "2022-08-22 19:03:52.874738: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 831529800 of size 256 next 828\n",
      "2022-08-22 19:03:52.874741: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 831529900 of size 256 next 823\n",
      "2022-08-22 19:03:52.874744: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 831529a00 of size 256 next 805\n",
      "2022-08-22 19:03:52.874748: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 831529b00 of size 256 next 840\n",
      "2022-08-22 19:03:52.874751: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 831529c00 of size 256 next 778\n",
      "2022-08-22 19:03:52.874754: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 831529d00 of size 256 next 790\n",
      "2022-08-22 19:03:52.874757: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 831529e00 of size 256 next 787\n",
      "2022-08-22 19:03:52.874761: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 831529f00 of size 256 next 829\n",
      "2022-08-22 19:03:52.874764: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 83152a000 of size 256 next 780\n",
      "2022-08-22 19:03:52.874767: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 83152a100 of size 256 next 818\n",
      "2022-08-22 19:03:52.874770: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 83152a200 of size 256 next 817\n",
      "2022-08-22 19:03:52.874774: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 83152a300 of size 256 next 826\n",
      "2022-08-22 19:03:52.874777: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 83152a400 of size 256 next 786\n",
      "2022-08-22 19:03:52.874780: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 83152a500 of size 256 next 791\n",
      "2022-08-22 19:03:52.874783: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 83152a600 of size 256 next 868\n",
      "2022-08-22 19:03:52.875152: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 83152a700 of size 256 next 859\n",
      "2022-08-22 19:03:52.875162: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 83152a800 of size 256 next 855\n",
      "2022-08-22 19:03:52.875167: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 83152a900 of size 16384 next 869\n",
      "2022-08-22 19:03:52.875171: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 83152e900 of size 256 next 889\n",
      "2022-08-22 19:03:52.875174: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 83152ea00 of size 256 next 849\n",
      "2022-08-22 19:03:52.875177: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 83152eb00 of size 256 next 836\n",
      "2022-08-22 19:03:52.875180: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 83152ec00 of size 256 next 841\n",
      "2022-08-22 19:03:52.875184: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 83152ed00 of size 25600 next 856\n",
      "2022-08-22 19:03:52.875187: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 831535100 of size 256 next 890\n",
      "2022-08-22 19:03:52.875191: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 831535200 of size 256 next 871\n",
      "2022-08-22 19:03:52.875194: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 831535300 of size 16384 next 870\n",
      "2022-08-22 19:03:52.875197: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 831539300 of size 256 next 900\n",
      "2022-08-22 19:03:52.875201: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 831539400 of size 256 next 882\n",
      "2022-08-22 19:03:52.875204: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 831539500 of size 256 next 898\n",
      "2022-08-22 19:03:52.875207: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 831539600 of size 256 next 880\n",
      "2022-08-22 19:03:52.875210: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 831539700 of size 256 next 799\n",
      "2022-08-22 19:03:52.875214: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 831539800 of size 256 next 887\n",
      "2022-08-22 19:03:52.875217: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 831539900 of size 256 next 848\n",
      "2022-08-22 19:03:52.875220: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 831539a00 of size 256 next 867\n",
      "2022-08-22 19:03:52.875224: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 831539b00 of size 256 next 866\n",
      "2022-08-22 19:03:52.875227: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 831539c00 of size 256 next 865\n",
      "2022-08-22 19:03:52.875230: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 831539d00 of size 256 next 899\n",
      "2022-08-22 19:03:52.875233: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 831539e00 of size 256 next 881\n",
      "2022-08-22 19:03:52.875236: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 831539f00 of size 256 next 852\n",
      "2022-08-22 19:03:52.875240: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 83153a000 of size 256 next 846\n",
      "2022-08-22 19:03:52.875243: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 83153a100 of size 256 next 847\n",
      "2022-08-22 19:03:52.875246: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 83153a200 of size 256 next 876\n",
      "2022-08-22 19:03:52.875249: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 83153a300 of size 256 next 886\n",
      "2022-08-22 19:03:52.875253: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 83153a400 of size 256 next 843\n",
      "2022-08-22 19:03:52.875256: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 83153a500 of size 256 next 893\n",
      "2022-08-22 19:03:52.875259: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 83153a600 of size 256 next 916\n",
      "2022-08-22 19:03:52.875262: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 83153a700 of size 256 next 903\n",
      "2022-08-22 19:03:52.875597: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 83153a800 of size 256 next 894\n",
      "2022-08-22 19:03:52.875609: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 83153a900 of size 256 next 781\n",
      "2022-08-22 19:03:52.875615: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 83153aa00 of size 256 next 803\n",
      "2022-08-22 19:03:52.875620: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 83153ab00 of size 256 next 915\n",
      "2022-08-22 19:03:52.875625: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 83153ac00 of size 256 next 913\n",
      "2022-08-22 19:03:52.875630: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 83153ad00 of size 256 next 873\n",
      "2022-08-22 19:03:52.875635: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 83153ae00 of size 256 next 797\n",
      "2022-08-22 19:03:52.875640: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 83153af00 of size 256 next 857\n",
      "2022-08-22 19:03:52.875645: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 83153b000 of size 256 next 850\n",
      "2022-08-22 19:03:52.875651: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 83153b100 of size 256 next 917\n",
      "2022-08-22 19:03:52.875656: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 83153b200 of size 256 next 904\n",
      "2022-08-22 19:03:52.875661: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 83153b300 of size 256 next 864\n",
      "2022-08-22 19:03:52.875666: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 83153b400 of size 256 next 942\n",
      "2022-08-22 19:03:52.875671: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 83153b500 of size 256 next 912\n",
      "2022-08-22 19:03:52.875676: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 83153b600 of size 256 next 914\n",
      "2022-08-22 19:03:52.875682: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 83153b700 of size 256 next 926\n",
      "2022-08-22 19:03:52.875687: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 83153b800 of size 256 next 796\n",
      "2022-08-22 19:03:52.875693: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 83153b900 of size 3072 next 946\n",
      "2022-08-22 19:03:52.875699: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 83153c500 of size 3072 next 938\n",
      "2022-08-22 19:03:52.875704: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] Free  at 83153d100 of size 3072 next 891\n",
      "2022-08-22 19:03:52.875708: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 83153dd00 of size 4352 next 875\n",
      "2022-08-22 19:03:52.875711: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 83153ee00 of size 256 next 844\n",
      "2022-08-22 19:03:52.875714: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] Free  at 83153ef00 of size 41984 next 833\n",
      "2022-08-22 19:03:52.875718: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 831549300 of size 131072 next 830\n",
      "2022-08-22 19:03:52.875721: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 831569300 of size 131072 next 789\n",
      "2022-08-22 19:03:52.875725: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 831589300 of size 131072 next 838\n",
      "2022-08-22 19:03:52.875728: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 8315a9300 of size 256 next 878\n",
      "2022-08-22 19:03:52.875731: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 8315a9400 of size 256 next 884\n",
      "2022-08-22 19:03:52.875735: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 8315a9500 of size 256 next 877\n",
      "2022-08-22 19:03:52.875738: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 8315a9600 of size 256 next 810\n",
      "2022-08-22 19:03:52.875741: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 8315a9700 of size 256 next 862\n",
      "2022-08-22 19:03:52.875744: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 8315a9800 of size 256 next 860\n",
      "2022-08-22 19:03:52.875984: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 8315a9900 of size 256 next 853\n",
      "2022-08-22 19:03:52.875987: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 8315a9a00 of size 256 next 858\n",
      "2022-08-22 19:03:52.875991: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 8315a9b00 of size 256 next 854\n",
      "2022-08-22 19:03:52.875994: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 8315a9c00 of size 256 next 863\n",
      "2022-08-22 19:03:52.875997: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 8315a9d00 of size 256 next 874\n",
      "2022-08-22 19:03:52.876001: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 8315a9e00 of size 256 next 821\n",
      "2022-08-22 19:03:52.876004: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 8315a9f00 of size 256 next 933\n",
      "2022-08-22 19:03:52.876007: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 8315aa000 of size 256 next 842\n",
      "2022-08-22 19:03:52.876010: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 8315aa100 of size 256 next 879\n",
      "2022-08-22 19:03:52.876013: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 8315aa200 of size 256 next 814\n",
      "2022-08-22 19:03:52.876017: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 8315aa300 of size 131072 next 883\n",
      "2022-08-22 19:03:52.876020: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 8315ca300 of size 131072 next 845\n",
      "2022-08-22 19:03:52.876023: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 8315ea300 of size 131072 next 897\n",
      "2022-08-22 19:03:52.876026: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] Free  at 83160a300 of size 1490688 next 802\n",
      "2022-08-22 19:03:52.876030: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 831776200 of size 256 next 806\n",
      "2022-08-22 19:03:52.876033: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] Free  at 831776300 of size 24814848 next 273\n",
      "2022-08-22 19:03:52.876036: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 832f20800 of size 34133760 next 274\n",
      "2022-08-22 19:03:52.876039: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 834fadf00 of size 14080 next 275\n",
      "2022-08-22 19:03:52.876043: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 834fb1600 of size 14080 next 276\n",
      "2022-08-22 19:03:52.876046: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 834fb4d00 of size 14080 next 277\n",
      "2022-08-22 19:03:52.876049: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 834fb8400 of size 14080 next 278\n",
      "2022-08-22 19:03:52.876052: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 834fbbb00 of size 14080 next 279\n",
      "2022-08-22 19:03:52.876055: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 834fbf200 of size 14080 next 280\n",
      "2022-08-22 19:03:52.876059: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 834fc2900 of size 14080 next 281\n",
      "2022-08-22 19:03:52.876062: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 834fc6000 of size 14080 next 282\n",
      "2022-08-22 19:03:52.876065: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 834fc9700 of size 14080 next 283\n",
      "2022-08-22 19:03:52.876068: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 834fcce00 of size 14080 next 284\n",
      "2022-08-22 19:03:52.876071: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 834fd0500 of size 34133760 next 285\n",
      "2022-08-22 19:03:52.876075: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 83705dc00 of size 14080 next 286\n",
      "2022-08-22 19:03:52.876078: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 837061300 of size 14080 next 287\n",
      "2022-08-22 19:03:52.876081: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 837064a00 of size 14080 next 288\n",
      "2022-08-22 19:03:52.876084: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 837068100 of size 14080 next 289\n",
      "2022-08-22 19:03:52.876257: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 83706b800 of size 14080 next 290\n",
      "2022-08-22 19:03:52.876260: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 83706ef00 of size 14080 next 291\n",
      "2022-08-22 19:03:52.876263: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 837072600 of size 14080 next 292\n",
      "2022-08-22 19:03:52.876267: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 837075d00 of size 14080 next 293\n",
      "2022-08-22 19:03:52.876270: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 837079400 of size 14080 next 294\n",
      "2022-08-22 19:03:52.876273: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 83707cb00 of size 14080 next 295\n",
      "2022-08-22 19:03:52.876276: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 837080200 of size 34133760 next 296\n",
      "2022-08-22 19:03:52.876280: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 83910d900 of size 12800 next 297\n",
      "2022-08-22 19:03:52.876283: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 839110b00 of size 12800 next 298\n",
      "2022-08-22 19:03:52.876286: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 839113d00 of size 12800 next 299\n",
      "2022-08-22 19:03:52.876289: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 839116f00 of size 12800 next 300\n",
      "2022-08-22 19:03:52.876293: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 83911a100 of size 12800 next 301\n",
      "2022-08-22 19:03:52.876296: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 83911d300 of size 12800 next 302\n",
      "2022-08-22 19:03:52.876299: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 839120500 of size 12800 next 303\n",
      "2022-08-22 19:03:52.876302: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 839123700 of size 12800 next 304\n",
      "2022-08-22 19:03:52.876305: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 839126900 of size 12800 next 305\n",
      "2022-08-22 19:03:52.876309: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 839129b00 of size 12800 next 306\n",
      "2022-08-22 19:03:52.876312: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 83912cd00 of size 34133760 next 307\n",
      "2022-08-22 19:03:52.876315: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 83b1ba400 of size 14080 next 308\n",
      "2022-08-22 19:03:52.876318: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 83b1bdb00 of size 14080 next 309\n",
      "2022-08-22 19:03:52.876321: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 83b1c1200 of size 14080 next 310\n",
      "2022-08-22 19:03:52.876325: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 83b1c4900 of size 14080 next 311\n",
      "2022-08-22 19:03:52.876328: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 83b1c8000 of size 14080 next 312\n",
      "2022-08-22 19:03:52.876331: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 83b1cb700 of size 14080 next 313\n",
      "2022-08-22 19:03:52.876334: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 83b1cee00 of size 14080 next 314\n",
      "2022-08-22 19:03:52.876337: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 83b1d2500 of size 14080 next 315\n",
      "2022-08-22 19:03:52.876340: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 83b1d5c00 of size 14080 next 316\n",
      "2022-08-22 19:03:52.876344: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 83b1d9300 of size 14080 next 317\n",
      "2022-08-22 19:03:52.876347: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 83b1dca00 of size 34133760 next 318\n",
      "2022-08-22 19:03:52.876350: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 83d26a100 of size 14080 next 319\n",
      "2022-08-22 19:03:52.876353: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 83d26d800 of size 14080 next 320\n",
      "2022-08-22 19:03:52.876356: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 83d270f00 of size 14080 next 321\n",
      "2022-08-22 19:03:52.876530: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 83d274600 of size 14080 next 322\n",
      "2022-08-22 19:03:52.876534: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 83d277d00 of size 14080 next 323\n",
      "2022-08-22 19:03:52.876537: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 83d27b400 of size 14080 next 324\n",
      "2022-08-22 19:03:52.876540: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 83d27eb00 of size 14080 next 325\n",
      "2022-08-22 19:03:52.876544: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 83d282200 of size 14080 next 326\n",
      "2022-08-22 19:03:52.876547: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 83d285900 of size 14080 next 327\n",
      "2022-08-22 19:03:52.876550: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 83d289000 of size 14080 next 328\n",
      "2022-08-22 19:03:52.876553: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 83d28c700 of size 34133760 next 329\n",
      "2022-08-22 19:03:52.876556: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 83f319e00 of size 12800 next 330\n",
      "2022-08-22 19:03:52.876560: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 83f31d000 of size 12800 next 331\n",
      "2022-08-22 19:03:52.876563: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 83f320200 of size 12800 next 332\n",
      "2022-08-22 19:03:52.876566: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 83f323400 of size 12800 next 333\n",
      "2022-08-22 19:03:52.876569: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 83f326600 of size 12800 next 334\n",
      "2022-08-22 19:03:52.876573: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 83f329800 of size 12800 next 335\n",
      "2022-08-22 19:03:52.876576: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 83f32ca00 of size 12800 next 336\n",
      "2022-08-22 19:03:52.876579: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 83f32fc00 of size 12800 next 337\n",
      "2022-08-22 19:03:52.876582: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 83f332e00 of size 12800 next 338\n",
      "2022-08-22 19:03:52.876585: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 83f336000 of size 256 next 555\n",
      "2022-08-22 19:03:52.876589: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 83f336100 of size 256 next 556\n",
      "2022-08-22 19:03:52.876592: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 83f336200 of size 256 next 557\n",
      "2022-08-22 19:03:52.876595: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 83f336300 of size 256 next 558\n",
      "2022-08-22 19:03:52.876599: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 83f336400 of size 256 next 559\n",
      "2022-08-22 19:03:52.876602: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 83f336500 of size 256 next 560\n",
      "2022-08-22 19:03:52.876605: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 83f336600 of size 256 next 561\n",
      "2022-08-22 19:03:52.876608: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 83f336700 of size 256 next 562\n",
      "2022-08-22 19:03:52.876611: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 83f336800 of size 256 next 563\n",
      "2022-08-22 19:03:52.876615: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 83f336900 of size 256 next 564\n",
      "2022-08-22 19:03:52.876618: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 83f336a00 of size 256 next 565\n",
      "2022-08-22 19:03:52.876621: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 83f336b00 of size 256 next 566\n",
      "2022-08-22 19:03:52.876624: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 83f336c00 of size 256 next 567\n",
      "2022-08-22 19:03:52.876627: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 83f336d00 of size 256 next 568\n",
      "2022-08-22 19:03:52.876630: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 83f336e00 of size 256 next 569\n",
      "2022-08-22 19:03:52.876809: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 83f336f00 of size 256 next 570\n",
      "2022-08-22 19:03:52.876813: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 83f337000 of size 256 next 571\n",
      "2022-08-22 19:03:52.876816: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 83f337100 of size 256 next 572\n",
      "2022-08-22 19:03:52.876819: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 83f337200 of size 256 next 573\n",
      "2022-08-22 19:03:52.876822: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 83f337300 of size 256 next 588\n",
      "2022-08-22 19:03:52.876835: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 83f337400 of size 256 next 362\n",
      "2022-08-22 19:03:52.876839: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 83f337500 of size 7424 next 339\n",
      "2022-08-22 19:03:52.876842: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 83f339200 of size 34133760 next 340\n",
      "2022-08-22 19:03:52.876846: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 8413c6900 of size 22528 next 341\n",
      "2022-08-22 19:03:52.876859: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 8413cc100 of size 22528 next 342\n",
      "2022-08-22 19:03:52.876862: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 8413d1900 of size 22528 next 343\n",
      "2022-08-22 19:03:52.876865: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 8413d7100 of size 22528 next 344\n",
      "2022-08-22 19:03:52.876868: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 8413dc900 of size 22528 next 345\n",
      "2022-08-22 19:03:52.876871: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 8413e2100 of size 22528 next 346\n",
      "2022-08-22 19:03:52.876874: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 8413e7900 of size 22528 next 347\n",
      "2022-08-22 19:03:52.876877: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 8413ed100 of size 22528 next 348\n",
      "2022-08-22 19:03:52.876880: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 8413f2900 of size 22528 next 349\n",
      "2022-08-22 19:03:52.876884: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 8413f8100 of size 22528 next 350\n",
      "2022-08-22 19:03:52.876887: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 8413fd900 of size 34133760 next 352\n",
      "2022-08-22 19:03:52.876890: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 84348b000 of size 34133760 next 368\n",
      "2022-08-22 19:03:52.876893: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 845518700 of size 34133760 next 369\n",
      "2022-08-22 19:03:52.876896: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 8475a5e00 of size 34133760 next 366\n",
      "2022-08-22 19:03:52.876899: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 849633500 of size 34133760 next 382\n",
      "2022-08-22 19:03:52.876902: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 84b6c0c00 of size 34133760 next 386\n",
      "2022-08-22 19:03:52.876905: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 84d74e300 of size 34133760 next 397\n",
      "2022-08-22 19:03:52.876918: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 84f7dba00 of size 34133760 next 408\n",
      "2022-08-22 19:03:52.876922: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 851869100 of size 34133760 next 419\n",
      "2022-08-22 19:03:52.876925: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 8538f6800 of size 34133760 next 430\n",
      "2022-08-22 19:03:52.876928: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 855983f00 of size 34133760 next 441\n",
      "2022-08-22 19:03:52.876931: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 857a11600 of size 34133760 next 452\n",
      "2022-08-22 19:03:52.876935: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 859a9ed00 of size 34133760 next 453\n",
      "2022-08-22 19:03:52.876938: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 85bb2c400 of size 34133760 next 454\n",
      "2022-08-22 19:03:52.877105: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 85dbb9b00 of size 34133760 next 455\n",
      "2022-08-22 19:03:52.877108: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 85fc47200 of size 34133760 next 456\n",
      "2022-08-22 19:03:52.877111: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 861cd4900 of size 34133760 next 457\n",
      "2022-08-22 19:03:52.877115: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 863d62000 of size 34133760 next 458\n",
      "2022-08-22 19:03:52.877118: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 865def700 of size 34133760 next 459\n",
      "2022-08-22 19:03:52.877121: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 867e7ce00 of size 34133760 next 460\n",
      "2022-08-22 19:03:52.877124: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 869f0a500 of size 34133760 next 461\n",
      "2022-08-22 19:03:52.877127: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 86bf97c00 of size 34133760 next 462\n",
      "2022-08-22 19:03:52.877131: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 86e025300 of size 34133760 next 463\n",
      "2022-08-22 19:03:52.877134: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 8700b2a00 of size 34133760 next 464\n",
      "2022-08-22 19:03:52.877137: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 872140100 of size 34133760 next 465\n",
      "2022-08-22 19:03:52.877140: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 8741cd800 of size 34133760 next 475\n",
      "2022-08-22 19:03:52.877144: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 87625af00 of size 34133760 next 486\n",
      "2022-08-22 19:03:52.877147: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 8782e8600 of size 34133760 next 497\n",
      "2022-08-22 19:03:52.877150: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 87a375d00 of size 34133760 next 508\n",
      "2022-08-22 19:03:52.877153: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 87c403400 of size 34133760 next 519\n",
      "2022-08-22 19:03:52.877156: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 87e490b00 of size 34133760 next 530\n",
      "2022-08-22 19:03:52.877160: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 88051e200 of size 34133760 next 542\n",
      "2022-08-22 19:03:52.877163: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 8825ab900 of size 34133760 next 616\n",
      "2022-08-22 19:03:52.877166: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 884639000 of size 34133760 next 648\n",
      "2022-08-22 19:03:52.877169: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 8866c6700 of size 34133760 next 606\n",
      "2022-08-22 19:03:52.877172: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 888753e00 of size 34133760 next 583\n",
      "2022-08-22 19:03:52.877176: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 88a7e1500 of size 34133760 next 623\n",
      "2022-08-22 19:03:52.877179: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 88c86ec00 of size 34133760 next 575\n",
      "2022-08-22 19:03:52.877182: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 88e8fc300 of size 34133760 next 657\n",
      "2022-08-22 19:03:52.877185: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 890989a00 of size 34133760 next 668\n",
      "2022-08-22 19:03:52.877189: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 892a17100 of size 34133760 next 679\n",
      "2022-08-22 19:03:52.877192: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 894aa4800 of size 34133760 next 690\n",
      "2022-08-22 19:03:52.877195: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 896b31f00 of size 34133760 next 691\n",
      "2022-08-22 19:03:52.877198: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 898bbf600 of size 34133760 next 692\n",
      "2022-08-22 19:03:52.877202: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 89ac4cd00 of size 34133760 next 693\n",
      "2022-08-22 19:03:52.877373: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 89ccda400 of size 34133760 next 694\n",
      "2022-08-22 19:03:52.877376: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 89ed67b00 of size 34133760 next 695\n",
      "2022-08-22 19:03:52.877379: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 8a0df5200 of size 34133760 next 696\n",
      "2022-08-22 19:03:52.877382: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 8a2e82900 of size 34133760 next 697\n",
      "2022-08-22 19:03:52.877386: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 8a4f10000 of size 34133760 next 698\n",
      "2022-08-22 19:03:52.877389: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 8a6f9d700 of size 34133760 next 699\n",
      "2022-08-22 19:03:52.877392: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 8a902ae00 of size 34133760 next 700\n",
      "2022-08-22 19:03:52.877396: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 8ab0b8500 of size 34133760 next 701\n",
      "2022-08-22 19:03:52.877399: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 8ad145c00 of size 34133760 next 702\n",
      "2022-08-22 19:03:52.877402: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 8af1d3300 of size 34133760 next 703\n",
      "2022-08-22 19:03:52.877405: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 8b1260a00 of size 34133760 next 714\n",
      "2022-08-22 19:03:52.877409: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 8b32ee100 of size 34133760 next 725\n",
      "2022-08-22 19:03:52.877412: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 8b537b800 of size 45697024 next 18446744073709551615\n",
      "2022-08-22 19:03:52.877416: I tensorflow/core/common_runtime/bfc_allocator.cc:1033]      Summary of in-use Chunks by size: \n",
      "2022-08-22 19:03:52.877422: I tensorflow/core/common_runtime/bfc_allocator.cc:1036] 318 Chunks of size 256 totalling 79.5KiB\n",
      "2022-08-22 19:03:52.877427: I tensorflow/core/common_runtime/bfc_allocator.cc:1036] 6 Chunks of size 512 totalling 3.0KiB\n",
      "2022-08-22 19:03:52.877430: I tensorflow/core/common_runtime/bfc_allocator.cc:1036] 7 Chunks of size 1024 totalling 7.0KiB\n",
      "2022-08-22 19:03:52.877434: I tensorflow/core/common_runtime/bfc_allocator.cc:1036] 2 Chunks of size 1280 totalling 2.5KiB\n",
      "2022-08-22 19:03:52.877437: I tensorflow/core/common_runtime/bfc_allocator.cc:1036] 2 Chunks of size 1536 totalling 3.0KiB\n",
      "2022-08-22 19:03:52.877441: I tensorflow/core/common_runtime/bfc_allocator.cc:1036] 2 Chunks of size 1792 totalling 3.5KiB\n",
      "2022-08-22 19:03:52.877444: I tensorflow/core/common_runtime/bfc_allocator.cc:1036] 1 Chunks of size 2560 totalling 2.5KiB\n",
      "2022-08-22 19:03:52.877448: I tensorflow/core/common_runtime/bfc_allocator.cc:1036] 7 Chunks of size 3072 totalling 21.0KiB\n",
      "2022-08-22 19:03:52.877451: I tensorflow/core/common_runtime/bfc_allocator.cc:1036] 1 Chunks of size 4352 totalling 4.2KiB\n",
      "2022-08-22 19:03:52.877455: I tensorflow/core/common_runtime/bfc_allocator.cc:1036] 1 Chunks of size 4864 totalling 4.8KiB\n",
      "2022-08-22 19:03:52.877458: I tensorflow/core/common_runtime/bfc_allocator.cc:1036] 1 Chunks of size 7424 totalling 7.2KiB\n",
      "2022-08-22 19:03:52.877462: I tensorflow/core/common_runtime/bfc_allocator.cc:1036] 2 Chunks of size 7680 totalling 15.0KiB\n",
      "2022-08-22 19:03:52.877466: I tensorflow/core/common_runtime/bfc_allocator.cc:1036] 47 Chunks of size 12800 totalling 587.5KiB\n",
      "2022-08-22 19:03:52.877472: I tensorflow/core/common_runtime/bfc_allocator.cc:1036] 112 Chunks of size 14080 totalling 1.50MiB\n",
      "2022-08-22 19:03:52.877479: I tensorflow/core/common_runtime/bfc_allocator.cc:1036] 2 Chunks of size 16384 totalling 32.0KiB\n",
      "2022-08-22 19:03:52.877484: I tensorflow/core/common_runtime/bfc_allocator.cc:1036] 57 Chunks of size 20480 totalling 1.11MiB\n",
      "2022-08-22 19:03:52.877489: I tensorflow/core/common_runtime/bfc_allocator.cc:1036] 110 Chunks of size 22528 totalling 2.36MiB\n",
      "2022-08-22 19:03:52.877503: I tensorflow/core/common_runtime/bfc_allocator.cc:1036] 1 Chunks of size 25600 totalling 25.0KiB\n",
      "2022-08-22 19:03:52.877770: I tensorflow/core/common_runtime/bfc_allocator.cc:1036] 1 Chunks of size 26880 totalling 26.2KiB\n",
      "2022-08-22 19:03:52.877787: I tensorflow/core/common_runtime/bfc_allocator.cc:1036] 84 Chunks of size 34816 totalling 2.79MiB\n",
      "2022-08-22 19:03:52.877793: I tensorflow/core/common_runtime/bfc_allocator.cc:1036] 1 Chunks of size 35328 totalling 34.5KiB\n",
      "2022-08-22 19:03:52.877797: I tensorflow/core/common_runtime/bfc_allocator.cc:1036] 7 Chunks of size 36864 totalling 252.0KiB\n",
      "2022-08-22 19:03:52.877802: I tensorflow/core/common_runtime/bfc_allocator.cc:1036] 3 Chunks of size 49152 totalling 144.0KiB\n",
      "2022-08-22 19:03:52.877808: I tensorflow/core/common_runtime/bfc_allocator.cc:1036] 1 Chunks of size 57344 totalling 56.0KiB\n",
      "2022-08-22 19:03:52.877812: I tensorflow/core/common_runtime/bfc_allocator.cc:1036] 9 Chunks of size 65536 totalling 576.0KiB\n",
      "2022-08-22 19:03:52.877816: I tensorflow/core/common_runtime/bfc_allocator.cc:1036] 3 Chunks of size 81920 totalling 240.0KiB\n",
      "2022-08-22 19:03:52.877820: I tensorflow/core/common_runtime/bfc_allocator.cc:1036] 1 Chunks of size 90880 totalling 88.8KiB\n",
      "2022-08-22 19:03:52.877823: I tensorflow/core/common_runtime/bfc_allocator.cc:1036] 3 Chunks of size 98304 totalling 288.0KiB\n",
      "2022-08-22 19:03:52.877827: I tensorflow/core/common_runtime/bfc_allocator.cc:1036] 21 Chunks of size 131072 totalling 2.62MiB\n",
      "2022-08-22 19:03:52.877832: I tensorflow/core/common_runtime/bfc_allocator.cc:1036] 1 Chunks of size 258816 totalling 252.8KiB\n",
      "2022-08-22 19:03:52.877839: I tensorflow/core/common_runtime/bfc_allocator.cc:1036] 86 Chunks of size 34133760 totalling 2.73GiB\n",
      "2022-08-22 19:03:52.877846: I tensorflow/core/common_runtime/bfc_allocator.cc:1036] 1 Chunks of size 45697024 totalling 43.58MiB\n",
      "2022-08-22 19:03:52.877853: I tensorflow/core/common_runtime/bfc_allocator.cc:1040] Sum Total of in-use chunks: 2.79GiB\n",
      "2022-08-22 19:03:52.877860: I tensorflow/core/common_runtime/bfc_allocator.cc:1042] total_region_allocated_bytes_: 3021275136 memory_limit_: 3021275136 available bytes: 0 curr_region_allocation_bytes_: 6042550272\n",
      "2022-08-22 19:03:52.877870: I tensorflow/core/common_runtime/bfc_allocator.cc:1048] Stats: \n",
      "Limit:                      3021275136\n",
      "InUse:                      2994921984\n",
      "MaxInUse:                   3000993536\n",
      "NumAllocs:                     2393418\n",
      "MaxAllocSize:                 61423616\n",
      "Reserved:                            0\n",
      "PeakReserved:                        0\n",
      "LargestFreeBlock:                    0\n",
      "\n",
      "2022-08-22 19:03:52.877912: W tensorflow/core/common_runtime/bfc_allocator.cc:441] ****************************************************************************************************\n",
      "2022-08-22 19:03:52.877938: W tensorflow/core/framework/op_kernel.cc:1763] OP_REQUIRES failed at resource_variable_ops.cc:157 : Resource exhausted: OOM when allocating tensor with shape[26667,320] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n"
     ]
    },
    {
     "ename": "ResourceExhaustedError",
     "evalue": "OOM when allocating tensor with shape[26667,320] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc [Op:ReadVariableOp]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_4934/1068071163.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Save TF Hub Sentence Encoder model to HDF5 format\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel_6\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"model_6.h5\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/MinicondaProjects/tensor-flow-cert/env/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, filepath, overwrite, include_optimizer, save_format, signatures, options, save_traces)\u001b[0m\n\u001b[1;32m   2000\u001b[0m     \u001b[0;31m# pylint: enable=line-too-long\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2001\u001b[0m     save.save_model(self, filepath, overwrite, include_optimizer, save_format,\n\u001b[0;32m-> 2002\u001b[0;31m                     signatures, options, save_traces)\n\u001b[0m\u001b[1;32m   2003\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2004\u001b[0m   def save_weights(self,\n",
      "\u001b[0;32m~/MinicondaProjects/tensor-flow-cert/env/lib/python3.7/site-packages/tensorflow/python/keras/saving/save.py\u001b[0m in \u001b[0;36msave_model\u001b[0;34m(model, filepath, overwrite, include_optimizer, save_format, signatures, options, save_traces)\u001b[0m\n\u001b[1;32m    152\u001b[0m           'or using `save_weights`.')\n\u001b[1;32m    153\u001b[0m     hdf5_format.save_model_to_hdf5(\n\u001b[0;32m--> 154\u001b[0;31m         model, filepath, overwrite, include_optimizer)\n\u001b[0m\u001b[1;32m    155\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m     saved_model_save.save(model, filepath, overwrite, include_optimizer,\n",
      "\u001b[0;32m~/MinicondaProjects/tensor-flow-cert/env/lib/python3.7/site-packages/tensorflow/python/keras/saving/hdf5_format.py\u001b[0m in \u001b[0;36msave_model_to_hdf5\u001b[0;34m(model, filepath, overwrite, include_optimizer)\u001b[0m\n\u001b[1;32m    123\u001b[0m     \u001b[0mmodel_weights_group\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_group\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'model_weights'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m     \u001b[0mmodel_layers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 125\u001b[0;31m     \u001b[0msave_weights_to_hdf5_group\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_weights_group\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_layers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m     \u001b[0;31m# TODO(b/128683857): Add integration tests between tf.keras and external\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/MinicondaProjects/tensor-flow-cert/env/lib/python3.7/site-packages/tensorflow/python/keras/saving/hdf5_format.py\u001b[0m in \u001b[0;36msave_weights_to_hdf5_group\u001b[0;34m(f, layers)\u001b[0m\n\u001b[1;32m    636\u001b[0m     \u001b[0mg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_group\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    637\u001b[0m     \u001b[0mweights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_legacy_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 638\u001b[0;31m     \u001b[0mweight_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_get_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    639\u001b[0m     \u001b[0mweight_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'utf8'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    640\u001b[0m     \u001b[0msave_attributes_to_hdf5_group\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'weight_names'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight_names\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/MinicondaProjects/tensor-flow-cert/env/lib/python3.7/site-packages/tensorflow/python/util/dispatch.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    199\u001b[0m     \u001b[0;34m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 201\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    202\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m       \u001b[0;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/MinicondaProjects/tensor-flow-cert/env/lib/python3.7/site-packages/tensorflow/python/keras/backend.py\u001b[0m in \u001b[0;36mbatch_get_value\u001b[0;34m(tensors)\u001b[0m\n\u001b[1;32m   3644\u001b[0m   \"\"\"\n\u001b[1;32m   3645\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3646\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtensors\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3647\u001b[0m   \u001b[0;32melif\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minside_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3648\u001b[0m     \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Cannot get value inside Tensorflow graph function.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/MinicondaProjects/tensor-flow-cert/env/lib/python3.7/site-packages/tensorflow/python/keras/backend.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   3644\u001b[0m   \"\"\"\n\u001b[1;32m   3645\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3646\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtensors\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3647\u001b[0m   \u001b[0;32melif\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minside_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3648\u001b[0m     \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Cannot get value inside Tensorflow graph function.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/MinicondaProjects/tensor-flow-cert/env/lib/python3.7/site-packages/tensorflow/python/ops/resource_variable_ops.py\u001b[0m in \u001b[0;36mnumpy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    616\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    617\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 618\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    619\u001b[0m     raise NotImplementedError(\n\u001b[1;32m    620\u001b[0m         \"numpy() is only available when eager execution is enabled.\")\n",
      "\u001b[0;32m~/MinicondaProjects/tensor-flow-cert/env/lib/python3.7/site-packages/tensorflow/python/ops/resource_variable_ops.py\u001b[0m in \u001b[0;36mread_value\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    691\u001b[0m     \"\"\"\n\u001b[1;32m    692\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Read\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 693\u001b[0;31m       \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_read_variable_op\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    694\u001b[0m     \u001b[0;31m# Return an identity so it can get placed on whatever device the context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    695\u001b[0m     \u001b[0;31m# specifies instead of the device where the variable is.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/MinicondaProjects/tensor-flow-cert/env/lib/python3.7/site-packages/tensorflow/python/ops/resource_variable_ops.py\u001b[0m in \u001b[0;36m_read_variable_op\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    670\u001b[0m           \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mread_and_set_handle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    671\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 672\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mread_and_set_handle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    673\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    674\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/MinicondaProjects/tensor-flow-cert/env/lib/python3.7/site-packages/tensorflow/python/ops/resource_variable_ops.py\u001b[0m in \u001b[0;36mread_and_set_handle\u001b[0;34m()\u001b[0m\n\u001b[1;32m    661\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mread_and_set_handle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    662\u001b[0m       result = gen_resource_variable_ops.read_variable_op(\n\u001b[0;32m--> 663\u001b[0;31m           self._handle, self._dtype)\n\u001b[0m\u001b[1;32m    664\u001b[0m       \u001b[0m_maybe_set_handle_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    665\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/MinicondaProjects/tensor-flow-cert/env/lib/python3.7/site-packages/tensorflow/python/ops/gen_resource_variable_ops.py\u001b[0m in \u001b[0;36mread_variable_op\u001b[0;34m(resource, dtype, name)\u001b[0m\n\u001b[1;32m    472\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    473\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 474\u001b[0;31m       \u001b[0m_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from_not_ok_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    475\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_FallbackException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m       \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/MinicondaProjects/tensor-flow-cert/env/lib/python3.7/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mraise_from_not_ok_status\u001b[0;34m(e, name)\u001b[0m\n\u001b[1;32m   6860\u001b[0m   \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\" name: \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6861\u001b[0m   \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6862\u001b[0;31m   \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6863\u001b[0m   \u001b[0;31m# pylint: enable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6864\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/MinicondaProjects/tensor-flow-cert/env/lib/python3.7/site-packages/six.py\u001b[0m in \u001b[0;36mraise_from\u001b[0;34m(value, from_value)\u001b[0m\n",
      "\u001b[0;31mResourceExhaustedError\u001b[0m: OOM when allocating tensor with shape[26667,320] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc [Op:ReadVariableOp]"
     ]
    }
   ],
   "source": [
    "# KAS - My machine craps out with an out of memory error\n",
    "\n",
    "# Save TF Hub Sentence Encoder model to HDF5 format\n",
    "model_6.save(\"model_6.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Cp6zvmprm9A3"
   },
   "source": [
    "If you save a model as a `HDF5`, when loading it back in, you need to let [TensorFlow know about any custom objects you've used](https://www.tensorflow.org/tutorials/keras/save_and_load#saving_custom_objects) (e.g. components which aren't built from pure TensorFlow, such as TensorFlow Hub components)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sSINZ0Q-nRb2"
   },
   "outputs": [],
   "source": [
    "# Load model with custom Hub Layer (required with HDF5 format)\n",
    "loaded_model_6 = tf.keras.models.load_model(\"model_6.h5\", \n",
    "                                            custom_objects={\"KerasLayer\": hub.KerasLayer})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "G4BCJ8iXnZ4r",
    "outputId": "e53af7df-411d-4a91-f1f3-194d4dd16891"
   },
   "outputs": [],
   "source": [
    "# How does our loaded model perform?\n",
    "loaded_model_6.evaluate(val_sentences, val_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "02rbT4fwn0It"
   },
   "source": [
    "Calling the `save()` method on our target model and passing it a filepath allows us to save our model in the `SavedModel` format. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e3eVaNBDoMsv",
    "outputId": "8800d005-ac0c-40e0-8d58-9bc220dd1989"
   },
   "outputs": [],
   "source": [
    "# Save TF Hub Sentence Encoder model to SavedModel format (default)\n",
    "model_6.save(\"model_6_SavedModel_format\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l-t01S-JoOqK"
   },
   "source": [
    "If you use SavedModel format (default), you can reload your model without specifying custom objects using the [`tensorflow.keras.models.load_model()`](https://www.tensorflow.org/tutorials/keras/save_and_load) function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Dw3zf4fVoU5H"
   },
   "outputs": [],
   "source": [
    "# Load TF Hub Sentence Encoder SavedModel\n",
    "loaded_model_6_SavedModel = tf.keras.models.load_model(\"model_6_SavedModel_format\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IqiPr6iiofi1",
    "outputId": "ed6ece1e-a8d1-4622-e24c-4841d1e2879f"
   },
   "outputs": [],
   "source": [
    "# Evaluate loaded SavedModel format\n",
    "loaded_model_6_SavedModel.evaluate(val_sentences, val_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xzp3SHi3oQ3u"
   },
   "source": [
    "As you can see saving and loading our model with either format results in the same performance.\n",
    "\n",
    "> 🤔 **Question:** Should you used the `SavedModel` format or `HDF5` format?\n",
    "\n",
    "For most use cases, the `SavedModel` format will suffice. However, this is a TensorFlow specific standard. If you need a more general-purpose data standard, `HDF5` might be better. For more, check out the [TensorFlow documentation on saving and loading models](https://www.tensorflow.org/tutorials/keras/save_and_load)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V5a1648rG3z1"
   },
   "source": [
    "## Finding the most wrong examples\n",
    "\n",
    "We mentioned before that if many of our modelling experiments are returning similar results, despite using different kinds of models, it's a good idea to return to the data and inspect why this might be.\n",
    "\n",
    "One of the best ways to inspect your data is to sort your model's predictions and find the samples it got *most* wrong, meaning, what predictions had a high prediction probability but turned out to be wrong.\n",
    "\n",
    "Once again, visualization is your friend. Visualize, visualize, visualize.\n",
    "\n",
    "To make things visual, let's take our best performing model's prediction probabilities and classes along with the validation samples (text and ground truth labels) and combine them in a pandas DataFrame.\n",
    "\n",
    "* If our best model still isn't perfect, what examples is it getting wrong? \n",
    "* Which ones are the *most* wrong?\n",
    "* Are there some labels which are wrong? E.g. the model gets it right but the ground truth label doesn't reflect this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "gnHfX--TwMIW",
    "outputId": "8f2f92e4-da85-40f9-8542-92f31c02f71f"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "      <th>pred</th>\n",
       "      <th>pred_prob</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>DFR EP016 Monthly Meltdown - On Dnbheaven 2015...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.025590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>FedEx no longer to transport bioterror germs i...</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.785016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Gunmen kill four in El Salvador bus attack: Su...</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.999848</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@camilacabello97 Internally and externally scr...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.145094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Radiation emergency #preparedness starts with ...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.431277</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  target  pred  pred_prob\n",
       "0  DFR EP016 Monthly Meltdown - On Dnbheaven 2015...       0   0.0   0.025590\n",
       "1  FedEx no longer to transport bioterror germs i...       0   1.0   0.785016\n",
       "2  Gunmen kill four in El Salvador bus attack: Su...       1   1.0   0.999848\n",
       "3  @camilacabello97 Internally and externally scr...       1   0.0   0.145094\n",
       "4  Radiation emergency #preparedness starts with ...       1   0.0   0.431277"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create dataframe with validation sentences and best performing model predictions\n",
    "val_df = pd.DataFrame({\"text\": val_sentences,\n",
    "                       \"target\": val_labels,\n",
    "                       \"pred\": model_6_preds,\n",
    "                       \"pred_prob\": tf.squeeze(model_6_pred_probs)})\n",
    "val_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SKJ9dTbPrIG4"
   },
   "source": [
    "Oh yeah! Now let's find our model's wrong predictions (where `target != pred`) and sort them by their prediction probability (the `pred_prob` column)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 363
    },
    "id": "0DwBXQS1wvZx",
    "outputId": "109998cb-ba76-4e7d-81df-dd692e1bb77c"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "      <th>pred</th>\n",
       "      <th>pred_prob</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>[55436] 1950 LIONEL TRAINS SMOKE LOCOMOTIVES W...</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.995954</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>628</th>\n",
       "      <td>@noah_anyname That's where the concentration c...</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.993536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>381</th>\n",
       "      <td>Deaths 3 http://t.co/nApviyGKYK</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.989708</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>393</th>\n",
       "      <td>@SonofLiberty357 all illuminated by the bright...</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.983600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>344</th>\n",
       "      <td>Air Group is here to the rescue! We have 24/7 ...</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.976571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>303</th>\n",
       "      <td>Trafford Centre film fans angry after Odeon ci...</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.972327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>588</th>\n",
       "      <td>.@AIGinsurance CEO: Divestitures and #Catastro...</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.964662</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>209</th>\n",
       "      <td>Ashes 2015: AustraliaÛªs collapse at Trent Br...</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.961885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>698</th>\n",
       "      <td>åÈMGN-AFRICAå¨ pin:263789F4 åÈ Correction: Ten...</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.949379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>The Five Fatal Flaws in the Iran Deal https://...</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.928982</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  text  target  pred  \\\n",
       "109  [55436] 1950 LIONEL TRAINS SMOKE LOCOMOTIVES W...       0   1.0   \n",
       "628  @noah_anyname That's where the concentration c...       0   1.0   \n",
       "381                    Deaths 3 http://t.co/nApviyGKYK       0   1.0   \n",
       "393  @SonofLiberty357 all illuminated by the bright...       0   1.0   \n",
       "344  Air Group is here to the rescue! We have 24/7 ...       0   1.0   \n",
       "303  Trafford Centre film fans angry after Odeon ci...       0   1.0   \n",
       "588  .@AIGinsurance CEO: Divestitures and #Catastro...       0   1.0   \n",
       "209  Ashes 2015: AustraliaÛªs collapse at Trent Br...       0   1.0   \n",
       "698  åÈMGN-AFRICAå¨ pin:263789F4 åÈ Correction: Ten...       0   1.0   \n",
       "11   The Five Fatal Flaws in the Iran Deal https://...       0   1.0   \n",
       "\n",
       "     pred_prob  \n",
       "109   0.995954  \n",
       "628   0.993536  \n",
       "381   0.989708  \n",
       "393   0.983600  \n",
       "344   0.976571  \n",
       "303   0.972327  \n",
       "588   0.964662  \n",
       "209   0.961885  \n",
       "698   0.949379  \n",
       "11    0.928982  "
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find the wrong predictions and sort by prediction probabilities\n",
    "most_wrong = val_df[val_df[\"target\"] != val_df[\"pred\"]].sort_values(\"pred_prob\", ascending=False)\n",
    "most_wrong[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r3VcRHOusB2D"
   },
   "source": [
    "Finally, we can write some code to visualize the sample text, truth label, prediction class and prediction probability. Because we've sorted our samples by prediction probability, viewing samples from the head of our `most_wrong` DataFrame will show us false positives.\n",
    "\n",
    "A reminder:\n",
    "* `0` = Not a real diaster Tweet\n",
    "* `1` = Real diaster Tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xLFYDEsoxRFP",
    "outputId": "933353bc-7ea7-4186-ccdf-19e97364f012"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target: 0, Pred: 1, Prob: 0.9959539175033569\n",
      "Text:\n",
      "[55436] 1950 LIONEL TRAINS SMOKE LOCOMOTIVES WITH MAGNE-TRACTION INSTRUCTIONS http://t.co/xEZBs3sq0y http://t.co/C2x0QoKGlY\n",
      "\n",
      "----\n",
      "\n",
      "Target: 0, Pred: 1, Prob: 0.9935357570648193\n",
      "Text:\n",
      "@noah_anyname That's where the concentration camps and mass murder come in. \n",
      " \n",
      "EVERY. FUCKING. TIME.\n",
      "\n",
      "----\n",
      "\n",
      "Target: 0, Pred: 1, Prob: 0.9897081851959229\n",
      "Text:\n",
      "Deaths 3 http://t.co/nApviyGKYK\n",
      "\n",
      "----\n",
      "\n",
      "Target: 0, Pred: 1, Prob: 0.9836002588272095\n",
      "Text:\n",
      "@SonofLiberty357 all illuminated by the brightly burning buildings all around the town!\n",
      "\n",
      "----\n",
      "\n",
      "Target: 0, Pred: 1, Prob: 0.9765712022781372\n",
      "Text:\n",
      "Air Group is here to the rescue! We have 24/7 Emergency Service! Learn more about it here - http://t.co/9lyx7zMtHE http://t.co/5PbC96rTMJ\n",
      "\n",
      "----\n",
      "\n",
      "Target: 0, Pred: 1, Prob: 0.9723268747329712\n",
      "Text:\n",
      "Trafford Centre film fans angry after Odeon cinema evacuated following false fire alarm   http://t.co/6GLDwx71DA\n",
      "\n",
      "----\n",
      "\n",
      "Target: 0, Pred: 1, Prob: 0.9646617770195007\n",
      "Text:\n",
      ".@AIGinsurance CEO: Divestitures and #Catastrophe Losses Temper Q2 #Results http://t.co/2y2wZk1FrM\n",
      "\n",
      "----\n",
      "\n",
      "Target: 0, Pred: 1, Prob: 0.9618851542472839\n",
      "Text:\n",
      "Ashes 2015: AustraliaÛªs collapse at Trent Bridge among worst in history: England bundled out Australia for 60 ... http://t.co/t5TrhjUAU0\n",
      "\n",
      "----\n",
      "\n",
      "Target: 0, Pred: 1, Prob: 0.9493793845176697\n",
      "Text:\n",
      "åÈMGN-AFRICAå¨ pin:263789F4 åÈ Correction: Tent Collapse Story: Correction: Tent Collapse story åÈ http://t.co/fDJUYvZMrv @wizkidayo\n",
      "\n",
      "----\n",
      "\n",
      "Target: 0, Pred: 1, Prob: 0.9289823770523071\n",
      "Text:\n",
      "The Five Fatal Flaws in the Iran Deal https://t.co/ztfEAd8GId via @YouTube\n",
      "\n",
      "----\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Check the false positives (model predicted 1 when should've been 0)\n",
    "for row in most_wrong[:10].itertuples(): # loop through the top 10 rows (change the index to view different rows)\n",
    "  _, text, target, pred, prob = row\n",
    "  print(f\"Target: {target}, Pred: {int(pred)}, Prob: {prob}\")\n",
    "  print(f\"Text:\\n{text}\\n\")\n",
    "  print(\"----\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aXCH9J-UspWg"
   },
   "source": [
    "We can view the bottom end of our `most_wrong` DataFrame to inspect false negatives (model predicts 0, not a real diaster Tweet, when it should've predicted 1, real diaster Tweet)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6EaMchehxwLq",
    "outputId": "3d442c41-52c2-44c6-8dae-45787bc11f3b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target: 1, Pred: 0, Prob: 0.006319171283394098\n",
      "Text:\n",
      "Perspectives on the Grateful Dead: Critical Writings (Contributions to the Study http://t.co/fmu0fnuMxf http://t.co/AgGRyhVXKr\n",
      "\n",
      "----\n",
      "\n",
      "Target: 1, Pred: 0, Prob: 0.0061522251926362514\n",
      "Text:\n",
      "Ron &amp; Fez - Dave's High School Crush https://t.co/aN3W16c8F6 via @YouTube\n",
      "\n",
      "----\n",
      "\n",
      "Target: 1, Pred: 0, Prob: 0.0051508075557649136\n",
      "Text:\n",
      "Just came back from camping and returned with a new song which gets recorded tomorrow. Can't wait! #Desolation #TheConspiracyTheory #NewEP\n",
      "\n",
      "----\n",
      "\n",
      "Target: 1, Pred: 0, Prob: 0.004923267289996147\n",
      "Text:\n",
      "Next May I'll be free...from school from obligations like family.... Best of all that damn curfew...\n",
      "\n",
      "----\n",
      "\n",
      "Target: 1, Pred: 0, Prob: 0.004761665128171444\n",
      "Text:\n",
      "Reddit Will Now QuarantineÛ_ http://t.co/pkUAMXw6pm #onlinecommunities #reddit #amageddon #freespeech #Business http://t.co/PAWvNJ4sAP\n",
      "\n",
      "----\n",
      "\n",
      "Target: 1, Pred: 0, Prob: 0.004262831062078476\n",
      "Text:\n",
      "I get to smoke my shit in peace\n",
      "\n",
      "----\n",
      "\n",
      "Target: 1, Pred: 0, Prob: 0.003737614257261157\n",
      "Text:\n",
      "going to redo my nails and watch behind the scenes of desolation of smaug ayyy\n",
      "\n",
      "----\n",
      "\n",
      "Target: 1, Pred: 0, Prob: 0.003465235698968172\n",
      "Text:\n",
      "Petition | Heartless owner that whipped horse until it collapsed is told he can KEEP his animal! Act Now! http://t.co/87eFCBIczM\n",
      "\n",
      "----\n",
      "\n",
      "Target: 1, Pred: 0, Prob: 0.003290836000815034\n",
      "Text:\n",
      "Why are you deluged with low self-image? Take the quiz: http://t.co/XsPqdOrIqj http://t.co/CQYvFR4UCy\n",
      "\n",
      "----\n",
      "\n",
      "Target: 1, Pred: 0, Prob: 0.0012757411459460855\n",
      "Text:\n",
      "You can never escape me. Bullets don't harm me. Nothing harms me. But I know pain. I know pain. Sometimes I share it. With someone like you.\n",
      "\n",
      "----\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Check the most wrong false negatives (model predicted 0 when should've predict 1)\n",
    "for row in most_wrong[-10:].itertuples():\n",
    "  _, text, target, pred, prob = row\n",
    "  print(f\"Target: {target}, Pred: {int(pred)}, Prob: {prob}\")\n",
    "  print(f\"Text:\\n{text}\\n\")\n",
    "  print(\"----\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lRKQPEAgtpJq"
   },
   "source": [
    "Do you notice anything interesting about the most wrong samples?\n",
    "\n",
    "Are the ground truth labels correct? What do you think would happen if we went back and corrected the labels which aren't?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U0W3DWgWJCWs"
   },
   "source": [
    "## Making predictions on the test dataset\n",
    "\n",
    "Alright we've seen how our model's perform on the validation set.\n",
    "\n",
    "But how about the test dataset?\n",
    "\n",
    "We don't have labels for the test dataset so we're going to have to make some predictions and inspect them for ourselves.\n",
    "\n",
    "Let's write some code to make predictions on random samples from the test dataset and visualize them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6Q9lgqoDyequ",
    "outputId": "eea120ec-7270-4c50-d446-10493de658c5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pred: 1, Prob: 0.9990453124046326\n",
      "Text:\n",
      "The Rocky Fire burning in CA still threatening more than 7k buildings. Largest in state &amp; one of nearly 2 dozen burning now #9newsmornings\n",
      "\n",
      "----\n",
      "\n",
      "Pred: 0, Prob: 0.21958331763744354\n",
      "Text:\n",
      "Next up: a charity program for disadvantaged people displaced by the tech boom in SF based on Tinder technology.  https://t.co/yNVz1WrgtN\n",
      "\n",
      "----\n",
      "\n",
      "Pred: 0, Prob: 0.09113992750644684\n",
      "Text:\n",
      "Double ebony attack http://t.co/33V0RLlrKf #Black-haired #Blowjob http://t.co/t7TG3nRBje\n",
      "\n",
      "----\n",
      "\n",
      "Pred: 0, Prob: 0.2507534325122833\n",
      "Text:\n",
      "Please donate and spread the word! A training accident left the pole-vaulter Kira GrÌ_nberg a paraplegic http://t.co/6MpnyCl8PK\n",
      "\n",
      "----\n",
      "\n",
      "Pred: 1, Prob: 0.9937978386878967\n",
      "Text:\n",
      "Madhya Pradesh Train Derailment: Village Youth Saved Many Lives: A group of villagers saved over 70 passengers' lives after two train...\n",
      "\n",
      "----\n",
      "\n",
      "Pred: 0, Prob: 0.4444139301776886\n",
      "Text:\n",
      "FedEx no longer to transport bioterror germs in wake of anthrax lab mishaps\n",
      "\n",
      "http://t.co/SjNKpJ8lEe\n",
      "\n",
      "#watchcbs19 http://t.co/JiRXfok46c\n",
      "\n",
      "----\n",
      "\n",
      "Pred: 1, Prob: 0.9929381012916565\n",
      "Text:\n",
      "Green Line derailment in Chicago http://t.co/UtbXLcBIuY\n",
      "\n",
      "----\n",
      "\n",
      "Pred: 0, Prob: 0.11293720453977585\n",
      "Text:\n",
      "The bartender at work described a drunk man as annihilated @kdunning1919 @hsnowberger @gabrielasmith29. 16 more days\n",
      "\n",
      "----\n",
      "\n",
      "Pred: 1, Prob: 0.6325808763504028\n",
      "Text:\n",
      "I swear theres a bud drought\n",
      "\n",
      "----\n",
      "\n",
      "Pred: 0, Prob: 0.029433507472276688\n",
      "Text:\n",
      "Devastated. Already missing my @coleenlisa.  https://t.co/3p6Xakt7rh\n",
      "\n",
      "----\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Making predictions on the test dataset\n",
    "test_sentences = test_df[\"text\"].to_list()\n",
    "test_samples = random.sample(test_sentences, 10)\n",
    "for test_sample in test_samples:\n",
    "  pred_prob = tf.squeeze(model_6.predict([test_sample])) # has to be list\n",
    "  pred = tf.round(pred_prob)\n",
    "  print(f\"Pred: {int(pred)}, Prob: {pred_prob}\")\n",
    "  print(f\"Text:\\n{test_sample}\\n\")\n",
    "  print(\"----\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QcvI5zgJ0Tgp"
   },
   "source": [
    "How do our model's predictions look on the test dataset?\n",
    "\n",
    "It's important to do these kind of visualization checks as often as possible to get a glance of how your model performs on unseen data and subsequently how it might perform on the real test: Tweets from the wild."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eT1jhk8xdod5"
   },
   "source": [
    "## Predicting on Tweets from the wild\n",
    "\n",
    "How about we find some Tweets and use our model to predict whether or not they're about a diaster or not?\n",
    "\n",
    "To start, let's take one of my own [Tweets on living life like an ensemble model](https://twitter.com/mrdbourke/status/1313649328351662082). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "id": "qHmXxuPH0aUB"
   },
   "outputs": [],
   "source": [
    "# Turn Tweet into string\n",
    "daniels_tweet = \"Life like an ensemble: take the best choices from others and make your own\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uPbZaGznvbEx"
   },
   "source": [
    "Now we'll write a small function to take a model and an example sentence and return a prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "id": "KyH9tn9upjld"
   },
   "outputs": [],
   "source": [
    "def predict_on_sentence(model, sentence):\n",
    "  \"\"\"\n",
    "  Uses model to make a prediction on sentence.\n",
    "\n",
    "  Returns the sentence, the predicted label and the prediction probability.\n",
    "  \"\"\"\n",
    "  pred_prob = model.predict([sentence])\n",
    "  pred_label = tf.squeeze(tf.round(pred_prob)).numpy()\n",
    "  print(f\"Pred: {pred_label}\", \"(real disaster)\" if pred_label > 0 else \"(not real disaster)\", f\"Prob: {pred_prob[0][0]}\")\n",
    "  print(f\"Text:\\n{sentence}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IvCG4RuUvj6d"
   },
   "source": [
    "Great! Time to test our model out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BxONpJV8qmWP",
    "outputId": "27af1032-c696-4a1d-a7a6-d976fc5b99df"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pred: 0.0 (not real disaster) Prob: 0.09857428818941116\n",
      "Text:\n",
      "Life like an ensemble: take the best choices from others and make your own\n"
     ]
    }
   ],
   "source": [
    "# Make a prediction on Tweet from the wild\n",
    "predict_on_sentence(model=model_6, # use the USE model\n",
    "                    sentence=daniels_tweet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tYOfNacw08Of"
   },
   "source": [
    "Woohoo! Our model predicted correctly. My Tweet wasn't about a diaster.\n",
    "\n",
    "How about we find a few Tweets about actual diasters?\n",
    "\n",
    "Such as the following two Tweets about the 2020 Beirut explosions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "id": "AqILBsTK2i9R"
   },
   "outputs": [],
   "source": [
    "# Source - https://twitter.com/BeirutCityGuide/status/1290696551376007168\n",
    "beirut_tweet_1 = \"Reports that the smoke in Beirut sky contains nitric acid, which is toxic. Please share and refrain from stepping outside unless urgent. #Lebanon\"\n",
    "\n",
    "# Source - https://twitter.com/BeirutCityGuide/status/1290773498743476224\n",
    "beirut_tweet_2 = \"#Beirut declared a “devastated city”, two-week state of emergency officially declared. #Lebanon\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FvlbHDISrVmX",
    "outputId": "7917cddc-afbd-4c29-d6f9-ff4ff87c371f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pred: 1.0 (real disaster) Prob: 0.997285008430481\n",
      "Text:\n",
      "Reports that the smoke in Beirut sky contains nitric acid, which is toxic. Please share and refrain from stepping outside unless urgent. #Lebanon\n"
     ]
    }
   ],
   "source": [
    "# Predict on diaster Tweet 1\n",
    "predict_on_sentence(model=model_6, \n",
    "                    sentence=beirut_tweet_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5uKYx11p2zCd",
    "outputId": "ce4ed56c-e0c7-45bf-f546-1b1787cc34c5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pred: 1.0 (real disaster) Prob: 0.9997084736824036\n",
      "Text:\n",
      "#Beirut declared a “devastated city”, two-week state of emergency officially declared. #Lebanon\n"
     ]
    }
   ],
   "source": [
    "# Predict on diaster Tweet 2\n",
    "predict_on_sentence(model=model_6, \n",
    "                    sentence=beirut_tweet_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fczP1dFcwe98"
   },
   "source": [
    "Looks like our model is performing as expected, predicting both of the diaster Tweets as actual diasters.\n",
    "\n",
    "> 🔑 **Note:** The above examples are cherry-picked and are cases where you'd expect a model to function at high performance. For actual production systems, you'll want to continaully perform tests to see how your model is performing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Fp0fkK-tHPRE"
   },
   "source": [
    "## The speed/score tradeoff\n",
    "\n",
    "One of the final tests we're going to do is to find the speed/score tradeoffs between our best model and baseline model.\n",
    "\n",
    "Why is this important?\n",
    "\n",
    "Although it can be tempting to just choose the best performing model you find through experimentation, this model might not actually work in a production setting.\n",
    "\n",
    "Put it this way, imagine you're Twitter and receive 1 million Tweets per hour (this is a made up number, the actual number is much higher). And you're trying to build a diaster detection system to read Tweets and alert authorities with details about a diaster in close to real-time.\n",
    "\n",
    "Compute power isn't free so you're limited to a single compute machine for the project. On that machine, one of your models makes 10,000 predictions per second at 80% accuracy where as another one of your models (a larger model) makes 100 predictions per second at 85% accuracy.\n",
    "\n",
    "Which model do you choose?\n",
    "\n",
    "Is the second model's performance boost worth missing out on the extra capacity?\n",
    "\n",
    "Of course, there are many options you could try here, such as sending as many Tweets as possible to the first model and then sending the ones which the model is least certain of to the second model. \n",
    "\n",
    "The point here is to illustrate the best model you find through experimentation, might not be the model you end up using in production.\n",
    "\n",
    "To make this more concrete, let's write a function to take a model and a number of samples and time how long the given model takes to make predictions on those samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "id": "DnXp8DKOp3J6"
   },
   "outputs": [],
   "source": [
    "# Calculate the time of predictions\n",
    "import time\n",
    "def pred_timer(model, samples):\n",
    "  \"\"\"\n",
    "  Times how long a model takes to make predictions on samples.\n",
    "  \n",
    "  Args:\n",
    "  ----\n",
    "  model = a trained model\n",
    "  sample = a list of samples\n",
    "\n",
    "  Returns:\n",
    "  ----\n",
    "  total_time = total elapsed time for model to make predictions on samples\n",
    "  time_per_pred = time in seconds per single sample\n",
    "  \"\"\"\n",
    "  start_time = time.perf_counter() # get start time\n",
    "  model.predict(samples) # make predictions\n",
    "  end_time = time.perf_counter() # get finish time\n",
    "  total_time = end_time-start_time # calculate how long predictions took to make\n",
    "  time_per_pred = total_time/len(val_sentences) # find prediction time per sample\n",
    "  return total_time, time_per_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GxWwS73hze6Z"
   },
   "source": [
    "Looking good!\n",
    "\n",
    "Now let's use our `pred_timer()` function to evaluate the prediction times of our best performing model (`model_6`) and our baseline model (`model_0`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JMbGMIWd5c9N",
    "outputId": "6e8cc789-c739-4d51-8522-7697b9820306"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.17930469699967944, 0.00023530800131191528)"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate TF Hub Sentence Encoder prediction times\n",
    "model_6_total_pred_time, model_6_time_per_pred = pred_timer(model_6, val_sentences)\n",
    "model_6_total_pred_time, model_6_time_per_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "I4ej2VyT5oQs",
    "outputId": "5091253d-dad0-4136-9cc5-46037886f591"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.013782846999674803, 1.808772572135801e-05)"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate Naive Bayes prediction times\n",
    "baseline_total_pred_time, baseline_time_per_pred = pred_timer(model_0, val_sentences)\n",
    "baseline_total_pred_time, baseline_time_per_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nqNnKMxhz8Kl"
   },
   "source": [
    "It seems with our current hardware (in my case, I'm using a Google Colab notebook) our best performing model takes over 10x the time to make predictions as our baseline model.\n",
    "\n",
    "Is that extra prediction time worth it?\n",
    "\n",
    "Let's compare time per prediction versus our model's F1-scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 458
    },
    "id": "ANKHEfRN7Nhd",
    "outputId": "b80e077a-3fd8-436f-f245-be18414c1d32"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x504 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(10, 7))\n",
    "plt.scatter(baseline_time_per_pred, baseline_results[\"f1\"], label=\"baseline\")\n",
    "plt.scatter(model_6_time_per_pred, model_6_results[\"f1\"], label=\"tf_hub_sentence_encoder\")\n",
    "plt.legend()\n",
    "plt.title(\"F1-score versus time per prediction\")\n",
    "plt.xlabel(\"Time per prediction\")\n",
    "plt.ylabel(\"F1-Score\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QlHdTqTl0aOq"
   },
   "source": [
    "![](https://raw.githubusercontent.com/mrdbourke/tensorflow-deep-learning/main/images/08-ideal-performance-speed-of-pred-tradeoff-highlighted.png)\n",
    "*Ideal position for speed and performance tradeoff model (fast predictions with great results).*\n",
    "\n",
    "Of course, the ideal position for each of these dots is to be in the top left of the plot (low time per prediction, high F1-score). \n",
    "\n",
    "In our case, there's a clear tradeoff for time per prediction and performance. Our best performing model takes an order of magnitude longer per prediction but only results in a few F1-score point increase.\n",
    "\n",
    "This kind of tradeoff is something you'll need to keep in mind when incorporating machine learning models into your own applications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DJWGI6GpH4Gl"
   },
   "source": [
    "## 🛠 Exercises\n",
    "\n",
    "1. Rebuild, compile and train `model_1`, `model_2` and `model_5` using the [Keras Sequential API](https://www.tensorflow.org/api_docs/python/tf/keras/Sequential) instead of the Functional API.\n",
    "2. Retrain the baseline model with 10% of the training data. How does perform compared to the Universal Sentence Encoder model with 10% of the training data?\n",
    "3. Try fine-tuning the TF Hub Universal Sentence Encoder model by setting `training=True` when instantiating it as a Keras layer.\n",
    "\n",
    "```\n",
    "We can use this encoding layer in place of our text_vectorizer and embedding layer\n",
    "\n",
    "sentence_encoder_layer = hub.KerasLayer(\"https://tfhub.dev/google/universal-sentence-encoder/4\",\n",
    "                                        input_shape=[],\n",
    "                                        dtype=tf.string,\n",
    "                                        trainable=True) # turn training on to fine-tune the TensorFlow Hub model\n",
    "```\n",
    "4. Retrain the best model you've got so far on the whole training set (no validation split). Then use this trained model to make predictions on the test dataset and format the predictions into the same format as the `sample_submission.csv` file from Kaggle (see the Files tab in Colab for what the `sample_submission.csv` file looks like). Once you've done this, [make a submission to the Kaggle competition](https://www.kaggle.com/c/nlp-getting-started/data), how did your model perform?\n",
    "5. Combine the ensemble predictions using the majority vote (mode), how does this perform compare to averaging the prediction probabilities of each model?\n",
    "6. Make a confusion matrix with the best performing model's predictions on the validation set and the validation ground truth labels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BarVJji8H6M4"
   },
   "source": [
    "## 📖 Extra-curriculum \n",
    "\n",
    "To practice what you've learned, a good idea would be to spend an hour on 3 of the following (3-hours total, you could through them all if you want) and then write a blog post about what you've learned.\n",
    "\n",
    "* For an overview of the different problems within NLP and how to solve them read through: \n",
    " * [A Simple Introduction to Natural Language Processing](https://becominghuman.ai/a-simple-introduction-to-natural-language-processing-ea66a1747b32)\n",
    " * [How to solve 90% of NLP problems: a step-by-step guide](https://blog.insightdatascience.com/how-to-solve-90-of-nlp-problems-a-step-by-step-guide-fda605278e4e)\n",
    "* Go through [MIT's Recurrent Neural Networks lecture](https://youtu.be/SEnXr6v2ifU). This will be one of the greatest additions to what's happening behind the RNN model's you've been building.\n",
    "* Read through the [word embeddings page on the TensorFlow website](https://www.tensorflow.org/tutorials/text/word_embeddings). Embeddings are such a large part of NLP. We've covered them throughout this notebook but extra practice would be well worth it. A good exercise would be to write out all the code in the guide in a new notebook. \n",
    "* For more on RNN's in TensorFlow, read and reproduce [the TensorFlow RNN guide](https://www.tensorflow.org/guide/keras/rnn). We've covered many of the concepts in this guide, but it's worth writing the code again for yourself.\n",
    "* Text data doesn't always come in a nice package like the data we've downloaded. So if you're after more on preparing different text sources for being with your TensorFlow deep learning models, it's worth checking out the following:\n",
    " * [TensorFlow text loading tutorial](https://www.tensorflow.org/tutorials/load_data/text).\n",
    "  * [Reading text files with Python](https://realpython.com/read-write-files-python/) by Real Python.\n",
    "* This notebook has focused on writing NLP code. For a mathematically rich overview of how NLP with Deep Learning happens, read [Standford's Natural Language Processing with Deep Learning lecture notes Part 1](https://web.stanford.edu/class/cs224n/readings/cs224n-2019-notes01-wordvecs1.pdf).  \n",
    "  * For an even deeper dive, you could even do the whole [CS224n](http://web.stanford.edu/class/cs224n/) (Natural Language Processing with Deep Learning) course. \n",
    "* Great blog posts to read:\n",
    "  * Andrei Karpathy's [The Unreasonable Effectiveness of RNNs](https://karpathy.github.io/2015/05/21/rnn-effectiveness/) dives into generating Shakespeare text with RNNs.\n",
    "  * [Text Classification with NLP: Tf-Idf vs Word2Vec vs BERT](https://towardsdatascience.com/text-classification-with-nlp-tf-idf-vs-word2vec-vs-bert-41ff868d1794) by Mauro Di Pietro. An overview of different techniques for turning text into numbers and then classifying it.\n",
    "  * [What are word embeddings?](https://machinelearningmastery.com/what-are-word-embeddings/) by Machine Learning Mastery.\n",
    "* Other topics worth looking into:\n",
    "  * [Attention mechanisms](https://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/). These are a foundational component of the transformer architecture and also often add improvments to deep NLP models.\n",
    "  * [Transformer architectures](http://jalammar.github.io/illustrated-transformer/). This model architecture has recently taken the NLP world by storm, achieving state of the art on many benchmarks. However, it does take a little more processing to get off the ground, the [HuggingFace Models (formerly HuggingFace Transformers) library](https://huggingface.co/models/) is probably your best quick start.\n",
    "    * And now [HuggingFace even have their own course](https://huggingface.co/course/chapter1) on how their library works! I haven't done it but anything HuggingFace makes is world-class.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CLzfxgXkzEdr"
   },
   "source": [
    "> 📖 **Resource:** See the full set of course materials on GitHub: https://github.com/mrdbourke/tensorflow-deep-learning"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyNl2+t3VUTcweJBoXgoWw7J",
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "08_introduction_to_nlp_in_tensorflow.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
